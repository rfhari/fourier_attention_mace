/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))
from run_train args: Namespace(config=None, name='dimer-PP-lr-remove-adjusted-mace', seed=123, work_dir='.', log_dir='./logs', model_dir=None, checkpoints_dir=None, results_dir=None, downloads_dir=None, device='cuda', default_dtype='float64', distributed=False, log_level='INFO', error_table='PerAtomMAE', model='MACE', r_max=6.0, radial_type='bessel', num_radial_basis=8, num_cutoff_basis=5, pair_repulsion=False, distance_transform='None', interaction='RealAgnosticResidualInteractionBlock', interaction_first='RealAgnosticResidualInteractionBlock', max_ell=3, correlation=3, num_interactions=2, MLP_irreps='16x0e', radial_MLP='[64, 64, 64]', hidden_irreps=None, num_channels=256, max_L=2, gate='silu', scaling='rms_forces_scaling', avg_num_neighbors=1, compute_avg_num_neighbors=True, compute_stress=False, compute_forces=True, train_file='./custom_dataset/dimer_datasets/dimers_PP_train.xyz', valid_file='./custom_dataset/dimer_datasets/dimers_PP_test.xyz', valid_fraction=0.1, test_file='./custom_dataset/dimer_datasets/dimers_PP_test.xyz', test_dir=None, multi_processed_test=False, num_workers=0, pin_memory=True, atomic_numbers=None, mean=None, std=None, statistics_file=None, E0s='average', foundation_filter_elements=True, heads=None, multiheads_finetuning=True, weight_pt_head=1.0, num_samples_pt=1000, subselect_pt='random', pt_train_file=None, pt_valid_file=None, keep_isolated_atoms=False, energy_key='energy', forces_key='forces', virials_key='REF_virials', stress_key='REF_stress', dipole_key='REF_dipole', charges_key='REF_charges', loss='weighted', forces_weight=100.0, swa_forces_weight=10.0, energy_weight=10.0, swa_energy_weight=100.0, virials_weight=1.0, swa_virials_weight=10.0, stress_weight=1.0, swa_stress_weight=10.0, dipole_weight=1.0, swa_dipole_weight=1.0, config_type_weights='{"Default":1.0}', huber_delta=0.01, optimizer='adam', beta=0.9, batch_size=2, valid_batch_size=2, lr=0.01, swa_lr=0.001, weight_decay=5e-07, amsgrad=True, scheduler='ReduceLROnPlateau', lr_factor=0.8, scheduler_patience=5, lr_scheduler_gamma=0.9993, swa=True, start_swa=200, ema=True, ema_decay=0.99, max_num_epochs=400, patience=7, foundation_model=None, foundation_model_readout=True, eval_interval=3, keep_checkpoints=False, save_all_checkpoints=False, restart_latest=False, save_cpu=False, clip_grad=10.0, wandb=False, wandb_dir=None, wandb_project='', wandb_entity='', wandb_name='', wandb_log_hypers=['num_channels', 'max_L', 'correlation', 'lr', 'swa_lr', 'weight_decay', 'batch_size', 'max_num_epochs', 'start_swa', 'energy_weight', 'forces_weight'])
args model: MACE
2024-11-06 08:44:28.654 INFO: ===========VERIFYING SETTINGS===========
2024-11-06 08:44:28.654 INFO: MACE version: 0.3.7
2024-11-06 08:44:28.832 INFO: CUDA version: 11.8, CUDA device: 0
2024-11-06 08:44:29.073 INFO: ===========LOADING INPUT DATA===========
2024-11-06 08:44:29.073 INFO: Using heads: ['default']
2024-11-06 08:44:29.073 INFO: =============    Processing head default     ===========
2024-11-06 08:44:29.091 WARNING: Since ASE version 3.23.0b1, using energy_key 'energy' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting 'energy' to 'REF_energy'. You need to use --energy_key='REF_energy' to specify the chosen key name.
2024-11-06 08:44:29.093 WARNING: Since ASE version 3.23.0b1, using forces_key 'forces' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting 'forces' to 'REF_forces'. You need to use --forces_key='REF_forces' to specify the chosen key name.
2024-11-06 08:44:29.095 INFO: Training set [10 configs, 10 energy, 540 forces] loaded from './custom_dataset/dimer_datasets/dimers_PP_train.xyz'
2024-11-06 08:44:29.098 WARNING: Since ASE version 3.23.0b1, using energy_key 'energy' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting 'energy' to 'REF_energy'. You need to use --energy_key='REF_energy' to specify the chosen key name.
2024-11-06 08:44:29.099 WARNING: Since ASE version 3.23.0b1, using forces_key 'forces' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting 'forces' to 'REF_forces'. You need to use --forces_key='REF_forces' to specify the chosen key name.
2024-11-06 08:44:29.099 INFO: Validation set [3 configs, 3 energy, 162 forces] loaded from './custom_dataset/dimer_datasets/dimers_PP_test.xyz'
2024-11-06 08:44:29.103 WARNING: Since ASE version 3.23.0b1, using energy_key 'energy' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting 'energy' to 'REF_energy'. You need to use --energy_key='REF_energy' to specify the chosen key name.
2024-11-06 08:44:29.104 WARNING: Since ASE version 3.23.0b1, using forces_key 'forces' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting 'forces' to 'REF_forces'. You need to use --forces_key='REF_forces' to specify the chosen key name.
2024-11-06 08:44:29.104 INFO: Test set (3 configs) loaded from './custom_dataset/dimer_datasets/dimers_PP_test.xyz':
2024-11-06 08:44:29.104 INFO: Default_Default: 3 configs, 3 energy, 162 forces
2024-11-06 08:44:29.104 INFO: Total number of configurations: train=10, valid=3, tests=[Default_Default: 3],
2024-11-06 08:44:29.104 INFO: Atomic Numbers used: [1, 6, 7, 8]
2024-11-06 08:44:29.105 INFO: Isolated Atomic Energies (E0s) not in training file, using command line argument
2024-11-06 08:44:29.105 INFO: Computing average Atomic Energies using least squares regression
2024-11-06 08:44:29.105 INFO: Atomic Energies used (z: eV) for head default: {1: -918.5613261165753, 6: -367.4245304466303, 7: -183.7122652233151, 8: -183.7122652233151}
2024-11-06 08:44:29.134 INFO: Computing average number of neighbors
2024-11-06 08:44:29.378 INFO: Average number of neighbors: 9.555555555555555
2024-11-06 08:44:29.379 INFO: During training the following quantities will be reported: energy, forces
2024-11-06 08:44:29.379 INFO: ===========MODEL DETAILS===========
2024-11-06 08:44:29.428 INFO: Building model
2024-11-06 08:44:29.428 INFO: Message passing with 256 channels and max_L=2 (256x0e+256x1o+256x2e)
2024-11-06 08:44:29.428 INFO: 2 layers, each with correlation order: 3 (body order: 4) and spherical harmonics up to: l=3
2024-11-06 08:44:29.428 INFO: 8 radial and 5 basis functions
2024-11-06 08:44:29.428 INFO: Radial cutoff: 6.0 A (total receptive field for each atom: 12.0 A)
2024-11-06 08:44:29.428 INFO: Distance transform for radial basis functions: None
2024-11-06 08:44:29.428 INFO: Hidden irreps: 256x0e+256x1o+256x2e
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
in the linear readout block 256x0e 16x0e 1
2024-11-06 08:45:49.378 INFO: Total number of parameters: 3925520
2024-11-06 08:45:49.378 INFO: 
2024-11-06 08:45:49.378 INFO: ===========OPTIMIZER INFORMATION===========
2024-11-06 08:45:49.378 INFO: Using ADAM as parameter optimizer
2024-11-06 08:45:49.378 INFO: Batch size: 2
2024-11-06 08:45:49.378 INFO: Using Exponential Moving Average with decay: 0.99
2024-11-06 08:45:49.379 INFO: Number of gradient updates: 2000
2024-11-06 08:45:49.379 INFO: Learning rate: 0.01, weight decay: 5e-07
2024-11-06 08:45:49.379 INFO: WeightedEnergyForcesLoss(energy_weight=10.000, forces_weight=100.000)
2024-11-06 08:45:49.379 INFO: Stage Two (after 200 epochs) with loss function: WeightedEnergyForcesLoss(energy_weight=100.000, forces_weight=10.000), with energy weight : 100.0, forces weight : 10.0 and learning rate : 0.001
2024-11-06 08:45:49.503 INFO: Using gradient clipping with tolerance=10.000
2024-11-06 08:45:49.503 INFO: 
2024-11-06 08:45:49.503 INFO: ===========TRAINING===========
2024-11-06 08:45:49.503 INFO: Started training, reporting errors on validation set
2024-11-06 08:45:49.503 INFO: Loss metrics on validation set
2024-11-06 08:45:49.797 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:45:49.877 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:45:49.885 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:45:49.890 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:45:49.895 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:45:49.900 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:45:49.905 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02532, max. 0.02530
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.08170, max. 0.09557
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09671, max. 0.08166
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00296, max. 0.00279
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02083, max. 0.02101
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02532, max. 0.02530
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.08190, max. 0.09557
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09671, max. 0.08166
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00290, max. 0.00320
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02071, max. 0.01870
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:45:50.290 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:45:50.293 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:45:50.293 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:45:50.294 INFO: total_energy: torch.Size([2])
2024-11-06 08:45:50.294 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:45:50.294 INFO: node_energy: torch.Size([36])
2024-11-06 08:45:50.928 INFO: node_feats: torch.Size([18, 2304])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:45:51.012 INFO: node_feats: torch.Size([18, 256])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:45:51.019 INFO: node_feats_out:torch.Size([18, 256]), torch.Size([2560]), 18
2024-11-06 08:45:51.025 INFO: linear_node_feats: torch.Size([18, 256])
2024-11-06 08:45:51.031 INFO: q_node_feats: torch.Size([18, 256])
2024-11-06 08:45:51.038 INFO: v_node_feats: torch.Size([18, 256])
2024-11-06 08:45:51.044 INFO: k_node_feats: torch.Size([18, 256])
q_vector from forward: torch.Size([18, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02532, max. 0.02530
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.08170, max. 0.09557
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09671, max. 0.08166
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00336, max. 0.00300
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02076, max. 0.01867
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([18, 256])
2024-11-06 08:45:51.186 INFO: long_range_embedding: torch.Size([18, 256])
2024-11-06 08:45:51.189 INFO: long_range_energy: torch.Size([18, 1]), torch.Size([1])
2024-11-06 08:45:51.190 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-11-06 08:45:51.190 INFO: total_energy: torch.Size([1])
2024-11-06 08:45:51.190 INFO: node_energy_contributions: torch.Size([18, 4])
2024-11-06 08:45:51.190 INFO: node_energy: torch.Size([18])
2024-11-06 08:45:51.396 INFO: Initial: head: default, loss=  0.1337, MAE_E_per_atom=     2.7 meV, MAE_F=    36.6 meV / A
2024-11-06 08:45:52.575 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:45:52.638 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:45:52.642 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:45:52.645 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:45:52.649 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:45:52.653 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:45:52.656 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02532, max. 0.02530
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.08412, max. 0.09557
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09671, max. 0.08166
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00328, max. 0.00269
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02074, max. 0.02266
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02532, max. 0.02530
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.08298, max. 0.09557
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09671, max. 0.08166
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00324, max. 0.00322
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02085, max. 0.02472
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:45:52.943 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:45:52.945 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:45:52.945 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:45:52.945 INFO: total_energy: torch.Size([2])
2024-11-06 08:45:52.946 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:45:52.946 INFO: node_energy: torch.Size([36])
2024-11-06 08:45:56.819 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:45:56.880 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:45:56.884 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:45:56.887 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:45:56.890 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:45:56.894 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:45:56.897 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02144, max. 0.02144
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.08828, max. 0.08988
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10828, max. 0.09065
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00272, max. 0.00279
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02568, max. 0.01984
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02140, max. 0.02137
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.08912, max. 0.09061
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10891, max. 0.09077
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00272, max. 0.00265
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02480, max. 0.01892
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:45:57.315 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:45:57.317 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:45:57.317 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:45:57.318 INFO: total_energy: torch.Size([2])
2024-11-06 08:45:57.318 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:45:57.318 INFO: node_energy: torch.Size([36])
2024-11-06 08:46:01.162 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:01.224 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:01.227 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:46:01.230 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:46:01.233 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:46:01.236 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:46:01.240 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02157, max. 0.02129
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.08692, max. 0.09481
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.12208, max. 0.10567
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00267, max. 0.00245
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00012, max. 0.00011
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02879, max. 0.02559
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02140, max. 0.02138
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.08801, max. 0.09615
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.12276, max. 0.10607
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00253, max. 0.00237
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00013, max. 0.00011
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02954, max. 0.03152
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:46:01.523 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:46:01.525 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:46:01.525 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:46:01.525 INFO: total_energy: torch.Size([2])
2024-11-06 08:46:01.526 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:46:01.526 INFO: node_energy: torch.Size([36])
2024-11-06 08:46:07.080 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:07.141 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:07.145 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:46:07.148 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:46:07.151 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:46:07.155 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:46:07.158 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02151, max. 0.02123
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.08208, max. 0.09176
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10538, max. 0.10211
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00349, max. 0.00235
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00010
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02739, max. 0.02466
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02097, max. 0.02096
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.08568, max. 0.09352
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10667, max. 0.10135
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00314, max. 0.00216
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00010
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02840, max. 0.02665
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:46:07.446 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:46:07.447 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:46:07.447 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:46:07.448 INFO: total_energy: torch.Size([2])
2024-11-06 08:46:07.448 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:46:07.448 INFO: node_energy: torch.Size([36])
2024-11-06 08:46:11.715 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:11.775 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:11.778 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:46:11.782 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:46:11.785 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:46:11.788 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:46:11.791 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02022, max. 0.02021
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.08300, max. 0.08922
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08979, max. 0.08884
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00311, max. 0.00205
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02422, max. 0.02436
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02022, max. 0.02020
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.08196, max. 0.08894
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08977, max. 0.08876
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00309, max. 0.00210
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02460, max. 0.02359
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:46:12.076 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:46:12.078 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:46:12.078 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:46:12.079 INFO: total_energy: torch.Size([2])
2024-11-06 08:46:12.079 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:46:12.079 INFO: node_energy: torch.Size([36])
2024-11-06 08:46:16.440 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:16.496 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:16.499 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:46:16.502 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:46:16.504 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:46:16.506 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:46:16.509 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01893, max. 0.01893
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.07424, max. 0.08071
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08239, max. 0.08013
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00279, max. 0.00189
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02068, max. 0.01984
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01893, max. 0.01893
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.07516, max. 0.08071
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08239, max. 0.08013
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00279, max. 0.00189
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02059, max. 0.01972
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:46:16.785 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:46:16.786 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:46:16.786 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:46:16.786 INFO: total_energy: torch.Size([2])
2024-11-06 08:46:16.786 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:46:16.787 INFO: node_energy: torch.Size([36])
2024-11-06 08:46:17.144 INFO: node_feats: torch.Size([18, 2304])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:46:17.196 INFO: node_feats: torch.Size([18, 256])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:46:17.199 INFO: node_feats_out:torch.Size([18, 256]), torch.Size([2560]), 18
2024-11-06 08:46:17.201 INFO: linear_node_feats: torch.Size([18, 256])
2024-11-06 08:46:17.204 INFO: q_node_feats: torch.Size([18, 256])
2024-11-06 08:46:17.206 INFO: v_node_feats: torch.Size([18, 256])
2024-11-06 08:46:17.208 INFO: k_node_feats: torch.Size([18, 256])
q_vector from forward: torch.Size([18, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01893, max. 0.01893
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.07735, max. 0.08071
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08239, max. 0.08013
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00279, max. 0.00186
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02068, max. 0.01975
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([18, 256])
2024-11-06 08:46:17.347 INFO: long_range_embedding: torch.Size([18, 256])
2024-11-06 08:46:17.348 INFO: long_range_energy: torch.Size([18, 1]), torch.Size([1])
2024-11-06 08:46:17.349 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-11-06 08:46:17.349 INFO: total_energy: torch.Size([1])
2024-11-06 08:46:17.349 INFO: node_energy_contributions: torch.Size([18, 4])
2024-11-06 08:46:17.349 INFO: node_energy: torch.Size([18])
2024-11-06 08:46:17.536 INFO: Epoch 0: head: default, loss=  0.3974, MAE_E_per_atom=     3.9 meV, MAE_F=    51.8 meV / A
2024-11-06 08:46:18.080 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:18.140 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:18.143 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:46:18.147 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:46:18.150 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:46:18.153 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:46:18.156 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01810, max. 0.01804
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06959, max. 0.07463
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07543, max. 0.07544
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00264, max. 0.00171
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01784, max. 0.01733
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01813, max. 0.01812
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.07072, max. 0.07405
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07392, max. 0.07485
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00257, max. 0.00161
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01767, max. 0.02024
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:46:18.567 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:46:18.569 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:46:18.569 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:46:18.569 INFO: total_energy: torch.Size([2])
2024-11-06 08:46:18.569 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:46:18.570 INFO: node_energy: torch.Size([36])
2024-11-06 08:46:19.894 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:19.955 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:19.959 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:46:19.962 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:46:19.965 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:46:19.968 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:46:19.971 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01610, max. 0.01609
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05948, max. 0.05864
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06353, max. 0.06431
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00195, max. 0.00133
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01347, max. 0.01416
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01610, max. 0.01610
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05863, max. 0.05891
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06337, max. 0.06414
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00197, max. 0.00130
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01292, max. 0.01504
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:46:20.260 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:46:20.262 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:46:20.262 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:46:20.262 INFO: total_energy: torch.Size([2])
2024-11-06 08:46:20.262 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:46:20.263 INFO: node_energy: torch.Size([36])
2024-11-06 08:46:21.607 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:21.667 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:21.670 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:46:21.674 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:46:21.677 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:46:21.680 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:46:21.683 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01461, max. 0.01444
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04531, max. 0.05558
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05931, max. 0.05464
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00167, max. 0.00111
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01123, max. 0.01042
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01424, max. 0.01411
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04853, max. 0.05608
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05833, max. 0.05521
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00161, max. 0.00108
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01161, max. 0.01078
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:46:21.968 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:46:21.970 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:46:21.970 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:46:21.970 INFO: total_energy: torch.Size([2])
2024-11-06 08:46:21.971 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:46:21.971 INFO: node_energy: torch.Size([36])
2024-11-06 08:46:23.329 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:23.389 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:23.393 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:46:23.396 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:46:23.399 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:46:23.402 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:46:23.405 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01284, max. 0.01284
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04327, max. 0.05341
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05083, max. 0.04683
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00107, max. 0.00104
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01021, max. 0.00996
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01282, max. 0.01282
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04356, max. 0.05310
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05028, max. 0.04610
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00105, max. 0.00105
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01038, max. 0.00989
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:46:23.693 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:46:23.695 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:46:23.695 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:46:23.696 INFO: total_energy: torch.Size([2])
2024-11-06 08:46:23.696 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:46:23.696 INFO: node_energy: torch.Size([36])
2024-11-06 08:46:25.026 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:25.099 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:25.102 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:46:25.105 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:46:25.108 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:46:25.112 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:46:25.115 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01170, max. 0.01170
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04162, max. 0.05131
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04371, max. 0.04154
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00112, max. 0.00106
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00917, max. 0.00906
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01170, max. 0.01170
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04162, max. 0.05131
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04371, max. 0.04154
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00112, max. 0.00109
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00902, max. 0.00882
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:46:25.404 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:46:25.406 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:46:25.406 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:46:25.406 INFO: total_energy: torch.Size([2])
2024-11-06 08:46:25.406 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:46:25.406 INFO: node_energy: torch.Size([36])
2024-11-06 08:46:26.870 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:26.930 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:26.934 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:46:26.937 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:46:26.940 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:46:26.943 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:46:26.947 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01073, max. 0.01070
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04137, max. 0.05030
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03988, max. 0.03940
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00121, max. 0.00117
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00833, max. 0.00754
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01075, max. 0.01075
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04224, max. 0.05004
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04036, max. 0.03878
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00122, max. 0.00112
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00841, max. 0.00870
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:46:27.236 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:46:27.238 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:46:27.238 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:46:27.238 INFO: total_energy: torch.Size([2])
2024-11-06 08:46:27.238 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:46:27.238 INFO: node_energy: torch.Size([36])
2024-11-06 08:46:28.564 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:28.624 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:28.628 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:46:28.631 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:46:28.634 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:46:28.637 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:46:28.640 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00999, max. 0.01031
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04310, max. 0.04917
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04054, max. 0.03755
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00128, max. 0.00105
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00821, max. 0.00882
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01031, max. 0.01031
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04310, max. 0.04917
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04054, max. 0.03755
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00128, max. 0.00110
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00836, max. 0.00834
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:46:28.943 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:46:28.944 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:46:28.945 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:46:28.945 INFO: total_energy: torch.Size([2])
2024-11-06 08:46:28.945 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:46:28.945 INFO: node_energy: torch.Size([36])
2024-11-06 08:46:30.274 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:30.335 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:30.339 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:46:30.342 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:46:30.345 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:46:30.348 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:46:30.352 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01014, max. 0.01014
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04381, max. 0.04853
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04002, max. 0.03865
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00148, max. 0.00125
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00797, max. 0.00792
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01006, max. 0.01015
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04380, max. 0.04865
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04116, max. 0.03799
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00129, max. 0.00121
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00843, max. 0.00790
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:46:30.641 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:46:30.642 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:46:30.643 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:46:30.643 INFO: total_energy: torch.Size([2])
2024-11-06 08:46:30.643 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:46:30.643 INFO: node_energy: torch.Size([36])
2024-11-06 08:46:31.985 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:32.046 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:32.050 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:46:32.053 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:46:32.056 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:46:32.059 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:46:32.062 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00962, max. 0.00984
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04366, max. 0.04858
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03811, max. 0.03723
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00137, max. 0.00133
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00835, max. 0.00830
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00987, max. 0.00991
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04473, max. 0.04817
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04008, max. 0.03854
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00132, max. 0.00119
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00867, max. 0.00897
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:46:32.348 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:46:32.350 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:46:32.350 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:46:32.351 INFO: total_energy: torch.Size([2])
2024-11-06 08:46:32.351 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:46:32.351 INFO: node_energy: torch.Size([36])
2024-11-06 08:46:33.692 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:33.753 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:33.756 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:46:33.759 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:46:33.762 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:46:33.766 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:46:33.769 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00976, max. 0.00977
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04562, max. 0.04816
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03856, max. 0.03855
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00134, max. 0.00123
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00882, max. 0.00878
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00970, max. 0.00977
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04567, max. 0.04792
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03862, max. 0.03829
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00135, max. 0.00124
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00885, max. 0.00871
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:46:34.162 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:46:34.164 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:46:34.164 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:46:34.164 INFO: total_energy: torch.Size([2])
2024-11-06 08:46:34.164 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:46:34.165 INFO: node_energy: torch.Size([36])
2024-11-06 08:46:35.502 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:35.563 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:35.566 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:46:35.569 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:46:35.572 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:46:35.576 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:46:35.579 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00976, max. 0.00976
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04599, max. 0.04790
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03710, max. 0.03817
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00133, max. 0.00126
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00895, max. 0.00880
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00963, max. 0.00955
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04498, max. 0.04831
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03711, max. 0.03687
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00140, max. 0.00146
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00854, max. 0.00844
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:46:35.867 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:46:35.868 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:46:35.868 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:46:35.869 INFO: total_energy: torch.Size([2])
2024-11-06 08:46:35.869 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:46:35.869 INFO: node_energy: torch.Size([36])
2024-11-06 08:46:37.198 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:37.259 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:37.262 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:46:37.265 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:46:37.268 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:46:37.272 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:46:37.275 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00986, max. 0.00984
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04632, max. 0.04789
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03685, max. 0.03719
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00134, max. 0.00142
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00889, max. 0.00846
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00987, max. 0.00949
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04625, max. 0.04769
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03654, max. 0.03739
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00134, max. 0.00121
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00896, max. 0.00883
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:46:37.566 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:46:37.567 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:46:37.567 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:46:37.568 INFO: total_energy: torch.Size([2])
2024-11-06 08:46:37.568 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:46:37.568 INFO: node_energy: torch.Size([36])
2024-11-06 08:46:38.900 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:38.961 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:38.964 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:46:38.968 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:46:38.971 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:46:38.974 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:46:38.977 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01013, max. 0.00990
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04543, max. 0.04763
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03565, max. 0.03611
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00135, max. 0.00147
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00869, max. 0.00807
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00992, max. 0.00936
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04599, max. 0.04703
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03607, max. 0.03610
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00136, max. 0.00134
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00919, max. 0.00893
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:46:39.265 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:46:39.267 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:46:39.267 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:46:39.267 INFO: total_energy: torch.Size([2])
2024-11-06 08:46:39.267 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:46:39.267 INFO: node_energy: torch.Size([36])
2024-11-06 08:46:40.603 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:40.664 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:40.668 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:46:40.671 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:46:40.674 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:46:40.677 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:46:40.680 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00998, max. 0.00998
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04560, max. 0.04687
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03519, max. 0.03528
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00141, max. 0.00138
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00903, max. 0.00841
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00979, max. 0.00979
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04567, max. 0.04724
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03906, max. 0.04145
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00141, max. 0.00151
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00844, max. 0.00850
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:46:40.968 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:46:40.970 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:46:40.970 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:46:40.970 INFO: total_energy: torch.Size([2])
2024-11-06 08:46:40.971 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:46:40.971 INFO: node_energy: torch.Size([36])
2024-11-06 08:46:42.413 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:42.473 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:42.477 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:46:42.480 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:46:42.483 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:46:42.486 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:46:42.489 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01012, max. 0.01008
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04537, max. 0.04665
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03392, max. 0.03373
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00138, max. 0.00150
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00003, max. 0.00003
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00923, max. 0.00896
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01012, max. 0.00983
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04540, max. 0.04674
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03390, max. 0.03377
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00144, max. 0.00152
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00003, max. 0.00003
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00907, max. 0.00897
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:46:42.778 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:46:42.780 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:46:42.780 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:46:42.781 INFO: total_energy: torch.Size([2])
2024-11-06 08:46:42.781 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:46:42.781 INFO: node_energy: torch.Size([36])
2024-11-06 08:46:44.109 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:44.166 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:44.169 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:46:44.172 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:46:44.174 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:46:44.176 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:46:44.179 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00999, max. 0.00999
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04514, max. 0.04694
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03464, max. 0.03476
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00136, max. 0.00149
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00905, max. 0.00855
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00999, max. 0.00994
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04514, max. 0.04694
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03464, max. 0.03476
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00139, max. 0.00136
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00903, max. 0.00855
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:46:44.459 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:46:44.460 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:46:44.460 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:46:44.461 INFO: total_energy: torch.Size([2])
2024-11-06 08:46:44.461 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:46:44.461 INFO: node_energy: torch.Size([36])
2024-11-06 08:46:46.327 INFO: node_feats: torch.Size([18, 2304])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:46:46.380 INFO: node_feats: torch.Size([18, 256])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:46:46.383 INFO: node_feats_out:torch.Size([18, 256]), torch.Size([2560]), 18
2024-11-06 08:46:46.385 INFO: linear_node_feats: torch.Size([18, 256])
2024-11-06 08:46:46.388 INFO: q_node_feats: torch.Size([18, 256])
2024-11-06 08:46:46.390 INFO: v_node_feats: torch.Size([18, 256])
2024-11-06 08:46:46.392 INFO: k_node_feats: torch.Size([18, 256])
q_vector from forward: torch.Size([18, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00999, max. 0.00998
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04514, max. 0.04694
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03464, max. 0.03476
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00135, max. 0.00131
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00892, max. 0.00861
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([18, 256])
2024-11-06 08:46:46.532 INFO: long_range_embedding: torch.Size([18, 256])
2024-11-06 08:46:46.533 INFO: long_range_energy: torch.Size([18, 1]), torch.Size([1])
2024-11-06 08:46:46.534 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-11-06 08:46:46.534 INFO: total_energy: torch.Size([1])
2024-11-06 08:46:46.534 INFO: node_energy_contributions: torch.Size([18, 4])
2024-11-06 08:46:46.534 INFO: node_energy: torch.Size([18])
2024-11-06 08:46:46.721 INFO: Epoch 3: head: default, loss=  0.1046, MAE_E_per_atom=     3.3 meV, MAE_F=    23.8 meV / A
2024-11-06 08:46:47.084 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:47.145 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:47.149 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:46:47.152 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:46:47.155 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:46:47.158 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:46:47.161 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01008, max. 0.01008
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04533, max. 0.04589
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03269, max. 0.03664
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00146, max. 0.00146
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00003, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00891, max. 0.00902
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01007, max. 0.01005
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04516, max. 0.04613
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03301, max. 0.03491
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00140, max. 0.00155
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00003, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00886, max. 0.00917
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:46:47.448 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:46:47.450 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:46:47.450 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:46:47.450 INFO: total_energy: torch.Size([2])
2024-11-06 08:46:47.450 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:46:47.450 INFO: node_energy: torch.Size([36])
2024-11-06 08:46:48.767 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:48.827 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:48.831 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:46:48.834 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:46:48.837 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:46:48.840 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:46:48.843 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00991, max. 0.00987
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04539, max. 0.04462
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03168, max. 0.03487
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00133, max. 0.00149
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00003, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00894, max. 0.00948
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00978, max. 0.00966
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04617, max. 0.04526
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03586, max. 0.03961
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00141, max. 0.00168
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00845, max. 0.00919
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:46:49.139 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:46:49.141 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:46:49.141 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:46:49.141 INFO: total_energy: torch.Size([2])
2024-11-06 08:46:49.142 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:46:49.142 INFO: node_energy: torch.Size([36])
2024-11-06 08:46:50.488 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:50.549 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:50.553 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:46:50.556 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:46:50.559 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:46:50.562 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:46:50.565 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00976, max. 0.00922
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04580, max. 0.04362
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03230, max. 0.03533
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00139, max. 0.00142
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00003, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00877, max. 0.00988
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00976, max. 0.00976
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04578, max. 0.04374
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03565, max. 0.03800
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00135, max. 0.00138
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00938, max. 0.00957
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:46:50.856 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:46:50.857 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:46:50.858 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:46:50.858 INFO: total_energy: torch.Size([2])
2024-11-06 08:46:50.858 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:46:50.858 INFO: node_energy: torch.Size([36])
2024-11-06 08:46:52.186 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:52.248 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:52.251 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:46:52.254 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:46:52.257 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:46:52.261 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:46:52.264 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00948, max. 0.00919
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04555, max. 0.04234
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03586, max. 0.03730
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00127, max. 0.00127
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00846, max. 0.00983
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00925, max. 0.00925
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04782, max. 0.04309
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04695, max. 0.04732
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00127, max. 0.00150
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00885, max. 0.00984
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:46:52.651 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:46:52.653 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:46:52.653 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:46:52.653 INFO: total_energy: torch.Size([2])
2024-11-06 08:46:52.653 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:46:52.653 INFO: node_energy: torch.Size([36])
2024-11-06 08:46:53.993 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:54.055 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:54.058 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:46:54.061 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:46:54.064 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:46:54.068 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:46:54.071 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00934, max. 0.00935
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04606, max. 0.04163
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03587, max. 0.03755
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00135, max. 0.00150
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01015, max. 0.01090
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01018, max. 0.01018
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04580, max. 0.04201
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03441, max. 0.03639
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00131, max. 0.00158
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00950, max. 0.00963
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:46:54.360 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:46:54.362 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:46:54.362 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:46:54.362 INFO: total_energy: torch.Size([2])
2024-11-06 08:46:54.362 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:46:54.363 INFO: node_energy: torch.Size([36])
2024-11-06 08:46:55.708 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:55.769 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:55.772 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:46:55.776 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:46:55.779 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:46:55.782 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:46:55.785 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00947, max. 0.00947
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04620, max. 0.04073
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03624, max. 0.03857
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00135, max. 0.00138
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00988, max. 0.01203
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00947, max. 0.00947
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04619, max. 0.04087
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03910, max. 0.03908
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00129, max. 0.00126
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00913, max. 0.01162
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:46:56.076 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:46:56.077 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:46:56.077 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:46:56.078 INFO: total_energy: torch.Size([2])
2024-11-06 08:46:56.078 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:46:56.078 INFO: node_energy: torch.Size([36])
2024-11-06 08:46:57.400 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:57.461 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:57.464 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:46:57.467 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:46:57.470 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:46:57.474 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:46:57.477 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00946, max. 0.00946
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04569, max. 0.03974
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03725, max. 0.04029
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00126, max. 0.00141
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01051, max. 0.01262
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00946, max. 0.00946
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04553, max. 0.03983
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03914, max. 0.03960
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00128, max. 0.00144
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00934, max. 0.01196
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:46:57.769 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:46:57.771 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:46:57.771 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:46:57.772 INFO: total_energy: torch.Size([2])
2024-11-06 08:46:57.772 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:46:57.772 INFO: node_energy: torch.Size([36])
2024-11-06 08:46:59.104 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:59.165 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:46:59.169 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:46:59.172 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:46:59.175 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:46:59.178 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:46:59.181 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00985, max. 0.00985
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04483, max. 0.03919
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03783, max. 0.04181
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00125, max. 0.00147
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00980, max. 0.01118
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00939, max. 0.00939
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04511, max. 0.03883
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04096, max. 0.04032
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00128, max. 0.00139
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01059, max. 0.01248
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:46:59.472 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:46:59.474 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:46:59.474 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:46:59.474 INFO: total_energy: torch.Size([2])
2024-11-06 08:46:59.474 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:46:59.475 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:00.911 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:00.972 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:00.975 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:00.979 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:00.982 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:00.985 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:00.988 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00930, max. 0.00931
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04685, max. 0.03889
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05348, max. 0.04953
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00118, max. 0.00137
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00990, max. 0.01241
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00933, max. 0.00934
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04454, max. 0.03806
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04441, max. 0.04081
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00126, max. 0.00127
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01091, max. 0.01248
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:01.278 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:01.280 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:01.280 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:01.280 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:01.280 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:01.280 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:02.618 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:02.679 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:02.683 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:02.686 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:02.689 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:02.692 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:02.695 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00994, max. 0.00994
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04580, max. 0.03793
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04787, max. 0.04431
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00129, max. 0.00147
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00984, max. 0.01222
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00948, max. 0.00948
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04494, max. 0.03813
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04408, max. 0.04310
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00122, max. 0.00124
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01112, max. 0.01334
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:02.985 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:02.987 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:02.987 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:02.987 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:02.987 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:02.988 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:04.351 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:04.412 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:04.416 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:04.419 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:04.422 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:04.425 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:04.428 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00971, max. 0.00971
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04813, max. 0.03938
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05838, max. 0.05292
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00126, max. 0.00142
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01098, max. 0.01389
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01006, max. 0.01006
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04545, max. 0.03886
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04391, max. 0.04821
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00131, max. 0.00146
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01069, max. 0.01279
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:04.718 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:04.720 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:04.720 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:04.720 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:04.720 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:04.720 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:06.058 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:06.120 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:06.123 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:06.126 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:06.130 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:06.133 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:06.136 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01026, max. 0.01026
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04791, max. 0.03987
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05229, max. 0.05246
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00145, max. 0.00142
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01403, max. 0.01539
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01026, max. 0.01026
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04791, max. 0.03987
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05205, max. 0.05246
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00141, max. 0.00127
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01274, max. 0.01546
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:06.427 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:06.429 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:06.429 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:06.429 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:06.429 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:06.429 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:07.755 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:07.816 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:07.820 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:07.823 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:07.826 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:07.829 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:07.832 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01056, max. 0.01056
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04908, max. 0.04059
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04764, max. 0.05614
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00149, max. 0.00153
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01380, max. 0.01700
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01055, max. 0.01055
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04889, max. 0.04068
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05314, max. 0.05510
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00150, max. 0.00159
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01240, max. 0.01621
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:08.121 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:08.123 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:08.123 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:08.124 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:08.124 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:08.124 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:09.562 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:09.622 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:09.626 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:09.629 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:09.632 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:09.635 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:09.639 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01069, max. 0.01069
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04981, max. 0.04099
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05417, max. 0.05663
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00153, max. 0.00149
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01316, max. 0.01744
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01069, max. 0.01069
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04980, max. 0.04115
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05465, max. 0.05732
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00153, max. 0.00149
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01507, max. 0.01715
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:09.927 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:09.929 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:09.929 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:09.929 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:09.930 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:09.930 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:11.258 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:11.328 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:11.331 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:11.334 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:11.337 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:11.340 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:11.344 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01060, max. 0.01061
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04981, max. 0.04121
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05822, max. 0.05589
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00155, max. 0.00158
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01378, max. 0.01709
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01078, max. 0.01078
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05073, max. 0.04102
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06212, max. 0.05594
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00161, max. 0.00174
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01246, max. 0.01584
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:11.633 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:11.634 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:11.635 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:11.635 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:11.635 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:11.635 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:12.973 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:13.030 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:13.032 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:13.035 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:13.037 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:13.040 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:13.042 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01019, max. 0.01020
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04816, max. 0.04053
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05142, max. 0.05031
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00143, max. 0.00156
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01579, max. 0.01534
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01019, max. 0.01020
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04816, max. 0.04053
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05397, max. 0.05031
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00143, max. 0.00139
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01491, max. 0.01533
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:13.324 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:13.325 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:13.325 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:13.326 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:13.326 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:13.326 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:13.691 INFO: node_feats: torch.Size([18, 2304])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:47:13.744 INFO: node_feats: torch.Size([18, 256])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:47:13.746 INFO: node_feats_out:torch.Size([18, 256]), torch.Size([2560]), 18
2024-11-06 08:47:13.749 INFO: linear_node_feats: torch.Size([18, 256])
2024-11-06 08:47:13.751 INFO: q_node_feats: torch.Size([18, 256])
2024-11-06 08:47:13.754 INFO: v_node_feats: torch.Size([18, 256])
2024-11-06 08:47:13.756 INFO: k_node_feats: torch.Size([18, 256])
q_vector from forward: torch.Size([18, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01019, max. 0.01020
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04816, max. 0.04053
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05252, max. 0.05031
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00143, max. 0.00137
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01167, max. 0.01535
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([18, 256])
2024-11-06 08:47:13.896 INFO: long_range_embedding: torch.Size([18, 256])
2024-11-06 08:47:13.897 INFO: long_range_energy: torch.Size([18, 1]), torch.Size([1])
2024-11-06 08:47:13.897 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-11-06 08:47:13.897 INFO: total_energy: torch.Size([1])
2024-11-06 08:47:13.898 INFO: node_energy_contributions: torch.Size([18, 4])
2024-11-06 08:47:13.898 INFO: node_energy: torch.Size([18])
2024-11-06 08:47:14.085 INFO: Epoch 6: head: default, loss=  0.1114, MAE_E_per_atom=     3.4 meV, MAE_F=    23.2 meV / A
2024-11-06 08:47:14.156 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:14.217 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:14.221 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:14.224 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:14.227 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:14.230 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:14.233 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01136, max. 0.01136
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04960, max. 0.04180
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05673, max. 0.05755
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00158, max. 0.00169
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01258, max. 0.01551
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01058, max. 0.01059
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04994, max. 0.04128
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06274, max. 0.05497
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00154, max. 0.00138
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01493, max. 0.01709
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:14.520 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:14.521 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:14.521 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:14.522 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:14.522 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:14.522 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:15.850 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:15.911 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:15.914 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:15.917 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:15.920 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:15.924 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:15.927 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01049, max. 0.01049
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04976, max. 0.04121
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06571, max. 0.05573
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00157, max. 0.00151
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01607, max. 0.01695
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01049, max. 0.01049
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04976, max. 0.04121
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06242, max. 0.05711
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00152, max. 0.00149
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01494, max. 0.01723
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:16.221 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:16.223 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:16.223 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:16.223 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:16.223 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:16.223 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:17.670 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:17.731 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:17.735 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:17.738 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:17.741 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:17.744 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:17.747 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01028, max. 0.01028
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04974, max. 0.04054
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07005, max. 0.05998
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00153, max. 0.00169
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01269, max. 0.01612
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01024, max. 0.01024
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04859, max. 0.04111
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06576, max. 0.05611
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00148, max. 0.00159
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01300, max. 0.01650
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:18.035 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:18.037 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:18.037 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:18.037 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:18.037 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:18.038 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:19.379 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:19.440 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:19.443 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:19.446 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:19.449 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:19.453 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:19.456 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01025, max. 0.01026
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04862, max. 0.04084
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06329, max. 0.05996
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00146, max. 0.00152
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01373, max. 0.01765
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01025, max. 0.01025
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04863, max. 0.04084
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06714, max. 0.05941
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00149, max. 0.00144
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01369, max. 0.01755
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:19.745 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:19.747 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:19.747 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:19.747 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:19.747 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:19.747 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:21.075 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:21.136 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:21.139 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:21.143 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:21.146 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:21.149 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:21.152 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01023, max. 0.01024
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04818, max. 0.04071
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07065, max. 0.05914
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00147, max. 0.00151
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01382, max. 0.01760
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01020, max. 0.01021
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05092, max. 0.04147
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08095, max. 0.06996
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00139, max. 0.00158
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01431, max. 0.01782
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:21.440 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:21.441 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:21.442 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:21.442 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:21.442 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:21.442 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:22.793 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:22.854 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:22.858 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:22.861 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:22.864 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:22.867 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:22.870 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01131, max. 0.01131
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04838, max. 0.04195
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06769, max. 0.06090
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00152, max. 0.00164
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01385, max. 0.01692
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01050, max. 0.01050
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04873, max. 0.04097
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07305, max. 0.06107
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00151, max. 0.00154
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01467, max. 0.01874
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:23.161 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:23.162 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:23.163 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:23.163 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:23.163 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:23.163 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:24.487 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:24.547 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:24.551 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:24.554 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:24.557 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:24.560 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:24.563 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01077, max. 0.01077
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04921, max. 0.04124
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07598, max. 0.06383
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00152, max. 0.00136
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01719, max. 0.01976
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01077, max. 0.01077
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04922, max. 0.04119
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07005, max. 0.06372
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00152, max. 0.00154
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01574, max. 0.02023
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:24.953 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:24.955 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:24.955 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:24.956 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:24.956 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:24.956 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:26.285 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:26.346 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:26.350 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:26.353 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:26.356 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:26.359 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:26.362 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01088, max. 0.01089
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05206, max. 0.04249
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08802, max. 0.07380
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00153, max. 0.00160
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01678, max. 0.02112
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01091, max. 0.01092
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04913, max. 0.04130
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07859, max. 0.06839
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00155, max. 0.00146
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01849, max. 0.02050
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:26.651 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:26.653 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:26.653 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:26.654 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:26.654 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:26.654 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:27.994 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:28.055 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:28.059 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:28.062 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:28.065 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:28.068 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:28.071 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01132, max. 0.01132
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05009, max. 0.04196
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07866, max. 0.07595
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00160, max. 0.00150
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01776, max. 0.02234
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01132, max. 0.01132
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05010, max. 0.04179
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07682, max. 0.07522
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00159, max. 0.00150
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01757, max. 0.02272
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:28.361 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:28.362 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:28.362 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:28.363 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:28.363 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:28.363 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:29.699 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:29.760 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:29.764 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:29.767 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:29.770 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:29.773 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:29.776 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01155, max. 0.01155
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05037, max. 0.04238
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08027, max. 0.07969
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00165, max. 0.00167
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01792, max. 0.02300
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01160, max. 0.01160
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05169, max. 0.04210
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08533, max. 0.07919
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00170, max. 0.00174
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01761, max. 0.02215
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:30.066 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:30.068 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:30.068 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:30.068 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:30.068 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:30.069 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:31.420 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:31.480 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:31.483 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:31.486 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:31.489 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:31.493 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:31.496 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01200, max. 0.01200
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05291, max. 0.04257
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08733, max. 0.08391
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00178, max. 0.00180
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01853, max. 0.02337
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01195, max. 0.01196
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05177, max. 0.04272
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08230, max. 0.08559
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00171, max. 0.00164
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01904, max. 0.02474
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:31.786 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:31.788 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:31.788 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:31.789 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:31.789 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:31.789 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:33.116 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:33.272 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:33.276 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:33.279 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:33.282 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:33.285 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:33.288 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01242, max. 0.01243
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05322, max. 0.04339
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08654, max. 0.09054
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00180, max. 0.00156
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02197, max. 0.02600
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01242, max. 0.01243
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05322, max. 0.04339
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08529, max. 0.09054
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00180, max. 0.00165
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02072, max. 0.02600
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:33.582 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:33.584 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:33.584 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:33.585 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:33.585 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:33.585 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:34.912 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:34.973 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:34.977 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:34.980 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:34.983 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:34.986 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:34.989 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01251, max. 0.01252
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05346, max. 0.04415
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08379, max. 0.09184
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00181, max. 0.00173
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02059, max. 0.02688
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01250, max. 0.01251
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05326, max. 0.04405
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08728, max. 0.09073
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00183, max. 0.00183
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02021, max. 0.02607
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:35.278 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:35.279 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:35.279 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:35.280 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:35.280 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:35.280 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:36.609 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:36.670 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:36.673 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:36.677 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:36.680 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:36.683 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:36.686 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01296, max. 0.01296
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05306, max. 0.04406
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09266, max. 0.09043
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00180, max. 0.00166
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02271, max. 0.02608
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01299, max. 0.01312
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05641, max. 0.04547
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10435, max. 0.08612
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00220, max. 0.00251
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02145, max. 0.02744
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:36.975 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:36.977 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:36.977 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:36.977 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:36.978 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:36.978 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:38.315 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:38.376 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:38.379 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:38.383 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:38.386 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:38.389 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:38.392 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01352, max. 0.01352
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05377, max. 0.04518
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09227, max. 0.09242
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00198, max. 0.00179
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02121, max. 0.02771
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01368, max. 0.01368
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05499, max. 0.04578
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09015, max. 0.09568
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00201, max. 0.00191
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00010
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01996, max. 0.02524
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:38.683 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:38.685 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:38.685 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:38.685 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:38.685 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:38.685 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:40.018 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:40.075 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:40.078 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:40.080 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:40.083 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:40.085 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:40.087 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01189, max. 0.01189
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05186, max. 0.04254
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08343, max. 0.08120
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00171, max. 0.00174
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02093, max. 0.02376
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01189, max. 0.01189
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05186, max. 0.04254
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08353, max. 0.08120
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00171, max. 0.00159
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02124, max. 0.02374
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:40.461 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:40.462 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:40.462 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:40.463 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:40.463 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:40.463 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:40.822 INFO: node_feats: torch.Size([18, 2304])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:47:40.874 INFO: node_feats: torch.Size([18, 256])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:47:40.877 INFO: node_feats_out:torch.Size([18, 256]), torch.Size([2560]), 18
2024-11-06 08:47:40.880 INFO: linear_node_feats: torch.Size([18, 256])
2024-11-06 08:47:40.882 INFO: q_node_feats: torch.Size([18, 256])
2024-11-06 08:47:40.884 INFO: v_node_feats: torch.Size([18, 256])
2024-11-06 08:47:40.887 INFO: k_node_feats: torch.Size([18, 256])
q_vector from forward: torch.Size([18, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01189, max. 0.01189
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05186, max. 0.04254
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08024, max. 0.08120
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00171, max. 0.00153
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01833, max. 0.02385
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([18, 256])
2024-11-06 08:47:41.027 INFO: long_range_embedding: torch.Size([18, 256])
2024-11-06 08:47:41.029 INFO: long_range_energy: torch.Size([18, 1]), torch.Size([1])
2024-11-06 08:47:41.029 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-11-06 08:47:41.029 INFO: total_energy: torch.Size([1])
2024-11-06 08:47:41.029 INFO: node_energy_contributions: torch.Size([18, 4])
2024-11-06 08:47:41.029 INFO: node_energy: torch.Size([18])
2024-11-06 08:47:41.217 INFO: Epoch 9: head: default, loss=  0.1123, MAE_E_per_atom=     4.2 meV, MAE_F=    23.8 meV / A
2024-11-06 08:47:41.288 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:41.349 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:41.352 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:41.355 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:41.358 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:41.362 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:41.365 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01379, max. 0.01379
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05551, max. 0.04701
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10081, max. 0.09411
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00216, max. 0.00204
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00010
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02126, max. 0.02710
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01376, max. 0.01376
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05423, max. 0.04652
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09125, max. 0.09641
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00209, max. 0.00186
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00010
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02186, max. 0.02868
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:41.652 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:41.654 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:41.654 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:41.654 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:41.655 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:41.655 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:42.982 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:43.043 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:43.046 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:43.050 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:43.053 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:43.056 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:43.059 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01406, max. 0.01406
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05590, max. 0.04812
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09545, max. 0.10173
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00230, max. 0.00192
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00010
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02305, max. 0.03028
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01405, max. 0.01405
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05666, max. 0.04820
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09534, max. 0.10157
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00227, max. 0.00203
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00010
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02259, max. 0.02941
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:43.352 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:43.354 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:43.354 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:43.354 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:43.354 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:43.354 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:44.684 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:44.745 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:44.749 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:44.752 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:44.755 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:44.758 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:44.761 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01430, max. 0.01430
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05744, max. 0.05014
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09942, max. 0.11009
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00238, max. 0.00188
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00011
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02635, max. 0.03134
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01430, max. 0.01430
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05744, max. 0.05014
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09825, max. 0.11009
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00238, max. 0.00200
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00011
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02471, max. 0.03124
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:45.051 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:45.053 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:45.053 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:45.053 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:45.053 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:45.053 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:46.378 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:46.439 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:46.443 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:46.446 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:46.449 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:46.452 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:46.455 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01508, max. 0.01508
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05738, max. 0.05229
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09917, max. 0.11908
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00226, max. 0.00236
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00012
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02468, max. 0.03270
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01513, max. 0.01530
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05764, max. 0.05373
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.11554, max. 0.11002
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00279, max. 0.00324
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00012, max. 0.00011
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02613, max. 0.03402
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:46.743 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:46.745 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:46.745 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:46.745 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:46.745 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:46.745 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:48.179 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:48.239 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:48.243 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:48.246 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:48.249 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:48.252 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:48.256 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01599, max. 0.01599
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05867, max. 0.05536
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10593, max. 0.13047
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00229, max. 0.00236
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00013
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02942, max. 0.03424
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01598, max. 0.01598
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06074, max. 0.05583
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10581, max. 0.13298
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00237, max. 0.00225
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00014
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02478, max. 0.03223
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:48.547 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:48.549 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:48.549 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:48.550 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:48.550 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:48.550 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:49.905 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:49.965 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:49.969 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:49.972 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:49.975 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:49.978 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:49.981 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01678, max. 0.01708
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05889, max. 0.05917
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.12102, max. 0.12901
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00282, max. 0.00379
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00012, max. 0.00013
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02823, max. 0.03717
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01671, max. 0.01672
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05926, max. 0.05769
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10930, max. 0.13844
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00226, max. 0.00273
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00014
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02713, max. 0.03639
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:50.269 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:50.270 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:50.271 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:50.271 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:50.271 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:50.271 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:51.606 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:51.666 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:51.670 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:51.673 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:51.677 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:51.680 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:51.683 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01700, max. 0.01700
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06089, max. 0.06003
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.11489, max. 0.14607
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00236, max. 0.00297
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00012, max. 0.00015
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02799, max. 0.03766
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01700, max. 0.01700
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06162, max. 0.05994
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.11425, max. 0.14499
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00236, max. 0.00288
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00012, max. 0.00015
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02779, max. 0.03716
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:51.975 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:51.977 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:51.977 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:51.977 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:51.977 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:51.977 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:53.306 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:53.367 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:53.371 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:53.374 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:53.377 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:53.380 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:53.384 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01695, max. 0.01695
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06113, max. 0.06062
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.11553, max. 0.14686
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00235, max. 0.00265
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00012, max. 0.00015
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03225, max. 0.03738
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01694, max. 0.01694
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06326, max. 0.06108
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.11764, max. 0.14932
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00243, max. 0.00248
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00012, max. 0.00015
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02680, max. 0.03528
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:53.673 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:53.675 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:53.675 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:53.675 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:53.675 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:53.676 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:55.008 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:55.069 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:55.072 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:55.076 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:55.079 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:55.082 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:55.085 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01656, max. 0.01656
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06049, max. 0.05998
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.11261, max. 0.14265
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00227, max. 0.00256
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00015
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03084, max. 0.03691
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01656, max. 0.01656
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06049, max. 0.05998
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.11261, max. 0.14265
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00227, max. 0.00271
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00015
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03030, max. 0.03687
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:55.372 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:55.373 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:55.374 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:55.374 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:55.374 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:55.374 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:56.801 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:56.862 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:56.865 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:56.868 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:56.872 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:56.875 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:56.878 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01654, max. 0.01654
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05933, max. 0.05851
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.11409, max. 0.13187
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00221, max. 0.00283
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00012, max. 0.00013
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02573, max. 0.03376
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01651, max. 0.01651
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05871, max. 0.05843
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10715, max. 0.13475
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00216, max. 0.00275
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00014
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02675, max. 0.03598
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:57.166 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:57.168 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:57.168 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:57.168 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:57.168 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:57.169 INFO: node_energy: torch.Size([36])
2024-11-06 08:47:58.519 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:58.580 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:47:58.584 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:47:58.587 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:47:58.590 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:47:58.593 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:47:58.596 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01616, max. 0.01616
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05880, max. 0.05750
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.11260, max. 0.12810
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00217, max. 0.00275
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00013
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02525, max. 0.03301
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01610, max. 0.01610
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06037, max. 0.05813
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10754, max. 0.13464
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00217, max. 0.00232
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00014
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02463, max. 0.03219
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:47:58.885 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:47:58.886 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:47:58.887 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:47:58.887 INFO: total_energy: torch.Size([2])
2024-11-06 08:47:58.887 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:47:58.887 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:00.222 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:00.283 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:00.287 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:00.290 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:00.293 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:00.296 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:00.299 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01554, max. 0.01554
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05963, max. 0.05880
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.11920, max. 0.12170
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00296, max. 0.00356
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00012, max. 0.00012
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02805, max. 0.03666
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01546, max. 0.01546
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05893, max. 0.05729
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10597, max. 0.13169
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00220, max. 0.00247
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00013
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02640, max. 0.03528
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:00.589 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:00.591 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:00.591 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:00.592 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:00.592 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:00.592 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:01.934 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:01.995 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:01.999 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:02.002 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:02.005 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:02.008 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:02.011 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01494, max. 0.01494
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06155, max. 0.05872
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.11143, max. 0.13870
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00244, max. 0.00213
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00014
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03126, max. 0.03581
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01494, max. 0.01494
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06154, max. 0.05871
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.11141, max. 0.13867
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00244, max. 0.00227
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00014
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02755, max. 0.03672
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:02.300 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:02.301 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:02.302 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:02.302 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:02.302 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:02.302 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:03.626 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:03.687 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:03.691 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:03.694 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:03.697 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:03.700 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:03.703 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01509, max. 0.01509
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06333, max. 0.05989
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.11478, max. 0.14320
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00253, max. 0.00242
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00012, max. 0.00015
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02735, max. 0.03639
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01509, max. 0.01509
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06237, max. 0.05976
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.11482, max. 0.14347
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00253, max. 0.00235
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00012, max. 0.00015
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03083, max. 0.03676
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:04.090 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:04.092 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:04.093 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:04.093 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:04.093 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:04.093 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:05.424 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:05.485 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:05.489 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:05.492 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:05.495 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:05.498 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:05.501 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01550, max. 0.01550
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06169, max. 0.06009
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.11510, max. 0.14458
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00247, max. 0.00267
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00012, max. 0.00015
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02721, max. 0.03667
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01550, max. 0.01550
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06169, max. 0.06009
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.11510, max. 0.14458
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00247, max. 0.00233
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00012, max. 0.00015
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02986, max. 0.03652
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:05.790 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:05.792 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:05.792 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:05.793 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:05.793 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:05.793 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:07.118 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:07.175 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:07.177 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:07.180 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:07.182 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:07.185 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:07.187 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01519, max. 0.01519
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05848, max. 0.05587
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10321, max. 0.12932
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00217, max. 0.00229
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00013
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02879, max. 0.03377
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01519, max. 0.01519
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05848, max. 0.05587
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10321, max. 0.12932
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00217, max. 0.00231
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00013
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02803, max. 0.03370
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:07.466 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:07.467 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:07.467 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:07.467 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:07.468 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:07.468 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:07.825 INFO: node_feats: torch.Size([18, 2304])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:48:07.877 INFO: node_feats: torch.Size([18, 256])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:48:07.880 INFO: node_feats_out:torch.Size([18, 256]), torch.Size([2560]), 18
2024-11-06 08:48:07.883 INFO: linear_node_feats: torch.Size([18, 256])
2024-11-06 08:48:07.885 INFO: q_node_feats: torch.Size([18, 256])
2024-11-06 08:48:07.887 INFO: v_node_feats: torch.Size([18, 256])
2024-11-06 08:48:07.890 INFO: k_node_feats: torch.Size([18, 256])
q_vector from forward: torch.Size([18, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01519, max. 0.01519
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05848, max. 0.05587
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10321, max. 0.12932
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00217, max. 0.00241
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00013
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02528, max. 0.03384
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([18, 256])
2024-11-06 08:48:08.028 INFO: long_range_embedding: torch.Size([18, 256])
2024-11-06 08:48:08.030 INFO: long_range_energy: torch.Size([18, 1]), torch.Size([1])
2024-11-06 08:48:08.030 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-11-06 08:48:08.030 INFO: total_energy: torch.Size([1])
2024-11-06 08:48:08.030 INFO: node_energy_contributions: torch.Size([18, 4])
2024-11-06 08:48:08.030 INFO: node_energy: torch.Size([18])
2024-11-06 08:48:08.217 INFO: Epoch 12: head: default, loss=  0.1203, MAE_E_per_atom=     6.4 meV, MAE_F=    23.8 meV / A
2024-11-06 08:48:08.288 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:08.349 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:08.353 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:08.356 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:08.359 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:08.362 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:08.365 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01636, max. 0.01636
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05899, max. 0.05926
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.11079, max. 0.14001
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00222, max. 0.00301
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00014
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02580, max. 0.03504
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01635, max. 0.01635
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06104, max. 0.05969
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.11260, max. 0.14210
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00230, max. 0.00250
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00014
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02462, max. 0.03257
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:08.651 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:08.653 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:08.653 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:08.653 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:08.654 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:08.654 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:09.978 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:10.039 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:10.042 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:10.045 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:10.048 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:10.052 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:10.055 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01704, max. 0.01764
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05743, max. 0.05919
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.11724, max. 0.12039
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00273, max. 0.00446
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00012, max. 0.00012
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02662, max. 0.03529
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01700, max. 0.01700
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05673, max. 0.05751
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10898, max. 0.12683
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00203, max. 0.00329
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00013
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02377, max. 0.03152
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:10.347 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:10.349 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:10.349 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:10.349 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:10.349 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:10.349 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:11.700 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:11.761 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:11.764 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:11.767 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:11.770 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:11.774 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:11.777 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01693, max. 0.01693
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05496, max. 0.05736
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10136, max. 0.12902
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00185, max. 0.00301
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00013
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02359, max. 0.03240
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01693, max. 0.01693
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05492, max. 0.05736
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10138, max. 0.12905
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00194, max. 0.00280
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00013
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02644, max. 0.03175
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:12.064 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:12.066 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:12.066 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:12.067 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:12.067 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:12.067 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:13.491 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:13.551 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:13.555 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:13.558 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:13.561 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:13.564 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:13.567 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01654, max. 0.01654
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05402, max. 0.05633
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09746, max. 0.12366
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00176, max. 0.00300
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00013
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02228, max. 0.03039
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01655, max. 0.01655
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05383, max. 0.05613
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09740, max. 0.12341
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00200, max. 0.00297
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00013
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02280, max. 0.03134
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:13.857 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:13.858 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:13.859 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:13.859 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:13.859 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:13.859 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:15.189 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:15.249 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:15.253 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:15.256 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:15.259 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:15.262 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:15.266 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01514, max. 0.01514
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05165, max. 0.05393
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09347, max. 0.11598
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00162, max. 0.00257
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00012
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02336, max. 0.02925
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01514, max. 0.01514
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05165, max. 0.05393
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09201, max. 0.11598
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00162, max. 0.00239
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00012
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02296, max. 0.02931
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:15.555 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:15.557 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:15.557 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:15.557 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:15.557 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:15.558 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:16.892 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:16.953 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:16.956 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:16.959 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:16.962 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:16.965 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:16.969 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01407, max. 0.01407
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04993, max. 0.05210
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08921, max. 0.11034
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00157, max. 0.00216
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00011
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02320, max. 0.02788
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01407, max. 0.01407
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04993, max. 0.05210
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08935, max. 0.11034
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00155, max. 0.00233
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00011
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02247, max. 0.02805
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:17.256 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:17.258 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:17.258 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:17.258 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:17.258 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:17.258 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:18.575 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:18.636 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:18.640 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:18.643 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:18.646 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:18.649 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:18.652 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01339, max. 0.01339
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04899, max. 0.05098
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08702, max. 0.10812
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00153, max. 0.00239
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00011
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02020, max. 0.02752
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01342, max. 0.01342
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05042, max. 0.05041
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09264, max. 0.10174
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00159, max. 0.00243
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00010
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02017, max. 0.02639
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:18.943 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:18.945 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:18.945 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:18.945 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:18.945 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:18.945 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:20.282 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:20.343 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:20.346 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:20.350 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:20.353 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:20.356 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:20.359 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01307, max. 0.01307
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04905, max. 0.05083
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08855, max. 0.10977
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00157, max. 0.00215
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00011
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02105, max. 0.02808
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01308, max. 0.01308
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04906, max. 0.05083
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08858, max. 0.10982
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00157, max. 0.00201
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00011
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02112, max. 0.02764
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:20.746 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:20.748 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:20.748 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:20.748 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:20.749 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:20.749 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:22.074 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:22.135 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:22.138 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:22.142 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:22.145 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:22.148 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:22.151 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01301, max. 0.01301
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05095, max. 0.05131
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09183, max. 0.11358
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00167, max. 0.00198
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00012
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02049, max. 0.02587
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01305, max. 0.01305
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04904, max. 0.05085
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09016, max. 0.11134
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00163, max. 0.00222
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00011
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02066, max. 0.02824
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:22.439 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:22.441 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:22.441 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:22.441 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:22.441 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:22.441 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:23.775 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:23.836 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:23.840 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:23.843 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:23.846 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:23.849 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:23.852 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01325, max. 0.01615
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05215, max. 0.05201
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09685, max. 0.09902
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00234, max. 0.00359
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00010
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02350, max. 0.03073
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01280, max. 0.01280
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04994, max. 0.05135
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09113, max. 0.11285
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00161, max. 0.00220
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00011
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02019, max. 0.02738
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:24.142 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:24.144 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:24.144 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:24.144 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:24.144 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:24.145 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:25.497 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:25.558 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:25.562 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:25.565 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:25.568 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:25.571 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:25.574 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01261, max. 0.01261
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05041, max. 0.05209
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09223, max. 0.11472
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00166, max. 0.00206
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00012
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02104, max. 0.02890
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01260, max. 0.01260
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05028, max. 0.05216
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09238, max. 0.11530
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00164, max. 0.00187
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00012
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02151, max. 0.02846
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:25.862 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:25.864 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:25.864 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:25.864 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:25.864 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:25.864 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:27.183 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:27.244 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:27.247 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:27.250 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:27.254 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:27.257 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:27.260 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01261, max. 0.01261
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05100, max. 0.05287
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09237, max. 0.11585
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00164, max. 0.00200
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00012
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02368, max. 0.02858
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01322, max. 0.01597
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05433, max. 0.05371
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09526, max. 0.10178
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00255, max. 0.00354
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00010
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02432, max. 0.03183
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:27.551 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:27.553 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:27.553 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:27.553 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:27.553 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:27.553 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:28.996 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:29.057 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:29.060 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:29.063 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:29.066 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:29.069 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:29.073 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01301, max. 0.01301
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05272, max. 0.05429
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09421, max. 0.11904
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00172, max. 0.00207
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00012
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02322, max. 0.02984
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01301, max. 0.01301
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05268, max. 0.05430
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09424, max. 0.11909
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00172, max. 0.00225
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00012
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02214, max. 0.02945
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:29.361 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:29.363 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:29.363 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:29.363 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:29.363 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:29.363 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:30.716 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:30.776 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:30.780 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:30.783 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:30.786 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:30.789 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:30.792 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01367, max. 0.01367
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05362, max. 0.05501
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09444, max. 0.11994
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00178, max. 0.00210
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00012
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02360, max. 0.02934
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01368, max. 0.01368
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05382, max. 0.05536
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09488, max. 0.12029
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00178, max. 0.00231
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00012
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02106, max. 0.02914
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:31.081 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:31.082 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:31.082 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:31.083 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:31.083 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:31.083 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:32.411 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:32.472 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:32.476 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:32.479 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:32.482 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:32.485 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:32.488 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01424, max. 0.01424
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05378, max. 0.05531
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09410, max. 0.11958
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00189, max. 0.00230
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00012
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02149, max. 0.02745
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01430, max. 0.01430
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05494, max. 0.05417
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08859, max. 0.11080
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00191, max. 0.00283
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00011
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02153, max. 0.02884
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:32.776 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:32.778 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:32.778 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:32.778 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:32.778 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:32.779 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:34.123 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:34.180 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:34.183 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:34.185 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:34.187 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:34.190 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:34.192 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01407, max. 0.01407
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05232, max. 0.05386
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09415, max. 0.11842
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00178, max. 0.00232
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00012
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02525, max. 0.02963
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01407, max. 0.01407
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05232, max. 0.05386
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09415, max. 0.11842
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00178, max. 0.00227
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00012
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02414, max. 0.02959
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:34.472 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:34.473 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:34.474 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:34.474 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:34.474 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:34.474 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:34.832 INFO: node_feats: torch.Size([18, 2304])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:48:34.885 INFO: node_feats: torch.Size([18, 256])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:48:34.888 INFO: node_feats_out:torch.Size([18, 256]), torch.Size([2560]), 18
2024-11-06 08:48:34.890 INFO: linear_node_feats: torch.Size([18, 256])
2024-11-06 08:48:34.893 INFO: q_node_feats: torch.Size([18, 256])
2024-11-06 08:48:34.895 INFO: v_node_feats: torch.Size([18, 256])
2024-11-06 08:48:34.898 INFO: k_node_feats: torch.Size([18, 256])
q_vector from forward: torch.Size([18, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01407, max. 0.01407
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05232, max. 0.05386
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09415, max. 0.11842
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00178, max. 0.00237
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00012
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02165, max. 0.02973
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([18, 256])
2024-11-06 08:48:35.037 INFO: long_range_embedding: torch.Size([18, 256])
2024-11-06 08:48:35.039 INFO: long_range_energy: torch.Size([18, 1]), torch.Size([1])
2024-11-06 08:48:35.039 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-11-06 08:48:35.039 INFO: total_energy: torch.Size([1])
2024-11-06 08:48:35.039 INFO: node_energy_contributions: torch.Size([18, 4])
2024-11-06 08:48:35.039 INFO: node_energy: torch.Size([18])
2024-11-06 08:48:35.226 INFO: Epoch 15: head: default, loss=  0.0898, MAE_E_per_atom=     7.3 meV, MAE_F=    20.7 meV / A
2024-11-06 08:48:35.771 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:35.831 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:35.835 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:35.838 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:35.841 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:35.844 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:35.848 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01467, max. 0.01467
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05512, max. 0.05543
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09350, max. 0.11914
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00193, max. 0.00265
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00012
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02074, max. 0.02893
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01466, max. 0.01466
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05492, max. 0.05505
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09309, max. 0.11882
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00194, max. 0.00280
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00012
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02173, max. 0.02949
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:36.135 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:36.136 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:36.137 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:36.137 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:36.137 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:36.137 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:37.453 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:37.514 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:37.517 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:37.520 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:37.523 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:37.527 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:37.530 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01479, max. 0.01479
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05501, max. 0.05476
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09227, max. 0.11747
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00206, max. 0.00258
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00012
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02109, max. 0.02725
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01481, max. 0.01481
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05520, max. 0.05457
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09139, max. 0.11673
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00200, max. 0.00249
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00012
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02087, max. 0.02907
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:37.923 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:37.925 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:37.925 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:37.925 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:37.925 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:37.925 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:39.260 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:39.321 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:39.325 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:39.328 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:39.331 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:39.334 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:39.338 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01482, max. 0.01482
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05537, max. 0.05419
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08949, max. 0.11441
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00204, max. 0.00274
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00012
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02251, max. 0.02850
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01482, max. 0.01482
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05540, max. 0.05417
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08944, max. 0.11436
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00204, max. 0.00275
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00012
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02123, max. 0.02892
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:39.626 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:39.628 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:39.628 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:39.629 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:39.629 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:39.629 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:40.954 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:41.016 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:41.019 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:41.022 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:41.025 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:41.028 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:41.032 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01487, max. 0.01487
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05725, max. 0.05340
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09293, max. 0.10644
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00214, max. 0.00344
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00011
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02158, max. 0.02911
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01483, max. 0.01483
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05553, max. 0.05439
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08974, max. 0.11481
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00206, max. 0.00261
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00012
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02204, max. 0.02846
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:41.321 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:41.323 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:41.323 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:41.323 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:41.324 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:41.324 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:42.659 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:42.720 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:42.724 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:42.727 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:42.730 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:42.733 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:42.736 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01628, max. 0.01985
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06045, max. 0.05708
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10278, max. 0.10556
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00313, max. 0.00530
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00011
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02624, max. 0.03505
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01525, max. 0.01525
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05659, max. 0.05573
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09394, max. 0.11988
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00211, max. 0.00298
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00012
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02073, max. 0.02975
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:43.026 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:43.027 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:43.028 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:43.028 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:43.028 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:43.028 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:44.380 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:44.441 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:44.444 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:44.448 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:44.451 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:44.454 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:44.457 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01552, max. 0.01552
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05730, max. 0.05717
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09929, max. 0.12681
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00212, max. 0.00299
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00013
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02316, max. 0.03070
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01557, max. 0.01557
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05762, max. 0.05616
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09275, max. 0.11792
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00219, max. 0.00369
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00012
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02267, max. 0.03072
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:44.747 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:44.749 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:44.749 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:44.749 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:44.749 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:44.750 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:46.189 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:46.250 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:46.253 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:46.257 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:46.260 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:46.263 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:46.266 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01562, max. 0.01562
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05793, max. 0.05807
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10265, max. 0.13037
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00213, max. 0.00305
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00013
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02226, max. 0.03123
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01560, max. 0.01560
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05771, max. 0.05804
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10259, max. 0.13070
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00211, max. 0.00328
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00013
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02450, max. 0.03091
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:46.557 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:46.559 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:46.559 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:46.559 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:46.559 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:46.560 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:47.888 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:47.949 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:47.952 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:47.955 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:47.958 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:47.962 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:47.965 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01507, max. 0.01507
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05612, max. 0.05708
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09720, max. 0.12380
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00195, max. 0.00280
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00013
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02330, max. 0.02925
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01504, max. 0.01504
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05598, max. 0.05710
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09724, max. 0.12342
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00200, max. 0.00281
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00013
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02175, max. 0.02776
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:48.253 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:48.255 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:48.255 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:48.255 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:48.255 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:48.255 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:49.587 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:49.647 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:49.651 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:49.654 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:49.657 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:49.660 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:49.663 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01440, max. 0.01440
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05423, max. 0.05548
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08805, max. 0.11264
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00177, max. 0.00241
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00011
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02002, max. 0.02723
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01440, max. 0.01440
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05423, max. 0.05548
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08805, max. 0.11264
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00177, max. 0.00243
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00011
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02046, max. 0.02702
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:49.971 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:49.973 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:49.973 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:49.973 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:49.974 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:49.974 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:51.299 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:51.360 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:51.363 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:51.366 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:51.369 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:51.373 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:51.376 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01439, max. 0.01439
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05302, max. 0.05542
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07922, max. 0.10274
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00156, max. 0.00259
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00010
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01782, max. 0.02428
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01558, max. 0.01842
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05909, max. 0.05548
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.09487, max. 0.08340
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00271, max. 0.00471
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02181, max. 0.02977
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:51.665 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:51.668 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:51.668 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:51.668 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:51.668 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:51.668 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:53.022 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:53.082 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:53.086 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:53.089 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:53.092 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:53.095 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:53.098 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01486, max. 0.01486
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05186, max. 0.05488
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07648, max. 0.10003
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00147, max. 0.00238
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00010
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01744, max. 0.02414
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01486, max. 0.01486
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05186, max. 0.05488
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07648, max. 0.10003
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00147, max. 0.00256
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00010
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01832, max. 0.02395
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:53.386 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:53.388 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:53.388 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:53.388 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:53.388 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:53.389 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:54.804 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:54.865 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:54.868 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:54.871 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:54.875 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:54.878 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:54.881 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01484, max. 0.01484
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05142, max. 0.05444
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07589, max. 0.09868
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00155, max. 0.00256
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00010
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01733, max. 0.02260
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01493, max. 0.01493
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05330, max. 0.05329
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08398, max. 0.08906
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00166, max. 0.00323
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01740, max. 0.02452
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:55.173 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:55.174 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:55.175 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:55.175 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:55.175 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:55.175 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:56.521 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:56.582 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:56.586 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:56.589 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:56.592 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:56.595 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:56.599 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01451, max. 0.01451
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05223, max. 0.05395
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08031, max. 0.10393
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00155, max. 0.00228
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00011
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01885, max. 0.02464
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01453, max. 0.01453
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05252, max. 0.05406
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08056, max. 0.10386
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00167, max. 0.00251
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00011
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01780, max. 0.02479
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:56.886 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:56.888 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:56.888 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:56.888 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:56.888 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:56.888 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:58.215 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:58.276 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:58.279 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:58.282 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:58.285 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:58.288 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:58.292 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01513, max. 0.01754
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05658, max. 0.05350
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08545, max. 0.08907
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00238, max. 0.00430
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02302, max. 0.02950
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01393, max. 0.01393
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05247, max. 0.05294
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08245, max. 0.10582
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00137, max. 0.00254
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00011
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02020, max. 0.02487
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:48:58.580 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:48:58.581 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:48:58.582 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:48:58.582 INFO: total_energy: torch.Size([2])
2024-11-06 08:48:58.582 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:48:58.582 INFO: node_energy: torch.Size([36])
2024-11-06 08:48:59.919 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:59.980 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:48:59.983 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:48:59.986 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:48:59.989 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:48:59.992 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:48:59.995 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01410, max. 0.01410
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05303, max. 0.05278
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08476, max. 0.10854
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00140, max. 0.00236
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00011
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02000, max. 0.02624
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01411, max. 0.01411
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05353, max. 0.05343
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08579, max. 0.10964
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00138, max. 0.00240
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00011
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01856, max. 0.02471
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:00.284 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:00.286 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:00.286 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:00.286 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:00.286 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:00.287 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:01.747 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:01.804 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:01.806 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:01.809 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:01.811 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:01.813 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:01.816 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01446, max. 0.01446
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05313, max. 0.05420
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08741, max. 0.11171
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00164, max. 0.00255
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00011
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02245, max. 0.02689
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01446, max. 0.01446
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05313, max. 0.05420
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08741, max. 0.11171
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00164, max. 0.00246
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00011
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02121, max. 0.02687
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:02.096 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:02.097 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:02.097 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:02.097 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:02.098 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:02.098 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:02.455 INFO: node_feats: torch.Size([18, 2304])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:49:02.507 INFO: node_feats: torch.Size([18, 256])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:49:02.510 INFO: node_feats_out:torch.Size([18, 256]), torch.Size([2560]), 18
2024-11-06 08:49:02.512 INFO: linear_node_feats: torch.Size([18, 256])
2024-11-06 08:49:02.515 INFO: q_node_feats: torch.Size([18, 256])
2024-11-06 08:49:02.517 INFO: v_node_feats: torch.Size([18, 256])
2024-11-06 08:49:02.520 INFO: k_node_feats: torch.Size([18, 256])
q_vector from forward: torch.Size([18, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01446, max. 0.01446
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05313, max. 0.05420
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08741, max. 0.11171
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00164, max. 0.00257
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00011
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01889, max. 0.02703
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([18, 256])
2024-11-06 08:49:02.660 INFO: long_range_embedding: torch.Size([18, 256])
2024-11-06 08:49:02.661 INFO: long_range_energy: torch.Size([18, 1]), torch.Size([1])
2024-11-06 08:49:02.661 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-11-06 08:49:02.662 INFO: total_energy: torch.Size([1])
2024-11-06 08:49:02.662 INFO: node_energy_contributions: torch.Size([18, 4])
2024-11-06 08:49:02.662 INFO: node_energy: torch.Size([18])
2024-11-06 08:49:02.848 INFO: Epoch 18: head: default, loss=  0.0722, MAE_E_per_atom=     7.3 meV, MAE_F=    18.6 meV / A
2024-11-06 08:49:03.327 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:03.388 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:03.392 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:03.395 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:03.398 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:03.401 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:03.404 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01433, max. 0.01433
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05307, max. 0.05108
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07671, max. 0.09728
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00150, max. 0.00316
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00010
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01950, max. 0.02552
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01501, max. 0.01758
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05622, max. 0.05301
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08224, max. 0.09154
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00231, max. 0.00434
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02383, max. 0.03100
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:03.692 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:03.694 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:03.694 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:03.694 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:03.694 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:03.695 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:05.033 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:05.094 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:05.098 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:05.101 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:05.104 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:05.107 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:05.110 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01502, max. 0.01503
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05320, max. 0.05265
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08271, max. 0.10699
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00161, max. 0.00232
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00011
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01988, max. 0.02574
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01503, max. 0.01503
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05320, max. 0.05265
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08271, max. 0.10699
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00143, max. 0.00268
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00011
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02046, max. 0.02576
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:05.405 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:05.406 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:05.406 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:05.407 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:05.407 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:05.407 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:06.734 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:06.795 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:06.799 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:06.802 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:06.805 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:06.808 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:06.811 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01574, max. 0.01574
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05236, max. 0.05217
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07760, max. 0.10149
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00158, max. 0.00259
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00010
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01916, max. 0.02542
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01574, max. 0.01574
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05234, max. 0.05219
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07765, max. 0.10156
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00146, max. 0.00243
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00010
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01760, max. 0.02506
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:07.102 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:07.104 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:07.104 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:07.104 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:07.104 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:07.105 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:08.430 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:08.491 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:08.495 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:08.498 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:08.501 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:08.504 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:08.507 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01600, max. 0.01600
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05049, max. 0.05045
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06987, max. 0.08931
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00163, max. 0.00271
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01694, max. 0.02055
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01604, max. 0.01604
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05034, max. 0.05071
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07065, max. 0.09144
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00145, max. 0.00258
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01776, max. 0.02326
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:08.797 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:08.799 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:08.799 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:08.799 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:08.800 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:08.800 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:10.136 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:10.198 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:10.201 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:10.204 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:10.208 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:10.211 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:10.214 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01570, max. 0.01570
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04923, max. 0.04866
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07147, max. 0.07998
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00173, max. 0.00250
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01680, max. 0.02084
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01569, max. 0.01569
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04905, max. 0.04911
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06355, max. 0.08061
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00153, max. 0.00250
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01720, max. 0.01919
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:10.604 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:10.605 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:10.606 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:10.606 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:10.606 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:10.606 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:11.946 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:12.007 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:12.010 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:12.013 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:12.017 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:12.020 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:12.023 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01454, max. 0.01454
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04792, max. 0.04606
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06799, max. 0.06958
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00152, max. 0.00221
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01493, max. 0.01868
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01452, max. 0.01453
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04790, max. 0.04591
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06525, max. 0.06949
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00123, max. 0.00203
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01218, max. 0.01899
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:12.312 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:12.314 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:12.314 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:12.314 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:12.314 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:12.314 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:13.631 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:13.692 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:13.696 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:13.699 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:13.702 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:13.705 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:13.708 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01359, max. 0.01359
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04947, max. 0.04351
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07470, max. 0.06491
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00129, max. 0.00274
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01538, max. 0.02024
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01363, max. 0.01662
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05221, max. 0.04832
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07985, max. 0.06966
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00169, max. 0.00389
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02009, max. 0.02622
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:14.001 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:14.002 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:14.003 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:14.003 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:14.003 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:14.003 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:15.354 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:15.415 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:15.418 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:15.421 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:15.425 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:15.428 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:15.431 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01291, max. 0.01291
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04640, max. 0.04476
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06258, max. 0.06721
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00116, max. 0.00179
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01302, max. 0.01882
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01291, max. 0.01292
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04617, max. 0.04549
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05709, max. 0.06851
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00123, max. 0.00185
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01390, max. 0.01700
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:15.719 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:15.721 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:15.721 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:15.721 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:15.722 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:15.722 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:17.050 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:17.111 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:17.115 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:17.118 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:17.121 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:17.124 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:17.127 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01266, max. 0.01266
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04641, max. 0.04633
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06115, max. 0.07514
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00126, max. 0.00165
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01419, max. 0.02040
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01266, max. 0.01266
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04641, max. 0.04633
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06124, max. 0.07514
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00118, max. 0.00177
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01536, max. 0.02051
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:17.415 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:17.417 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:17.417 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:17.417 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:17.417 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:17.417 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:18.844 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:18.906 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:18.909 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:18.912 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:18.916 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:18.919 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:18.922 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01301, max. 0.01301
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04825, max. 0.04885
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06751, max. 0.08742
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00124, max. 0.00197
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01759, max. 0.02309
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01296, max. 0.01296
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04849, max. 0.04880
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06718, max. 0.08621
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00131, max. 0.00199
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01546, max. 0.02072
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:19.211 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:19.213 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:19.213 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:19.213 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:19.213 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:19.214 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:20.555 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:20.616 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:20.619 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:20.623 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:20.626 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:20.629 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:20.632 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01373, max. 0.01373
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05101, max. 0.05060
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07159, max. 0.09073
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00146, max. 0.00279
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02051, max. 0.02694
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01367, max. 0.01367
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05010, max. 0.05131
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07667, max. 0.09945
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00120, max. 0.00203
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00010
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02076, max. 0.02605
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:20.921 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:20.923 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:20.923 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:20.923 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:20.924 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:20.924 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:22.252 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:22.313 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:22.317 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:22.320 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:22.323 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:22.326 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:22.329 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01445, max. 0.01445
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05123, max. 0.05303
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08256, max. 0.10740
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00147, max. 0.00207
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00011
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02066, max. 0.02779
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01445, max. 0.01446
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05190, max. 0.05366
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08323, max. 0.10802
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00135, max. 0.00227
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00011
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02017, max. 0.02580
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:22.622 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:22.624 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:22.624 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:22.624 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:22.624 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:22.624 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:23.955 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:24.015 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:24.019 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:24.022 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:24.025 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:24.028 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:24.031 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01480, max. 0.01480
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05084, max. 0.05281
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08049, max. 0.10419
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00143, max. 0.00240
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00011
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01864, max. 0.02496
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01485, max. 0.01485
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05073, max. 0.05298
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08174, max. 0.10662
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00137, max. 0.00243
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00011
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02109, max. 0.02781
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:24.320 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:24.322 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:24.322 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:24.322 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:24.322 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:24.323 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:25.755 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:25.816 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:25.819 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:25.823 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:25.826 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:25.829 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:25.832 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01488, max. 0.01488
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04924, max. 0.05172
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07621, max. 0.09964
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00135, max. 0.00217
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00010
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01753, max. 0.02651
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01499, max. 0.01795
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05135, max. 0.05332
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08193, max. 0.08561
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00173, max. 0.00428
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02564, max. 0.03385
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:26.120 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:26.122 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:26.122 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:26.122 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:26.122 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:26.123 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:27.459 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:27.520 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:27.524 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:27.527 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:27.530 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:27.533 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:27.536 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01460, max. 0.01460
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04795, max. 0.05029
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06790, max. 0.08880
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00159, max. 0.00230
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01821, max. 0.02397
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01458, max. 0.01458
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04758, max. 0.05008
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06751, max. 0.08864
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00142, max. 0.00222
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01845, max. 0.02448
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:27.827 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:27.828 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:27.828 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:27.829 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:27.829 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:27.829 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:29.152 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:29.209 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:29.212 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:29.215 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:29.217 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:29.219 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:29.222 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01427, max. 0.01428
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04952, max. 0.05075
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07287, max. 0.09460
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00136, max. 0.00228
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00010
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01933, max. 0.02372
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01428, max. 0.01428
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04952, max. 0.05075
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07287, max. 0.09460
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00130, max. 0.00218
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00010
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01830, max. 0.02369
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:29.502 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:29.504 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:29.504 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:29.504 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:29.504 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:29.504 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:29.880 INFO: node_feats: torch.Size([18, 2304])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:49:29.933 INFO: node_feats: torch.Size([18, 256])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:49:29.936 INFO: node_feats_out:torch.Size([18, 256]), torch.Size([2560]), 18
2024-11-06 08:49:29.939 INFO: linear_node_feats: torch.Size([18, 256])
2024-11-06 08:49:29.941 INFO: q_node_feats: torch.Size([18, 256])
2024-11-06 08:49:29.943 INFO: v_node_feats: torch.Size([18, 256])
2024-11-06 08:49:29.946 INFO: k_node_feats: torch.Size([18, 256])
q_vector from forward: torch.Size([18, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01428, max. 0.01428
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04952, max. 0.05075
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07287, max. 0.09460
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00145, max. 0.00231
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00010
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01585, max. 0.02367
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([18, 256])
2024-11-06 08:49:30.085 INFO: long_range_embedding: torch.Size([18, 256])
2024-11-06 08:49:30.086 INFO: long_range_energy: torch.Size([18, 1]), torch.Size([1])
2024-11-06 08:49:30.086 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-11-06 08:49:30.087 INFO: total_energy: torch.Size([1])
2024-11-06 08:49:30.087 INFO: node_energy_contributions: torch.Size([18, 4])
2024-11-06 08:49:30.087 INFO: node_energy: torch.Size([18])
2024-11-06 08:49:30.274 INFO: Epoch 21: head: default, loss=  0.0620, MAE_E_per_atom=     6.1 meV, MAE_F=    16.7 meV / A
2024-11-06 08:49:30.592 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:30.653 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:30.656 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:30.660 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:30.663 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:30.666 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:30.669 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01360, max. 0.01360
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04546, max. 0.04750
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06408, max. 0.07599
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00114, max. 0.00204
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01584, max. 0.02124
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01362, max. 0.01362
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04586, max. 0.04770
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06264, max. 0.07610
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00144, max. 0.00210
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01604, max. 0.02095
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:30.956 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:30.957 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:30.957 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:30.958 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:30.958 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:30.958 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:32.271 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:32.332 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:32.336 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:32.339 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:32.342 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:32.345 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:32.348 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01252, max. 0.01252
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04399, max. 0.04535
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06030, max. 0.06772
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00120, max. 0.00171
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01290, max. 0.01903
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01247, max. 0.01247
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04477, max. 0.04516
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06020, max. 0.06451
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00118, max. 0.00199
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01259, max. 0.01628
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:32.642 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:32.643 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:32.643 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:32.644 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:32.644 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:32.644 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:33.977 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:34.134 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:34.137 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:34.141 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:34.144 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:34.147 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:34.150 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01186, max. 0.01186
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04380, max. 0.04457
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05825, max. 0.06654
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00111, max. 0.00171
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01347, max. 0.01876
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01194, max. 0.01194
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04769, max. 0.04446
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06767, max. 0.06089
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00130, max. 0.00251
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01362, max. 0.01834
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:34.442 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:34.444 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:34.444 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:34.444 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:34.444 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:34.444 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:35.781 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:35.842 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:35.846 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:35.849 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:35.852 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:35.855 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:35.858 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01453, max. 0.01617
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05108, max. 0.05083
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07334, max. 0.06487
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00162, max. 0.00370
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02038, max. 0.02515
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01174, max. 0.01174
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04430, max. 0.04518
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05592, max. 0.07231
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00108, max. 0.00161
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01280, max. 0.01979
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:36.147 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:36.149 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:36.149 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:36.149 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:36.149 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:36.149 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:37.486 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:37.547 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:37.550 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:37.554 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:37.557 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:37.560 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:37.563 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01205, max. 0.01205
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04556, max. 0.04714
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06189, max. 0.08000
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00111, max. 0.00182
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01551, max. 0.01951
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01205, max. 0.01205
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04495, max. 0.04656
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06184, max. 0.08009
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00112, max. 0.00189
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01617, max. 0.02160
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:37.852 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:37.854 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:37.854 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:37.854 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:37.855 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:37.855 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:39.194 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:39.254 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:39.258 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:39.261 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:39.264 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:39.267 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:39.270 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01246, max. 0.01246
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04619, max. 0.04857
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06745, max. 0.08733
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00108, max. 0.00189
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01688, max. 0.02132
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01241, max. 0.01241
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04568, max. 0.04779
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06506, max. 0.08371
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00116, max. 0.00195
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01507, max. 0.02017
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:39.559 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:39.560 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:39.560 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:39.561 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:39.561 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:39.561 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:40.888 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:40.949 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:40.952 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:40.955 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:40.959 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:40.962 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:40.965 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01271, max. 0.01271
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04701, max. 0.04815
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06417, max. 0.07993
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00132, max. 0.00261
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01741, max. 0.02372
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01263, max. 0.01263
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04600, max. 0.04869
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07027, max. 0.09117
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00109, max. 0.00200
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01847, max. 0.02427
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:41.257 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:41.258 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:41.259 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:41.259 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:41.259 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:41.259 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:42.696 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:42.757 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:42.760 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:42.763 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:42.766 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:42.769 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:42.773 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01279, max. 0.01279
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04591, max. 0.04896
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07036, max. 0.09141
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00122, max. 0.00175
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01808, max. 0.02442
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01279, max. 0.01279
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04591, max. 0.04896
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07036, max. 0.09141
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00105, max. 0.00188
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01976, max. 0.02456
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:43.061 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:43.063 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:43.063 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:43.063 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:43.063 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:43.063 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:44.390 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:44.450 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:44.454 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:44.457 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:44.460 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:44.463 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:44.466 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01279, max. 0.01279
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04556, max. 0.04870
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06787, max. 0.08787
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00131, max. 0.00192
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01808, max. 0.02355
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01277, max. 0.01277
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04514, max. 0.04848
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06751, max. 0.08775
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00109, max. 0.00175
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01551, max. 0.02378
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:44.754 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:44.756 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:44.756 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:44.757 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:44.757 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:44.757 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:46.082 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:46.144 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:46.147 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:46.150 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:46.154 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:46.157 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:46.160 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01414, max. 0.01666
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04956, max. 0.04913
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07135, max. 0.06987
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00157, max. 0.00373
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02323, max. 0.02942
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01242, max. 0.01242
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04388, max. 0.04718
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06233, max. 0.08083
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00111, max. 0.00178
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01741, max. 0.02244
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:46.448 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:46.450 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:46.450 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:46.450 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:46.450 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:46.450 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:47.798 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:47.859 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:47.862 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:47.866 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:47.869 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:47.872 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:47.875 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01393, max. 0.01625
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05025, max. 0.04948
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07022, max. 0.06599
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00155, max. 0.00361
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02162, max. 0.02692
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01184, max. 0.01184
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04273, max. 0.04568
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05596, max. 0.07229
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00101, max. 0.00167
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01563, max. 0.02022
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:48.165 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:48.166 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:48.166 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:48.167 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:48.167 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:48.167 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:49.499 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:49.560 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:49.563 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:49.566 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:49.569 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:49.572 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:49.576 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01132, max. 0.01132
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04204, max. 0.04448
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05356, max. 0.06482
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00110, max. 0.00170
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01306, max. 0.01850
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01132, max. 0.01132
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04204, max. 0.04448
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05049, max. 0.06482
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00102, max. 0.00149
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01188, max. 0.01854
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:49.886 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:49.888 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:49.888 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:49.888 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:49.889 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:49.889 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:51.316 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:51.376 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:51.380 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:51.383 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:51.386 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:51.389 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:51.392 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01117, max. 0.01117
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04205, max. 0.04420
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05284, max. 0.06278
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00108, max. 0.00154
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01324, max. 0.01825
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01128, max. 0.01128
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04583, max. 0.04474
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06205, max. 0.05652
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00128, max. 0.00228
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01226, max. 0.01662
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:51.681 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:51.684 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:51.684 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:51.684 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:51.684 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:51.684 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:53.021 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:53.082 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:53.085 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:53.089 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:53.092 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:53.095 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:53.098 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01124, max. 0.01124
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04291, max. 0.04485
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05225, max. 0.06603
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00114, max. 0.00160
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01353, max. 0.01846
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01118, max. 0.01118
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04253, max. 0.04432
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05248, max. 0.06014
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00116, max. 0.00171
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01155, max. 0.01506
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:53.387 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:53.389 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:53.389 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:53.389 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:53.389 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:53.390 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:54.723 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:54.784 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:54.788 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:54.791 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:54.794 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:54.797 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:54.800 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01120, max. 0.01120
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04357, max. 0.04528
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05691, max. 0.07246
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00117, max. 0.00146
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01471, max. 0.02004
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01119, max. 0.01120
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04411, max. 0.04576
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05594, max. 0.07115
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00114, max. 0.00163
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01407, max. 0.01758
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:55.087 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:55.089 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:55.089 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:55.090 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:55.090 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:55.090 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:56.420 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:56.477 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:56.480 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:56.482 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:56.484 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:56.487 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:56.489 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01264, max. 0.01264
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04563, max. 0.04765
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06297, max. 0.08155
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00113, max. 0.00191
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01711, max. 0.02161
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01264, max. 0.01264
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04563, max. 0.04765
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06297, max. 0.08155
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00115, max. 0.00182
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01620, max. 0.02160
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:56.768 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:56.769 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:56.769 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:56.769 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:56.769 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:56.770 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:57.127 INFO: node_feats: torch.Size([18, 2304])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:49:57.180 INFO: node_feats: torch.Size([18, 256])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:49:57.183 INFO: node_feats_out:torch.Size([18, 256]), torch.Size([2560]), 18
2024-11-06 08:49:57.185 INFO: linear_node_feats: torch.Size([18, 256])
2024-11-06 08:49:57.187 INFO: q_node_feats: torch.Size([18, 256])
2024-11-06 08:49:57.190 INFO: v_node_feats: torch.Size([18, 256])
2024-11-06 08:49:57.192 INFO: k_node_feats: torch.Size([18, 256])
q_vector from forward: torch.Size([18, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01264, max. 0.01264
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04563, max. 0.04765
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06297, max. 0.08155
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00119, max. 0.00193
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01409, max. 0.02158
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([18, 256])
2024-11-06 08:49:57.331 INFO: long_range_embedding: torch.Size([18, 256])
2024-11-06 08:49:57.332 INFO: long_range_energy: torch.Size([18, 1]), torch.Size([1])
2024-11-06 08:49:57.333 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-11-06 08:49:57.333 INFO: total_energy: torch.Size([1])
2024-11-06 08:49:57.333 INFO: node_energy_contributions: torch.Size([18, 4])
2024-11-06 08:49:57.333 INFO: node_energy: torch.Size([18])
2024-11-06 08:49:57.521 INFO: Epoch 24: head: default, loss=  0.0529, MAE_E_per_atom=     5.7 meV, MAE_F=    15.3 meV / A
2024-11-06 08:49:57.836 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:57.897 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:57.901 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:57.904 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:57.907 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:57.910 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:57.913 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01390, max. 0.01604
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05250, max. 0.05201
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07061, max. 0.06319
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00186, max. 0.00351
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02044, max. 0.02408
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01124, max. 0.01124
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04474, max. 0.04614
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06309, max. 0.08005
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00111, max. 0.00159
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01724, max. 0.02191
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:58.203 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:58.204 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:58.205 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:58.205 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:58.205 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:58.205 INFO: node_energy: torch.Size([36])
2024-11-06 08:49:59.626 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:59.686 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:49:59.690 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:49:59.693 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:49:59.696 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:49:59.699 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:49:59.702 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01134, max. 0.01134
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04505, max. 0.04692
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06556, max. 0.08296
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00112, max. 0.00166
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01549, max. 0.02000
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01134, max. 0.01135
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04452, max. 0.04645
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06662, max. 0.08440
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00107, max. 0.00160
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01848, max. 0.02275
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:49:59.996 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:49:59.998 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:49:59.998 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:49:59.998 INFO: total_energy: torch.Size([2])
2024-11-06 08:49:59.998 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:49:59.998 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:01.327 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:01.389 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:01.392 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:01.395 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:01.398 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:01.401 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:01.404 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01134, max. 0.01134
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04381, max. 0.04626
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06755, max. 0.08539
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00111, max. 0.00171
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01770, max. 0.02287
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01147, max. 0.01147
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04440, max. 0.04548
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06188, max. 0.06923
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00130, max. 0.00225
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01452, max. 0.01997
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:01.693 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:01.695 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:01.695 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:01.696 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:01.696 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:01.696 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:03.046 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:03.107 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:03.110 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:03.113 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:03.116 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:03.120 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:03.123 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01143, max. 0.01143
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04252, max. 0.04582
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06619, max. 0.08366
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00110, max. 0.00145
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00009
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01692, max. 0.02268
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01139, max. 0.01139
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04242, max. 0.04547
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06206, max. 0.07736
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00112, max. 0.00169
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01434, max. 0.01870
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:03.410 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:03.411 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:03.412 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:03.412 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:03.412 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:03.412 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:04.746 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:04.806 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:04.810 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:04.813 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:04.816 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:04.820 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:04.823 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01147, max. 0.01148
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04123, max. 0.04522
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06329, max. 0.08018
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00098, max. 0.00145
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01449, max. 0.02209
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01149, max. 0.01149
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04163, max. 0.04543
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06349, max. 0.08014
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00107, max. 0.00159
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01670, max. 0.02172
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:05.111 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:05.113 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:05.113 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:05.113 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:05.113 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:05.114 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:06.551 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:06.612 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:06.615 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:06.619 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:06.622 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:06.625 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:06.628 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01138, max. 0.01138
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04002, max. 0.04418
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05610, max. 0.06997
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00104, max. 0.00163
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01292, max. 0.01785
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01140, max. 0.01140
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04047, max. 0.04499
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05902, max. 0.07474
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00105, max. 0.00156
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01478, max. 0.01892
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:06.916 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:06.917 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:06.917 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:06.918 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:06.918 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:06.918 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:08.245 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:08.305 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:08.309 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:08.312 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:08.315 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:08.318 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:08.321 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01112, max. 0.01113
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.03917, max. 0.04376
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05638, max. 0.07177
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00103, max. 0.00157
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01482, max. 0.02006
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01112, max. 0.01112
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.03920, max. 0.04374
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05634, max. 0.07172
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00100, max. 0.00144
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01578, max. 0.02028
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:08.614 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:08.615 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:08.616 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:08.616 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:08.616 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:08.616 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:09.941 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:10.002 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:10.005 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:10.009 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:10.012 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:10.015 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:10.018 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01098, max. 0.01098
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.03921, max. 0.04362
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05525, max. 0.07015
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00103, max. 0.00145
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01471, max. 0.01931
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01097, max. 0.01097
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.03879, max. 0.04340
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05494, max. 0.07009
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00098, max. 0.00141
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01557, max. 0.01965
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:10.306 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:10.308 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:10.308 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:10.309 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:10.309 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:10.309 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:11.637 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:11.698 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:11.701 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:11.704 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:11.708 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:11.711 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:11.714 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01275, max. 0.01502
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04890, max. 0.04868
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06508, max. 0.06062
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00153, max. 0.00299
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02032, max. 0.02470
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01084, max. 0.01085
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.03883, max. 0.04342
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05547, max. 0.07090
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00090, max. 0.00130
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01287, max. 0.01962
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:12.002 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:12.003 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:12.004 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:12.004 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:12.004 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:12.004 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:13.340 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:13.401 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:13.405 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:13.408 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:13.411 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:13.414 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:13.417 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01097, max. 0.01097
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04360, max. 0.04307
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05628, max. 0.05773
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00112, max. 0.00206
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01480, max. 0.01988
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01082, max. 0.01083
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.03967, max. 0.04394
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05658, max. 0.07252
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00104, max. 0.00130
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01488, max. 0.01985
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:13.803 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:13.805 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:13.805 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:13.805 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:13.805 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:13.806 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:15.152 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:15.212 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:15.216 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:15.219 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:15.222 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:15.225 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:15.228 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01115, max. 0.01115
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04422, max. 0.04381
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05643, max. 0.05974
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00116, max. 0.00213
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01518, max. 0.02037
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01100, max. 0.01101
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04066, max. 0.04466
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05850, max. 0.07523
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00107, max. 0.00134
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01547, max. 0.02042
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:15.516 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:15.518 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:15.518 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:15.518 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:15.519 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:15.519 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:16.844 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:16.905 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:16.909 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:16.912 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:16.915 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:16.918 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:16.921 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01131, max. 0.01132
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04150, max. 0.04533
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06047, max. 0.07800
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00107, max. 0.00164
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01629, max. 0.02107
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01131, max. 0.01132
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04150, max. 0.04533
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06047, max. 0.07800
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00103, max. 0.00150
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01745, max. 0.02119
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:17.214 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:17.215 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:17.216 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:17.216 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:17.216 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:17.216 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:18.542 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:18.603 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:18.607 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:18.610 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:18.613 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:18.616 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:18.619 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01168, max. 0.01169
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04186, max. 0.04578
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06191, max. 0.07988
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00098, max. 0.00149
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01445, max. 0.02178
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01168, max. 0.01168
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04248, max. 0.04625
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06105, max. 0.07837
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00110, max. 0.00166
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01525, max. 0.01970
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:18.907 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:18.909 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:18.909 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:18.909 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:18.909 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:18.909 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:20.237 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:20.298 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:20.302 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:20.305 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:20.308 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:20.311 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:20.314 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01188, max. 0.01188
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04226, max. 0.04604
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06225, max. 0.07979
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00112, max. 0.00167
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01714, max. 0.02167
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01186, max. 0.01186
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04184, max. 0.04576
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06190, max. 0.07968
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00105, max. 0.00162
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01801, max. 0.02219
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:20.602 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:20.604 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:20.604 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:20.605 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:20.605 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:20.605 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:21.932 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:22.088 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:22.091 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:22.095 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:22.098 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:22.101 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:22.104 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01157, max. 0.01157
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04244, max. 0.04495
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05715, max. 0.07117
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00111, max. 0.00179
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01336, max. 0.01895
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01409, max. 0.01660
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05299, max. 0.05169
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07245, max. 0.06777
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00170, max. 0.00350
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02252, max. 0.02704
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:22.394 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:22.396 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:22.396 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:22.396 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:22.397 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:22.397 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:23.745 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:23.802 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:23.804 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:23.807 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:23.809 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:23.812 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:23.814 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01183, max. 0.01183
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04279, max. 0.04594
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06140, max. 0.07884
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00108, max. 0.00170
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01657, max. 0.02130
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01183, max. 0.01183
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04279, max. 0.04594
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06140, max. 0.07884
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00110, max. 0.00161
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01529, max. 0.02130
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:24.094 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:24.095 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:24.096 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:24.096 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:24.096 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:24.096 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:24.453 INFO: node_feats: torch.Size([18, 2304])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:50:24.506 INFO: node_feats: torch.Size([18, 256])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:50:24.509 INFO: node_feats_out:torch.Size([18, 256]), torch.Size([2560]), 18
2024-11-06 08:50:24.511 INFO: linear_node_feats: torch.Size([18, 256])
2024-11-06 08:50:24.514 INFO: q_node_feats: torch.Size([18, 256])
2024-11-06 08:50:24.516 INFO: v_node_feats: torch.Size([18, 256])
2024-11-06 08:50:24.518 INFO: k_node_feats: torch.Size([18, 256])
q_vector from forward: torch.Size([18, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01183, max. 0.01183
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04279, max. 0.04594
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06140, max. 0.07884
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00103, max. 0.00170
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01373, max. 0.02127
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([18, 256])
2024-11-06 08:50:24.658 INFO: long_range_embedding: torch.Size([18, 256])
2024-11-06 08:50:24.659 INFO: long_range_energy: torch.Size([18, 1]), torch.Size([1])
2024-11-06 08:50:24.659 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-11-06 08:50:24.660 INFO: total_energy: torch.Size([1])
2024-11-06 08:50:24.660 INFO: node_energy_contributions: torch.Size([18, 4])
2024-11-06 08:50:24.660 INFO: node_energy: torch.Size([18])
2024-11-06 08:50:24.847 INFO: Epoch 27: head: default, loss=  0.0508, MAE_E_per_atom=     5.5 meV, MAE_F=    14.8 meV / A
2024-11-06 08:50:25.381 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:25.442 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:25.445 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:25.449 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:25.452 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:25.455 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:25.458 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01123, max. 0.01123
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04182, max. 0.04531
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05788, max. 0.07317
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00114, max. 0.00161
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01479, max. 0.01885
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01122, max. 0.01122
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04295, max. 0.04443
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05468, max. 0.06757
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00114, max. 0.00174
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01286, max. 0.01822
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:25.747 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:25.748 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:25.749 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:25.749 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:25.749 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:25.749 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:27.072 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:27.134 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:27.137 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:27.140 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:27.143 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:27.146 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:27.150 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01115, max. 0.01172
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04773, max. 0.04518
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06253, max. 0.05580
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00131, max. 0.00234
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01496, max. 0.02014
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01088, max. 0.01090
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04170, max. 0.04447
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05737, max. 0.07226
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00105, max. 0.00138
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01395, max. 0.02059
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:27.445 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:27.447 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:27.447 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:27.447 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:27.448 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:27.448 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:28.784 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:28.845 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:28.848 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:28.852 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:28.855 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:28.858 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:28.861 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01098, max. 0.01100
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04155, max. 0.04456
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05739, max. 0.07212
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00116, max. 0.00164
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01519, max. 0.02066
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01100, max. 0.01100
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04170, max. 0.04483
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05714, max. 0.07218
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00117, max. 0.00154
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01708, max. 0.02027
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:29.150 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:29.152 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:29.152 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:29.153 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:29.153 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:29.153 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:30.585 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:30.646 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:30.650 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:30.653 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:30.656 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:30.659 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:30.662 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01461, max. 0.01633
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05453, max. 0.05377
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06969, max. 0.06739
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00173, max. 0.00344
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02193, max. 0.02573
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01116, max. 0.01118
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04147, max. 0.04497
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05749, max. 0.07322
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00117, max. 0.00140
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01536, max. 0.02101
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:30.954 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:30.956 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:30.956 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:30.956 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:30.956 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:30.956 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:32.304 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:32.365 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:32.369 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:32.372 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:32.375 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:32.378 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:32.381 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01160, max. 0.01160
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04116, max. 0.04546
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05776, max. 0.07408
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00110, max. 0.00156
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01790, max. 0.02156
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01160, max. 0.01162
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04120, max. 0.04548
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05780, max. 0.07414
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00109, max. 0.00156
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01751, max. 0.02146
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:32.674 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:32.676 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:32.676 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:32.676 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:32.676 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:32.677 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:34.014 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:34.076 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:34.079 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:34.082 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:34.085 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:34.088 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:34.092 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01176, max. 0.01176
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04351, max. 0.04500
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05196, max. 0.06443
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00116, max. 0.00178
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01305, max. 0.01864
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01176, max. 0.01178
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04106, max. 0.04557
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05721, max. 0.07348
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00116, max. 0.00145
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01538, max. 0.02129
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:34.381 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:34.383 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:34.383 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:34.384 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:34.384 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:34.384 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:35.709 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:35.770 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:35.773 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:35.776 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:35.780 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:35.783 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:35.786 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01176, max. 0.01176
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04104, max. 0.04537
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05631, max. 0.07222
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00109, max. 0.00156
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01703, max. 0.02118
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01195, max. 0.01195
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04769, max. 0.04488
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06401, max. 0.05610
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00125, max. 0.00240
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01485, max. 0.02037
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:36.080 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:36.081 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:36.082 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:36.082 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:36.082 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:36.082 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:37.419 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:37.480 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:37.484 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:37.487 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:37.490 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:37.493 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:37.496 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01161, max. 0.01161
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04145, max. 0.04521
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05349, max. 0.06769
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00116, max. 0.00159
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01449, max. 0.01844
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01389, max. 0.01613
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05406, max. 0.05318
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07210, max. 0.06562
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00153, max. 0.00338
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02083, max. 0.02465
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:37.888 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:37.890 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:37.890 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:37.890 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:37.890 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:37.891 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:39.235 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:39.296 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:39.300 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:39.303 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:39.306 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:39.309 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:39.312 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01132, max. 0.01134
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04133, max. 0.04433
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05449, max. 0.06823
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00116, max. 0.00163
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01413, max. 0.01981
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01132, max. 0.01134
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04133, max. 0.04433
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05448, max. 0.06823
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00111, max. 0.00149
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01577, max. 0.01983
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:39.600 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:39.601 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:39.602 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:39.602 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:39.602 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:39.602 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:40.928 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:40.989 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:40.993 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:40.996 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:40.999 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:41.002 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:41.005 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01125, max. 0.01127
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04179, max. 0.04410
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05433, max. 0.06813
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00105, max. 0.00139
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01258, max. 0.01979
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01127, max. 0.01127
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04192, max. 0.04434
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05384, max. 0.06797
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00118, max. 0.00153
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01490, max. 0.01938
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:41.294 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:41.296 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:41.296 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:41.296 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:41.296 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:41.296 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:42.631 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:42.692 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:42.695 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:42.698 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:42.701 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:42.705 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:42.708 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01123, max. 0.01125
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04208, max. 0.04412
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05457, max. 0.06949
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00106, max. 0.00138
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01283, max. 0.02000
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01125, max. 0.01125
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04220, max. 0.04436
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05470, max. 0.06928
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00118, max. 0.00152
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01527, max. 0.01960
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:42.997 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:42.999 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:42.999 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:42.999 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:42.999 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:43.000 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:44.319 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:44.380 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:44.383 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:44.386 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:44.390 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:44.393 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:44.396 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01126, max. 0.01128
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04214, max. 0.04426
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05576, max. 0.07109
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00117, max. 0.00161
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01495, max. 0.02024
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01126, max. 0.01128
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04214, max. 0.04426
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05576, max. 0.07109
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00112, max. 0.00147
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01658, max. 0.02030
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:44.687 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:44.689 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:44.689 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:44.690 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:44.690 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:44.690 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:46.118 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:46.179 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:46.182 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:46.186 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:46.189 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:46.192 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:46.195 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01143, max. 0.01146
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04221, max. 0.04438
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05635, max. 0.07186
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00118, max. 0.00139
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01516, max. 0.02052
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01143, max. 0.01143
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04433, max. 0.04375
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05024, max. 0.06152
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00119, max. 0.00176
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01243, max. 0.01772
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:46.484 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:46.485 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:46.485 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:46.486 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:46.486 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:46.486 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:47.823 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:47.884 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:47.887 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:47.890 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:47.893 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:47.896 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:47.900 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01434, max. 0.01634
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05537, max. 0.05410
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06995, max. 0.06363
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00161, max. 0.00345
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02059, max. 0.02389
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01160, max. 0.01160
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04247, max. 0.04457
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05343, max. 0.06725
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00119, max. 0.00162
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01425, max. 0.01836
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:48.188 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:48.189 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:48.190 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:48.190 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:48.190 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:48.190 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:49.533 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:49.593 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:49.597 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:49.600 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:49.603 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:49.606 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:49.609 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01176, max. 0.01181
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04887, max. 0.04637
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06258, max. 0.05427
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00129, max. 0.00245
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01322, max. 0.01825
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01155, max. 0.01155
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04181, max. 0.04394
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05462, max. 0.06772
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00113, max. 0.00154
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01635, max. 0.02012
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:49.910 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:49.912 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:49.912 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:49.912 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:49.912 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:49.913 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:51.249 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:51.306 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:51.309 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:51.312 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:51.314 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:51.316 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:51.319 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01160, max. 0.01162
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04147, max. 0.04514
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05812, max. 0.07437
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00112, max. 0.00164
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01570, max. 0.02081
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01160, max. 0.01162
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04147, max. 0.04514
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05812, max. 0.07437
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00114, max. 0.00153
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01448, max. 0.02080
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:51.597 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:51.599 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:51.599 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:51.599 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:51.599 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:51.599 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:51.958 INFO: node_feats: torch.Size([18, 2304])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:50:52.012 INFO: node_feats: torch.Size([18, 256])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:50:52.015 INFO: node_feats_out:torch.Size([18, 256]), torch.Size([2560]), 18
2024-11-06 08:50:52.017 INFO: linear_node_feats: torch.Size([18, 256])
2024-11-06 08:50:52.020 INFO: q_node_feats: torch.Size([18, 256])
2024-11-06 08:50:52.022 INFO: v_node_feats: torch.Size([18, 256])
2024-11-06 08:50:52.024 INFO: k_node_feats: torch.Size([18, 256])
q_vector from forward: torch.Size([18, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01160, max. 0.01162
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04147, max. 0.04514
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05812, max. 0.07437
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00103, max. 0.00163
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00008
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01321, max. 0.02078
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([18, 256])
2024-11-06 08:50:52.164 INFO: long_range_embedding: torch.Size([18, 256])
2024-11-06 08:50:52.166 INFO: long_range_energy: torch.Size([18, 1]), torch.Size([1])
2024-11-06 08:50:52.166 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-11-06 08:50:52.166 INFO: total_energy: torch.Size([1])
2024-11-06 08:50:52.166 INFO: node_energy_contributions: torch.Size([18, 4])
2024-11-06 08:50:52.166 INFO: node_energy: torch.Size([18])
2024-11-06 08:50:52.354 INFO: Epoch 30: head: default, loss=  0.0486, MAE_E_per_atom=     5.0 meV, MAE_F=    14.4 meV / A
2024-11-06 08:50:52.831 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:52.892 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:52.896 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:52.899 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:52.902 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:52.905 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:52.908 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01142, max. 0.01142
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04181, max. 0.04340
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05451, max. 0.06383
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00114, max. 0.00153
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01550, max. 0.01928
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01163, max. 0.01177
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04888, max. 0.04695
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06260, max. 0.05436
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00129, max. 0.00243
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01270, max. 0.01748
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:53.196 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:53.197 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:53.198 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:53.198 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:53.198 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:53.198 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:54.526 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:54.587 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:54.591 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:54.594 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:54.597 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:54.600 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:54.603 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01132, max. 0.01132
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04255, max. 0.04380
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05143, max. 0.05705
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00122, max. 0.00159
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01292, max. 0.01640
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01131, max. 0.01134
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04173, max. 0.04311
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05411, max. 0.06170
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00119, max. 0.00164
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01283, max. 0.01855
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:55.010 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:55.012 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:55.012 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:55.013 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:55.013 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:55.013 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:56.342 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:56.402 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:56.406 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:56.409 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:56.412 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:56.415 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:56.419 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01126, max. 0.01126
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04182, max. 0.04327
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05326, max. 0.06096
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00119, max. 0.00152
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01376, max. 0.01799
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01124, max. 0.01127
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04170, max. 0.04309
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05391, max. 0.06161
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00113, max. 0.00148
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01459, max. 0.01840
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:56.708 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:56.709 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:56.710 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:56.710 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:56.710 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:56.710 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:58.039 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:58.100 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:58.103 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:58.106 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:58.109 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:58.112 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:58.115 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01114, max. 0.01117
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04159, max. 0.04351
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05340, max. 0.06445
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00107, max. 0.00135
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01183, max. 0.01878
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01115, max. 0.01115
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04431, max. 0.04504
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05046, max. 0.05434
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00119, max. 0.00172
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01159, max. 0.01631
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:50:58.405 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:50:58.407 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:50:58.407 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:50:58.407 INFO: total_energy: torch.Size([2])
2024-11-06 08:50:58.407 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:50:58.407 INFO: node_energy: torch.Size([36])
2024-11-06 08:50:59.739 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:59.800 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:50:59.803 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:50:59.806 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:50:59.809 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:50:59.813 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:50:59.816 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01128, max. 0.01131
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04151, max. 0.04429
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05465, max. 0.06914
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00119, max. 0.00133
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01435, max. 0.01953
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01493, max. 0.01614
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05495, max. 0.05432
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06882, max. 0.06133
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00157, max. 0.00326
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02006, max. 0.02308
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:00.104 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:00.105 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:00.105 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:00.106 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:00.106 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:00.106 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:01.456 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:01.517 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:01.520 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:01.524 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:01.527 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:01.530 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:01.533 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01158, max. 0.01158
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04183, max. 0.04531
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05706, max. 0.07200
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00119, max. 0.00153
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01555, max. 0.01985
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01174, max. 0.01197
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04860, max. 0.04589
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06221, max. 0.05401
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00131, max. 0.00240
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01453, max. 0.01950
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:01.822 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:01.823 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:01.824 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:01.824 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:01.824 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:01.824 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:03.255 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:03.316 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:03.320 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:03.323 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:03.326 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:03.329 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:03.333 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01173, max. 0.01175
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04165, max. 0.04538
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05760, max. 0.07340
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00120, max. 0.00166
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01509, max. 0.02027
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01173, max. 0.01175
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04165, max. 0.04538
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05760, max. 0.07340
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00114, max. 0.00152
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01676, max. 0.02034
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:03.626 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:03.628 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:03.628 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:03.629 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:03.629 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:03.629 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:04.958 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:05.019 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:05.022 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:05.025 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:05.029 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:05.032 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:05.035 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01195, max. 0.01195
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04464, max. 0.04456
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05242, max. 0.06165
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00122, max. 0.00189
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01253, max. 0.01758
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01194, max. 0.01196
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04186, max. 0.04521
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05639, max. 0.07190
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00109, max. 0.00148
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01304, max. 0.02007
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:05.322 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:05.324 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:05.324 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:05.324 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:05.324 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:05.324 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:06.657 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:06.717 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:06.721 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:06.724 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:06.727 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:06.730 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:06.733 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01217, max. 0.01218
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04317, max. 0.04486
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05120, max. 0.06417
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00124, max. 0.00173
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01372, max. 0.01759
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01504, max. 0.01693
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05602, max. 0.05505
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07153, max. 0.06299
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00160, max. 0.00359
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02000, max. 0.02320
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:07.021 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:07.023 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:07.023 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:07.023 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:07.023 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:07.024 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:08.366 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:08.427 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:08.430 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:08.433 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:08.437 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:08.440 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:08.443 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01218, max. 0.01218
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04289, max. 0.04436
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05474, max. 0.06518
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00118, max. 0.00164
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01491, max. 0.01876
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01218, max. 0.01219
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04291, max. 0.04438
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05196, max. 0.06523
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00125, max. 0.00153
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01354, max. 0.01865
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:08.732 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:08.734 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:08.734 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:08.734 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:08.734 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:08.735 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:10.068 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:10.129 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:10.132 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:10.136 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:10.139 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:10.142 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:10.145 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01211, max. 0.01212
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04334, max. 0.04418
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05132, max. 0.06362
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00126, max. 0.00151
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01322, max. 0.01817
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01211, max. 0.01212
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04334, max. 0.04418
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05170, max. 0.06362
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00124, max. 0.00179
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01278, max. 0.01821
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:10.527 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:10.529 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:10.529 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:10.529 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:10.529 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:10.529 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:11.847 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:11.908 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:11.911 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:11.914 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:11.917 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:11.920 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:11.923 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01220, max. 0.01220
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04363, max. 0.04447
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05479, max. 0.06482
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00119, max. 0.00164
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01477, max. 0.01853
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01240, max. 0.01272
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05139, max. 0.04883
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06714, max. 0.05725
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00135, max. 0.00272
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01257, max. 0.01700
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:12.214 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:12.216 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:12.216 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:12.216 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:12.217 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:12.217 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:13.554 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:13.615 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:13.619 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:13.622 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:13.625 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:13.628 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:13.631 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01230, max. 0.01230
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04673, max. 0.04666
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05543, max. 0.05575
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00125, max. 0.00201
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01131, max. 0.01606
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01229, max. 0.01231
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04350, max. 0.04491
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05323, max. 0.06768
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00112, max. 0.00154
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01232, max. 0.01910
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:13.921 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:13.923 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:13.923 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:13.923 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:13.923 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:13.924 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:15.257 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:15.317 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:15.320 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:15.324 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:15.327 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:15.330 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:15.333 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01607, max. 0.01756
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05841, max. 0.05724
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07377, max. 0.06283
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00170, max. 0.00374
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01945, max. 0.02197
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01244, max. 0.01244
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04357, max. 0.04563
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05723, max. 0.06989
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00122, max. 0.00173
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01500, max. 0.01934
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:15.621 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:15.623 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:15.623 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:15.623 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:15.624 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:15.624 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:16.964 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:17.024 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:17.028 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:17.031 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:17.034 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:17.037 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:17.040 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01236, max. 0.01238
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04316, max. 0.04568
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05627, max. 0.07140
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00117, max. 0.00165
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01639, max. 0.01987
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01238, max. 0.01238
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04432, max. 0.04554
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05234, max. 0.06512
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00126, max. 0.00177
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01398, max. 0.01749
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:17.329 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:17.331 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:17.331 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:17.332 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:17.332 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:17.332 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:18.764 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:18.821 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:18.824 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:18.826 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:18.829 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:18.831 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:18.833 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01180, max. 0.01182
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04179, max. 0.04497
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05580, max. 0.07119
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00116, max. 0.00165
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01495, max. 0.01999
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01180, max. 0.01182
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04179, max. 0.04497
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05580, max. 0.07119
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00119, max. 0.00153
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01385, max. 0.01997
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:19.113 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:19.114 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:19.114 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:19.115 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:19.115 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:19.115 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:19.481 INFO: node_feats: torch.Size([18, 2304])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:51:19.534 INFO: node_feats: torch.Size([18, 256])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:51:19.537 INFO: node_feats_out:torch.Size([18, 256]), torch.Size([2560]), 18
2024-11-06 08:51:19.539 INFO: linear_node_feats: torch.Size([18, 256])
2024-11-06 08:51:19.542 INFO: q_node_feats: torch.Size([18, 256])
2024-11-06 08:51:19.544 INFO: v_node_feats: torch.Size([18, 256])
2024-11-06 08:51:19.546 INFO: k_node_feats: torch.Size([18, 256])
q_vector from forward: torch.Size([18, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01180, max. 0.01182
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04179, max. 0.04497
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05580, max. 0.07119
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00107, max. 0.00164
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01259, max. 0.01996
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([18, 256])
2024-11-06 08:51:19.686 INFO: long_range_embedding: torch.Size([18, 256])
2024-11-06 08:51:19.687 INFO: long_range_energy: torch.Size([18, 1]), torch.Size([1])
2024-11-06 08:51:19.687 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-11-06 08:51:19.688 INFO: total_energy: torch.Size([1])
2024-11-06 08:51:19.688 INFO: node_energy_contributions: torch.Size([18, 4])
2024-11-06 08:51:19.688 INFO: node_energy: torch.Size([18])
2024-11-06 08:51:19.875 INFO: Epoch 33: head: default, loss=  0.0466, MAE_E_per_atom=     4.5 meV, MAE_F=    14.0 meV / A
2024-11-06 08:51:20.417 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:20.478 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:20.481 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:20.485 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:20.488 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:20.491 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:20.494 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01640, max. 0.01732
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05820, max. 0.05762
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07195, max. 0.06189
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00173, max. 0.00360
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01909, max. 0.02113
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01214, max. 0.01216
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04313, max. 0.04549
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05523, max. 0.06975
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00126, max. 0.00177
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01406, max. 0.01943
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:20.779 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:20.780 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:20.780 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:20.781 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:20.781 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:20.781 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:22.108 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:22.168 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:22.172 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:22.175 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:22.178 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:22.181 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:22.184 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01215, max. 0.01217
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04325, max. 0.04524
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05325, max. 0.06709
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00120, max. 0.00162
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01548, max. 0.01893
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01217, max. 0.01217
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04442, max. 0.04539
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04921, max. 0.06064
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00130, max. 0.00173
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01322, max. 0.01653
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:22.479 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:22.480 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:22.481 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:22.481 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:22.481 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:22.481 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:23.811 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:23.873 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:23.876 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:23.879 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:23.882 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:23.886 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:23.889 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01220, max. 0.01222
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04355, max. 0.04500
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05144, max. 0.06466
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00131, max. 0.00150
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01349, max. 0.01837
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01221, max. 0.01221
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04686, max. 0.04767
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05452, max. 0.05220
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00133, max. 0.00201
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01096, max. 0.01532
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:24.179 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:24.181 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:24.181 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:24.181 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:24.181 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:24.182 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:25.513 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:25.574 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:25.577 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:25.581 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:25.584 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:25.587 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:25.590 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01249, max. 0.01249
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04409, max. 0.04522
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05739, max. 0.06320
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00129, max. 0.00174
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01409, max. 0.01788
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01247, max. 0.01248
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04386, max. 0.04512
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05112, max. 0.06439
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00119, max. 0.00157
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01183, max. 0.01842
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:25.880 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:25.881 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:25.882 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:25.882 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:25.882 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:25.882 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:27.310 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:27.371 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:27.374 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:27.377 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:27.381 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:27.384 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:27.387 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01293, max. 0.01308
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05230, max. 0.05005
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07047, max. 0.05865
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00143, max. 0.00284
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01243, max. 0.01660
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01271, max. 0.01271
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04408, max. 0.04530
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05549, max. 0.06511
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00124, max. 0.00171
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01476, max. 0.01854
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:27.676 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:27.677 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:27.677 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:27.678 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:27.678 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:27.678 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:29.022 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:29.083 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:29.086 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:29.089 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:29.093 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:29.096 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:29.099 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01283, max. 0.01284
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04395, max. 0.04539
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05245, max. 0.06629
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00117, max. 0.00163
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01210, max. 0.01870
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01284, max. 0.01284
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04739, max. 0.04789
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05868, max. 0.05347
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00132, max. 0.00214
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01137, max. 0.01573
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:29.387 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:29.389 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:29.389 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:29.389 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:29.389 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:29.390 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:30.730 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:30.790 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:30.794 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:30.797 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:30.800 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:30.803 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:30.806 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01615, max. 0.01767
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05900, max. 0.05827
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07501, max. 0.06327
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00168, max. 0.00376
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01827, max. 0.02035
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01297, max. 0.01299
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04375, max. 0.04555
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05356, max. 0.06790
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00119, max. 0.00177
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01540, max. 0.01891
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:31.098 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:31.100 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:31.100 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:31.100 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:31.101 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:31.101 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:32.440 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:32.501 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:32.505 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:32.508 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:32.511 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:32.514 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:32.517 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01297, max. 0.01298
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04376, max. 0.04568
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05374, max. 0.06824
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00129, max. 0.00164
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01410, max. 0.01896
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01299, max. 0.01299
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04401, max. 0.04577
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05867, max. 0.06695
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00127, max. 0.00183
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01482, max. 0.01857
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:32.805 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:32.807 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:32.807 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:32.807 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:32.807 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:32.808 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:34.135 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:34.196 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:34.200 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:34.203 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:34.206 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:34.209 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:34.212 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01300, max. 0.01327
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05207, max. 0.04989
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06881, max. 0.05802
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00143, max. 0.00288
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01269, max. 0.01684
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01279, max. 0.01279
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04382, max. 0.04557
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05523, max. 0.06738
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00124, max. 0.00173
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01532, max. 0.01880
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:34.599 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:34.601 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:34.601 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:34.601 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:34.601 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:34.602 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:35.937 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:35.998 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:36.002 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:36.005 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:36.008 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:36.011 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:36.014 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01260, max. 0.01260
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04519, max. 0.04645
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04763, max. 0.05874
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00132, max. 0.00181
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01291, max. 0.01584
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01257, max. 0.01259
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04389, max. 0.04533
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05236, max. 0.06595
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00130, max. 0.00185
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01305, max. 0.01825
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:36.303 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:36.305 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:36.305 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:36.305 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:36.306 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:36.306 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:37.644 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:37.705 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:37.708 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:37.711 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:37.714 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:37.717 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:37.721 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01253, max. 0.01254
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04413, max. 0.04511
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05133, max. 0.06478
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00132, max. 0.00155
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01337, max. 0.01793
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01255, max. 0.01255
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04760, max. 0.04845
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05583, max. 0.05169
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00135, max. 0.00211
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01164, max. 0.01510
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:38.010 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:38.011 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:38.012 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:38.012 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:38.012 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:38.012 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:39.335 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:39.396 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:39.399 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:39.402 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:39.405 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:39.408 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:39.412 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01693, max. 0.01784
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06001, max. 0.05952
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07590, max. 0.06392
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00167, max. 0.00377
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01791, max. 0.01909
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01279, max. 0.01279
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04465, max. 0.04539
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05970, max. 0.06431
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00129, max. 0.00179
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01427, max. 0.01774
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:39.703 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:39.705 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:39.705 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:39.705 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:39.705 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:39.705 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:41.046 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:41.107 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:41.111 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:41.114 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:41.117 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:41.120 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:41.123 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01292, max. 0.01292
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04617, max. 0.04703
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04742, max. 0.05821
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00135, max. 0.00188
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01312, max. 0.01577
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01289, max. 0.01291
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04479, max. 0.04560
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05472, max. 0.06587
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00132, max. 0.00192
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01279, max. 0.01824
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:41.413 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:41.415 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:41.415 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:41.415 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:41.415 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:41.415 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:42.846 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:42.907 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:42.910 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:42.914 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:42.917 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:42.920 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:42.923 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01310, max. 0.01312
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04519, max. 0.04589
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05450, max. 0.06649
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00125, max. 0.00180
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01506, max. 0.01838
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01310, max. 0.01310
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04517, max. 0.04588
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05834, max. 0.06642
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00129, max. 0.00179
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01506, max. 0.01847
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:43.210 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:43.212 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:43.212 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:43.212 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:43.212 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:43.213 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:44.539 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:44.600 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:44.603 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:44.607 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:44.610 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:44.613 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:44.616 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01318, max. 0.01320
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04546, max. 0.04616
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05526, max. 0.06709
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00125, max. 0.00171
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01201, max. 0.01866
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01340, max. 0.01379
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05413, max. 0.05162
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07257, max. 0.06055
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00152, max. 0.00307
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01250, max. 0.01641
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:44.904 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:44.906 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:44.906 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:44.906 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:44.907 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:44.907 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:46.244 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:46.301 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:46.304 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:46.306 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:46.308 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:46.311 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:46.313 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01233, max. 0.01234
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04306, max. 0.04532
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05425, max. 0.06894
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00122, max. 0.00174
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01425, max. 0.01925
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01233, max. 0.01234
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04306, max. 0.04532
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05425, max. 0.06894
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00125, max. 0.00161
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01334, max. 0.01921
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:46.592 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:46.594 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:46.594 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:46.594 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:46.594 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:46.594 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:46.951 INFO: node_feats: torch.Size([18, 2304])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:51:47.004 INFO: node_feats: torch.Size([18, 256])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:51:47.006 INFO: node_feats_out:torch.Size([18, 256]), torch.Size([2560]), 18
2024-11-06 08:51:47.009 INFO: linear_node_feats: torch.Size([18, 256])
2024-11-06 08:51:47.011 INFO: q_node_feats: torch.Size([18, 256])
2024-11-06 08:51:47.013 INFO: v_node_feats: torch.Size([18, 256])
2024-11-06 08:51:47.016 INFO: k_node_feats: torch.Size([18, 256])
q_vector from forward: torch.Size([18, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01233, max. 0.01234
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04306, max. 0.04532
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05425, max. 0.06894
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00114, max. 0.00173
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01210, max. 0.01920
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([18, 256])
2024-11-06 08:51:47.156 INFO: long_range_embedding: torch.Size([18, 256])
2024-11-06 08:51:47.157 INFO: long_range_energy: torch.Size([18, 1]), torch.Size([1])
2024-11-06 08:51:47.157 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-11-06 08:51:47.157 INFO: total_energy: torch.Size([1])
2024-11-06 08:51:47.158 INFO: node_energy_contributions: torch.Size([18, 4])
2024-11-06 08:51:47.158 INFO: node_energy: torch.Size([18])
2024-11-06 08:51:47.345 INFO: Epoch 36: head: default, loss=  0.0481, MAE_E_per_atom=     4.2 meV, MAE_F=    14.2 meV / A
2024-11-06 08:51:47.417 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:47.478 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:47.481 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:47.484 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:47.487 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:47.490 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:47.494 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01343, max. 0.01343
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04690, max. 0.04759
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04898, max. 0.05935
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00136, max. 0.00201
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01387, max. 0.01620
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01340, max. 0.01340
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04543, max. 0.04625
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05925, max. 0.06734
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00129, max. 0.00186
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01518, max. 0.01872
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:47.780 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:47.782 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:47.782 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:47.782 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:47.782 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:47.782 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:49.105 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:49.166 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:49.169 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:49.173 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:49.176 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:49.179 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:49.182 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01322, max. 0.01325
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04547, max. 0.04588
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05233, max. 0.06554
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00135, max. 0.00170
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01339, max. 0.01813
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01322, max. 0.01325
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04547, max. 0.04588
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05472, max. 0.06554
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00124, max. 0.00184
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01479, max. 0.01808
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:49.474 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:49.476 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:49.476 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:49.476 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:49.476 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:49.477 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:50.814 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:50.875 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:50.879 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:50.882 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:50.885 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:50.888 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:50.891 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01315, max. 0.01315
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04579, max. 0.04606
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06154, max. 0.06299
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00131, max. 0.00189
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01452, max. 0.01741
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01314, max. 0.01314
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04915, max. 0.04991
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05951, max. 0.05089
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00138, max. 0.00230
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01208, max. 0.01520
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:51.279 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:51.281 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:51.281 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:51.281 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:51.281 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:51.281 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:52.619 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:52.680 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:52.683 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:52.686 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:52.689 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:52.692 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:52.696 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01287, max. 0.01289
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04552, max. 0.04586
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05533, max. 0.06344
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00132, max. 0.00193
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01209, max. 0.01747
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01287, max. 0.01289
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04552, max. 0.04586
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05500, max. 0.06344
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00123, max. 0.00165
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01136, max. 0.01753
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:52.983 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:52.985 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:52.985 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:52.985 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:52.985 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:52.985 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:54.310 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:54.371 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:54.374 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:54.378 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:54.381 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:54.384 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:54.387 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01739, max. 0.01811
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06140, max. 0.06104
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07730, max. 0.06493
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00173, max. 0.00389
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01772, max. 0.01880
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01318, max. 0.01373
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05393, max. 0.05179
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07088, max. 0.05949
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00148, max. 0.00304
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01256, max. 0.01622
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:54.677 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:54.678 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:54.679 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:54.679 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:54.679 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:54.679 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:56.040 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:56.101 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:56.105 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:56.108 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:56.111 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:56.114 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:56.117 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01327, max. 0.01329
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04546, max. 0.04580
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05521, max. 0.06560
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00124, max. 0.00185
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01479, max. 0.01798
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01330, max. 0.01330
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04691, max. 0.04772
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04848, max. 0.05745
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00136, max. 0.00199
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01332, max. 0.01577
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:56.406 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:56.408 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:56.408 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:56.408 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:56.408 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:56.409 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:57.729 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:57.790 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:57.793 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:57.797 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:57.800 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:57.803 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:57.806 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01348, max. 0.01348
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04894, max. 0.04912
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05975, max. 0.05371
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00144, max. 0.00239
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01210, max. 0.01605
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01367, max. 0.01404
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05371, max. 0.05097
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07230, max. 0.06035
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00154, max. 0.00321
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01339, max. 0.01738
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:58.095 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:58.097 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:58.097 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:58.097 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:58.098 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:58.098 INFO: node_energy: torch.Size([36])
2024-11-06 08:51:59.445 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:59.600 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:51:59.604 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:51:59.607 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:51:59.610 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:51:59.613 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:51:59.617 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01356, max. 0.01358
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04518, max. 0.04645
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05827, max. 0.06831
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00136, max. 0.00210
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01311, max. 0.01868
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01359, max. 0.01359
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04550, max. 0.04648
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06402, max. 0.06664
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00139, max. 0.00200
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01513, max. 0.01825
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:51:59.908 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:51:59.910 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:51:59.910 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:51:59.910 INFO: total_energy: torch.Size([2])
2024-11-06 08:51:59.910 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:51:59.911 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:01.237 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:01.297 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:01.301 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:01.304 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:01.307 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:01.310 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:01.313 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01347, max. 0.01347
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04531, max. 0.04632
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06056, max. 0.06727
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00132, max. 0.00189
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01530, max. 0.01832
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01744, max. 0.01865
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06098, max. 0.05999
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07845, max. 0.06603
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00182, max. 0.00412
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01891, max. 0.02024
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:01.602 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:01.604 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:01.604 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:01.604 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:01.604 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:01.604 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:02.944 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:03.005 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:03.009 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:03.012 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:03.015 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:03.018 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:03.021 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01324, max. 0.01326
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04561, max. 0.04604
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05827, max. 0.06474
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00128, max. 0.00174
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01146, max. 0.01763
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01324, max. 0.01326
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04561, max. 0.04604
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05141, max. 0.06474
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00138, max. 0.00172
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01314, max. 0.01753
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:03.311 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:03.313 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:03.313 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:03.313 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:03.313 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:03.313 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:04.648 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:04.709 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:04.712 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:04.715 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:04.718 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:04.721 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:04.725 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01335, max. 0.01335
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04748, max. 0.04869
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04892, max. 0.05596
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00136, max. 0.00203
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01301, max. 0.01508
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01352, max. 0.01413
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05440, max. 0.05216
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07179, max. 0.06086
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00155, max. 0.00322
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01300, max. 0.01645
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:05.012 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:05.014 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:05.014 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:05.014 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:05.015 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:05.015 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:06.352 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:06.413 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:06.417 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:06.420 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:06.423 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:06.426 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:06.429 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01351, max. 0.01353
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04614, max. 0.04625
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05750, max. 0.06464
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00125, max. 0.00191
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01426, max. 0.01725
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01354, max. 0.01354
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04649, max. 0.04672
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06537, max. 0.06283
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00137, max. 0.00198
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01455, max. 0.01692
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:06.719 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:06.721 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:06.721 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:06.722 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:06.722 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:06.722 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:08.151 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:08.212 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:08.215 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:08.218 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:08.221 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:08.224 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:08.228 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01351, max. 0.01353
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04611, max. 0.04625
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05310, max. 0.06515
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00133, max. 0.00176
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01294, max. 0.01737
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01351, max. 0.01351
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04609, max. 0.04622
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06177, max. 0.06507
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00127, max. 0.00188
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01431, max. 0.01737
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:08.513 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:08.515 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:08.515 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:08.516 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:08.516 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:08.516 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:09.843 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:09.904 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:09.907 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:09.910 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:09.913 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:09.916 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:09.919 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01726, max. 0.01859
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06148, max. 0.06080
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07914, max. 0.06668
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00180, max. 0.00408
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01845, max. 0.01891
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01347, max. 0.01348
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04589, max. 0.04629
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05925, max. 0.06629
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00123, max. 0.00176
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01153, max. 0.01771
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:10.208 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:10.210 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:10.210 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:10.210 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:10.210 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:10.210 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:11.546 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:11.607 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:11.610 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:11.614 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:11.617 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:11.620 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:11.623 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01356, max. 0.01357
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04586, max. 0.04644
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05946, max. 0.06697
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00133, max. 0.00206
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01246, max. 0.01784
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01357, max. 0.01357
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04933, max. 0.04988
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06030, max. 0.05374
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00142, max. 0.00242
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01186, max. 0.01537
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:11.911 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:11.913 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:11.913 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:11.913 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:11.913 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:11.914 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:13.246 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:13.302 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:13.305 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:13.307 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:13.310 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:13.312 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:13.314 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01284, max. 0.01286
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04431, max. 0.04575
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05340, max. 0.06755
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00125, max. 0.00185
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01357, max. 0.01856
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01284, max. 0.01286
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04431, max. 0.04575
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05340, max. 0.06755
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00129, max. 0.00169
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01278, max. 0.01849
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:13.595 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:13.596 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:13.596 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:13.597 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:13.597 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:13.597 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:13.954 INFO: node_feats: torch.Size([18, 2304])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:52:14.007 INFO: node_feats: torch.Size([18, 256])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:52:14.010 INFO: node_feats_out:torch.Size([18, 256]), torch.Size([2560]), 18
2024-11-06 08:52:14.012 INFO: linear_node_feats: torch.Size([18, 256])
2024-11-06 08:52:14.014 INFO: q_node_feats: torch.Size([18, 256])
2024-11-06 08:52:14.017 INFO: v_node_feats: torch.Size([18, 256])
2024-11-06 08:52:14.019 INFO: k_node_feats: torch.Size([18, 256])
q_vector from forward: torch.Size([18, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01284, max. 0.01286
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04431, max. 0.04575
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05340, max. 0.06755
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00119, max. 0.00184
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01169, max. 0.01850
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([18, 256])
2024-11-06 08:52:14.160 INFO: long_range_embedding: torch.Size([18, 256])
2024-11-06 08:52:14.161 INFO: long_range_energy: torch.Size([18, 1]), torch.Size([1])
2024-11-06 08:52:14.161 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-11-06 08:52:14.161 INFO: total_energy: torch.Size([1])
2024-11-06 08:52:14.161 INFO: node_energy_contributions: torch.Size([18, 4])
2024-11-06 08:52:14.162 INFO: node_energy: torch.Size([18])
2024-11-06 08:52:14.348 INFO: Epoch 39: head: default, loss=  0.0470, MAE_E_per_atom=     3.9 meV, MAE_F=    13.9 meV / A
2024-11-06 08:52:14.419 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:14.480 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:14.483 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:14.487 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:14.490 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:14.493 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:14.496 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01373, max. 0.01375
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04584, max. 0.04650
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05998, max. 0.06736
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00134, max. 0.00209
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01258, max. 0.01806
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01373, max. 0.01375
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04584, max. 0.04650
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05351, max. 0.06736
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00136, max. 0.00178
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01348, max. 0.01801
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:14.783 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:14.785 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:14.785 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:14.785 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:14.785 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:14.785 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:16.198 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:16.259 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:16.263 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:16.266 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:16.269 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:16.272 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:16.275 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01394, max. 0.01394
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04739, max. 0.04787
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05114, max. 0.05884
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00136, max. 0.00212
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01388, max. 0.01582
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01394, max. 0.01394
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04628, max. 0.04651
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06655, max. 0.06552
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00142, max. 0.00204
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01516, max. 0.01775
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:16.567 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:16.569 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:16.569 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:16.569 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:16.569 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:16.570 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:17.900 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:17.961 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:17.965 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:17.968 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:17.971 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:17.974 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:17.977 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01386, max. 0.01439
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05437, max. 0.05157
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07355, max. 0.06151
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00156, max. 0.00329
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01296, max. 0.01641
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01366, max. 0.01368
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04596, max. 0.04607
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05999, max. 0.06550
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00128, max. 0.00177
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01155, max. 0.01780
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:18.266 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:18.273 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:18.274 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:18.274 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:18.274 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:18.274 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:19.610 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:19.670 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:19.674 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:19.677 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:19.680 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:19.683 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:19.686 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01348, max. 0.01348
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04952, max. 0.04989
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05974, max. 0.04989
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00144, max. 0.00239
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01220, max. 0.01456
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01785, max. 0.01865
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06198, max. 0.06115
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08046, max. 0.06740
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00180, max. 0.00407
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01769, max. 0.01862
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:19.975 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:19.977 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:19.977 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:19.977 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:19.977 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:19.978 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:21.323 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:21.384 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:21.388 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:21.391 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:21.394 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:21.397 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:21.400 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01332, max. 0.01332
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04596, max. 0.04602
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06097, max. 0.06228
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00132, max. 0.00181
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01392, max. 0.01687
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01332, max. 0.01334
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04597, max. 0.04606
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05655, max. 0.06237
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00129, max. 0.00183
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01395, max. 0.01681
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:21.690 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:21.692 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:21.692 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:21.692 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:21.693 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:21.693 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:23.031 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:23.091 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:23.095 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:23.098 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:23.101 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:23.104 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:23.107 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01315, max. 0.01315
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04599, max. 0.04631
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06029, max. 0.06277
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00132, max. 0.00178
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01386, max. 0.01675
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01315, max. 0.01317
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04600, max. 0.04634
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05146, max. 0.06286
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00138, max. 0.00166
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01265, max. 0.01675
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:23.396 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:23.397 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:23.397 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:23.398 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:23.398 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:23.398 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:24.811 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:24.872 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:24.875 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:24.878 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:24.881 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:24.884 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:24.888 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01309, max. 0.01310
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04777, max. 0.04930
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04843, max. 0.05555
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00136, max. 0.00194
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01233, max. 0.01424
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01324, max. 0.01405
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05481, max. 0.05295
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07110, max. 0.06071
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00159, max. 0.00310
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01283, max. 0.01506
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:25.177 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:25.179 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:25.179 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:25.179 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:25.180 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:25.180 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:26.525 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:26.586 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:26.589 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:26.592 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:26.595 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:26.598 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:26.602 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01323, max. 0.01323
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04653, max. 0.04706
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06492, max. 0.06483
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00132, max. 0.00189
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01390, max. 0.01659
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01320, max. 0.01322
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04611, max. 0.04661
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05651, max. 0.06695
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00129, max. 0.00180
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01443, max. 0.01706
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:26.890 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:26.891 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:26.891 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:26.892 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:26.892 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:26.892 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:28.219 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:28.280 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:28.283 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:28.286 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:28.289 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:28.292 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:28.295 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01770, max. 0.01832
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06216, max. 0.06181
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07938, max. 0.06707
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00169, max. 0.00390
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01715, max. 0.01851
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01325, max. 0.01327
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04620, max. 0.04667
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06042, max. 0.06750
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00128, max. 0.00170
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01170, max. 0.01709
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:28.584 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:28.586 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:28.586 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:28.586 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:28.586 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:28.586 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:29.923 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:29.983 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:29.987 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:29.990 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:29.993 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:29.996 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:29.999 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01352, max. 0.01352
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04989, max. 0.05071
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06260, max. 0.05360
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00147, max. 0.00245
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01267, max. 0.01434
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01350, max. 0.01352
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04632, max. 0.04672
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06169, max. 0.06745
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00134, max. 0.00204
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01231, max. 0.01693
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:30.290 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:30.292 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:30.292 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:30.293 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:30.293 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:30.293 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:31.746 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:31.807 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:31.811 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:31.814 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:31.817 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:31.820 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:31.823 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01384, max. 0.01384
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04644, max. 0.04666
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06529, max. 0.06705
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00133, max. 0.00193
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01381, max. 0.01687
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01384, max. 0.01385
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04645, max. 0.04668
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06336, max. 0.06714
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00127, max. 0.00183
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01149, max. 0.01698
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:32.113 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:32.115 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:32.115 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:32.115 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:32.116 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:32.116 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:33.435 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:33.496 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:33.499 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:33.502 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:33.505 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:33.508 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:33.511 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01403, max. 0.01403
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04997, max. 0.05034
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06578, max. 0.05262
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00145, max. 0.00255
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01280, max. 0.01447
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01402, max. 0.01403
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04648, max. 0.04648
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06449, max. 0.06621
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00133, max. 0.00216
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01181, max. 0.01677
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:33.801 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:33.803 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:33.803 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:33.804 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:33.804 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:33.804 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:35.137 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:35.197 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:35.201 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:35.204 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:35.207 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:35.210 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:35.213 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01414, max. 0.01414
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04787, max. 0.04864
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05451, max. 0.05660
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00132, max. 0.00217
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01304, max. 0.01440
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01411, max. 0.01413
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04644, max. 0.04625
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06245, max. 0.06517
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00126, max. 0.00200
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01365, max. 0.01651
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:35.502 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:35.504 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:35.504 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:35.504 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:35.504 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:35.504 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:36.833 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:36.894 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:36.897 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:36.901 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:36.904 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:36.907 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:36.910 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01398, max. 0.01398
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04662, max. 0.04664
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07016, max. 0.06223
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00143, max. 0.00205
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01421, max. 0.01604
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01414, max. 0.01462
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05438, max. 0.05171
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07634, max. 0.06280
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00154, max. 0.00341
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01313, max. 0.01618
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:37.199 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:37.201 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:37.201 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:37.201 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:37.201 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:37.201 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:38.536 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:38.597 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:38.601 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:38.604 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:38.607 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:38.610 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:38.613 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01760, max. 0.01859
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06141, max. 0.06089
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08031, max. 0.06751
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00179, max. 0.00404
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01750, max. 0.01880
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01351, max. 0.01353
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04603, max. 0.04636
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05468, max. 0.06248
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00134, max. 0.00171
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01214, max. 0.01601
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:38.902 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:38.903 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:38.903 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:38.904 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:38.904 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:38.904 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:40.344 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:40.401 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:40.404 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:40.406 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:40.409 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:40.411 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:40.413 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01317, max. 0.01319
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04516, max. 0.04595
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05521, max. 0.06637
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00127, max. 0.00191
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01299, max. 0.01771
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01317, max. 0.01319
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04516, max. 0.04595
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05271, max. 0.06637
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00131, max. 0.00174
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01225, max. 0.01764
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:40.693 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:40.695 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:40.695 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:40.695 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:40.695 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:40.695 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:41.061 INFO: node_feats: torch.Size([18, 2304])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:52:41.113 INFO: node_feats: torch.Size([18, 256])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:52:41.116 INFO: node_feats_out:torch.Size([18, 256]), torch.Size([2560]), 18
2024-11-06 08:52:41.118 INFO: linear_node_feats: torch.Size([18, 256])
2024-11-06 08:52:41.121 INFO: q_node_feats: torch.Size([18, 256])
2024-11-06 08:52:41.123 INFO: v_node_feats: torch.Size([18, 256])
2024-11-06 08:52:41.125 INFO: k_node_feats: torch.Size([18, 256])
q_vector from forward: torch.Size([18, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01317, max. 0.01319
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04516, max. 0.04595
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05323, max. 0.06637
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00123, max. 0.00189
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01120, max. 0.01765
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([18, 256])
2024-11-06 08:52:41.266 INFO: long_range_embedding: torch.Size([18, 256])
2024-11-06 08:52:41.267 INFO: long_range_energy: torch.Size([18, 1]), torch.Size([1])
2024-11-06 08:52:41.268 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-11-06 08:52:41.268 INFO: total_energy: torch.Size([1])
2024-11-06 08:52:41.268 INFO: node_energy_contributions: torch.Size([18, 4])
2024-11-06 08:52:41.268 INFO: node_energy: torch.Size([18])
2024-11-06 08:52:41.455 INFO: Epoch 42: head: default, loss=  0.0442, MAE_E_per_atom=     3.8 meV, MAE_F=    13.4 meV / A
2024-11-06 08:52:41.999 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:42.060 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:42.064 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:42.067 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:42.070 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:42.073 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:42.076 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01324, max. 0.01417
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05445, max. 0.05280
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07078, max. 0.06062
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00159, max. 0.00312
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01302, max. 0.01556
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01309, max. 0.01309
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04963, max. 0.05095
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05802, max. 0.04948
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00148, max. 0.00234
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01280, max. 0.01321
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:42.363 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:42.365 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:42.365 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:42.366 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:42.366 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:42.366 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:43.698 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:43.759 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:43.763 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:43.766 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:43.769 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:43.772 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:43.775 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01288, max. 0.01291
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04632, max. 0.04737
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05683, max. 0.06076
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00133, max. 0.00172
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01299, max. 0.01524
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01291, max. 0.01292
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04673, max. 0.04793
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06595, max. 0.05858
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00134, max. 0.00181
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01270, max. 0.01480
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:44.066 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:44.068 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:44.068 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:44.068 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:44.069 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:44.069 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:45.397 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:45.459 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:45.462 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:45.466 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:45.469 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:45.472 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:45.475 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01287, max. 0.01287
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04670, max. 0.04779
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06214, max. 0.06139
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00134, max. 0.00171
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01262, max. 0.01513
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01291, max. 0.01291
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04823, max. 0.05040
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04880, max. 0.05233
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00137, max. 0.00192
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01182, max. 0.01272
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:45.763 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:45.765 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:45.765 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:45.765 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:45.766 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:45.766 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:47.096 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:47.156 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:47.160 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:47.163 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:47.166 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:47.169 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:47.172 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01842, max. 0.01847
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06484, max. 0.06402
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08137, max. 0.06892
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00175, max. 0.00392
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01669, max. 0.02063
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01303, max. 0.01305
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04715, max. 0.04813
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06334, max. 0.06328
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00135, max. 0.00167
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01093, max. 0.01535
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:47.461 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:47.463 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:47.463 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:47.463 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:47.463 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:47.463 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:48.898 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:48.959 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:48.962 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:48.966 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:48.969 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:48.972 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:48.975 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01361, max. 0.01363
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04729, max. 0.04785
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05759, max. 0.06604
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00138, max. 0.00179
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01232, max. 0.01578
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01361, max. 0.01363
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04729, max. 0.04785
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06546, max. 0.06604
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00137, max. 0.00209
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01163, max. 0.01579
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:49.265 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:49.266 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:49.266 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:49.267 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:49.267 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:49.267 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:50.619 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:50.680 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:50.684 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:50.687 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:50.690 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:50.693 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:50.696 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01436, max. 0.01438
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04754, max. 0.04763
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06600, max. 0.06920
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00130, max. 0.00210
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01377, max. 0.01642
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01454, max. 0.01508
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05640, max. 0.05370
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08345, max. 0.06661
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00163, max. 0.00365
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01383, max. 0.01706
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:50.986 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:50.988 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:50.988 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:50.988 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:50.988 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:50.988 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:52.316 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:52.376 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:52.380 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:52.383 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:52.386 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:52.389 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:52.392 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01479, max. 0.01480
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04743, max. 0.04796
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07027, max. 0.07083
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00140, max. 0.00206
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01165, max. 0.01709
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01479, max. 0.01480
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04743, max. 0.04796
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06421, max. 0.07083
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00145, max. 0.00204
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01291, max. 0.01697
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:52.685 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:52.689 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:52.689 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:52.689 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:52.689 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:52.689 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:54.016 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:54.077 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:54.080 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:54.084 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:54.087 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:54.090 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:54.093 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01494, max. 0.01494
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04715, max. 0.04796
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07288, max. 0.07087
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00152, max. 0.00218
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01424, max. 0.01703
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01496, max. 0.01497
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04858, max. 0.04909
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06005, max. 0.06246
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00130, max. 0.00239
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01401, max. 0.01487
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:54.383 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:54.384 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:54.385 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:54.385 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:54.385 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:54.385 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:55.811 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:55.872 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:55.875 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:55.879 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:55.882 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:55.885 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:55.888 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01428, max. 0.01429
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04674, max. 0.04713
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06841, max. 0.06736
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00131, max. 0.00223
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01143, max. 0.01628
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01429, max. 0.01429
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05008, max. 0.05064
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06703, max. 0.05435
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00143, max. 0.00262
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01272, max. 0.01429
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:56.176 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:56.178 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:56.178 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:56.178 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:56.178 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:56.178 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:57.519 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:57.580 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:57.584 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:57.587 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:57.590 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:57.593 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:57.596 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01676, max. 0.01846
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06154, max. 0.06158
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08174, max. 0.06871
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00185, max. 0.00409
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01724, max. 0.01973
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01351, max. 0.01351
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04680, max. 0.04771
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07089, max. 0.06057
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00137, max. 0.00197
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01329, max. 0.01475
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:57.885 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:57.887 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:57.887 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:57.888 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:57.888 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:57.888 INFO: node_energy: torch.Size([36])
2024-11-06 08:52:59.235 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:59.296 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:52:59.299 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:52:59.302 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:52:59.305 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:52:59.308 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:52:59.312 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01237, max. 0.01239
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04677, max. 0.04849
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06247, max. 0.05602
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00133, max. 0.00179
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00974, max. 0.01344
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01237, max. 0.01239
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04677, max. 0.04849
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05300, max. 0.05602
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00134, max. 0.00152
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01040, max. 0.01346
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:52:59.600 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:52:59.602 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:52:59.602 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:52:59.602 INFO: total_energy: torch.Size([2])
2024-11-06 08:52:59.602 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:52:59.602 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:00.920 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:00.981 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:00.984 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:00.988 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:00.991 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:00.994 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:00.997 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01173, max. 0.01173
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04858, max. 0.05177
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04876, max. 0.04595
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00133, max. 0.00167
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01209, max. 0.01043
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01169, max. 0.01169
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04715, max. 0.04933
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06024, max. 0.05271
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00130, max. 0.00148
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01023, max. 0.01235
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:01.290 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:01.291 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:01.292 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:01.292 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:01.292 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:01.292 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:02.624 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:02.685 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:02.688 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:02.691 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:02.694 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:02.698 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:02.701 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01139, max. 0.01139
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04796, max. 0.05047
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06418, max. 0.05056
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00131, max. 0.00153
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01053, max. 0.01143
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01816, max. 0.01695
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06575, max. 0.06579
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07485, max. 0.06524
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00168, max. 0.00337
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01560, max. 0.02180
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:03.087 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:03.089 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:03.089 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:03.090 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:03.090 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:03.090 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:04.431 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:04.492 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:04.496 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:04.499 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:04.502 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:04.505 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:04.508 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01144, max. 0.01144
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05133, max. 0.05403
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05590, max. 0.04876
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00151, max. 0.00207
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01408, max. 0.01334
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01212, max. 0.01306
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05810, max. 0.05624
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06829, max. 0.05961
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00163, max. 0.00271
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01431, max. 0.01895
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:04.796 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:04.798 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:04.798 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:04.798 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:04.798 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:04.798 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:06.142 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:06.202 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:06.206 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:06.209 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:06.212 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:06.215 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:06.218 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01189, max. 0.01192
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04697, max. 0.04896
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05723, max. 0.05725
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00130, max. 0.00158
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01108, max. 0.01292
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01189, max. 0.01193
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04697, max. 0.04896
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06304, max. 0.05725
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00130, max. 0.00149
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00960, max. 0.01306
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:06.508 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:06.509 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:06.509 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:06.510 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:06.510 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:06.510 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:07.836 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:07.894 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:07.896 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:07.899 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:07.901 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:07.904 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:07.906 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01306, max. 0.01309
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04598, max. 0.04645
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05767, max. 0.06407
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00127, max. 0.00189
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01212, max. 0.01626
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01307, max. 0.01309
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04598, max. 0.04645
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05342, max. 0.06407
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00132, max. 0.00172
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01149, max. 0.01617
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:08.185 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:08.187 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:08.187 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:08.187 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:08.187 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:08.187 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:08.544 INFO: node_feats: torch.Size([18, 2304])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:53:08.598 INFO: node_feats: torch.Size([18, 256])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:53:08.601 INFO: node_feats_out:torch.Size([18, 256]), torch.Size([2560]), 18
2024-11-06 08:53:08.603 INFO: linear_node_feats: torch.Size([18, 256])
2024-11-06 08:53:08.606 INFO: q_node_feats: torch.Size([18, 256])
2024-11-06 08:53:08.608 INFO: v_node_feats: torch.Size([18, 256])
2024-11-06 08:53:08.610 INFO: k_node_feats: torch.Size([18, 256])
q_vector from forward: torch.Size([18, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01307, max. 0.01309
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04598, max. 0.04645
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05642, max. 0.06407
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00126, max. 0.00187
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01023, max. 0.01620
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([18, 256])
2024-11-06 08:53:08.750 INFO: long_range_embedding: torch.Size([18, 256])
2024-11-06 08:53:08.751 INFO: long_range_energy: torch.Size([18, 1]), torch.Size([1])
2024-11-06 08:53:08.751 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-11-06 08:53:08.752 INFO: total_energy: torch.Size([1])
2024-11-06 08:53:08.752 INFO: node_energy_contributions: torch.Size([18, 4])
2024-11-06 08:53:08.752 INFO: node_energy: torch.Size([18])
2024-11-06 08:53:08.939 INFO: Epoch 45: head: default, loss=  0.0443, MAE_E_per_atom=     3.6 meV, MAE_F=    13.4 meV / A
2024-11-06 08:53:09.010 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:09.070 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:09.074 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:09.077 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:09.080 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:09.083 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:09.086 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01271, max. 0.01360
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05611, max. 0.05405
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07288, max. 0.06128
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00159, max. 0.00304
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01378, max. 0.01749
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01255, max. 0.01258
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04667, max. 0.04815
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05598, max. 0.06163
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00131, max. 0.00161
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01106, max. 0.01393
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:09.373 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:09.375 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:09.375 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:09.375 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:09.375 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:09.375 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:10.702 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:10.763 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:10.766 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:10.769 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:10.772 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:10.775 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:10.779 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01310, max. 0.01313
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04648, max. 0.04754
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06672, max. 0.06473
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00128, max. 0.00201
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01099, max. 0.01457
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01310, max. 0.01313
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04648, max. 0.04754
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06722, max. 0.06473
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00126, max. 0.00176
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01058, max. 0.01466
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:11.070 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:11.072 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:11.072 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:11.072 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:11.073 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:11.073 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:12.501 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:12.562 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:12.565 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:12.568 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:12.572 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:12.575 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:12.578 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01358, max. 0.01358
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04648, max. 0.04714
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07038, max. 0.06644
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00135, max. 0.00193
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01227, max. 0.01487
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01358, max. 0.01361
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04649, max. 0.04717
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06650, max. 0.06654
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00123, max. 0.00196
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01247, max. 0.01484
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:12.866 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:12.867 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:12.868 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:12.868 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:12.868 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:12.868 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:14.192 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:14.253 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:14.256 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:14.259 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:14.262 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:14.266 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:14.269 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01349, max. 0.01349
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04944, max. 0.05056
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06765, max. 0.05377
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00137, max. 0.00247
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01286, max. 0.01322
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01520, max. 0.01816
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06105, max. 0.06099
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.08371, max. 0.06919
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00179, max. 0.00408
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00009, max. 0.00007
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01671, max. 0.02015
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:14.556 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:14.558 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:14.558 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:14.558 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:14.558 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:14.558 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:15.903 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:15.964 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:15.967 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:15.971 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:15.974 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:15.977 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:15.980 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01265, max. 0.01265
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04693, max. 0.04841
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07082, max. 0.05988
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00128, max. 0.00185
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01193, max. 0.01329
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01264, max. 0.01265
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04791, max. 0.05020
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05361, max. 0.05364
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00125, max. 0.00193
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01158, max. 0.01168
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:16.269 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:16.270 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:16.270 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:16.271 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:16.271 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:16.271 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:17.612 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:17.672 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:17.676 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:17.679 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:17.682 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:17.685 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:17.688 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01139, max. 0.01143
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04672, max. 0.04903
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05694, max. 0.05547
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00123, max. 0.00153
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01038, max. 0.01221
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01139, max. 0.01139
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04672, max. 0.04901
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06158, max. 0.05536
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00122, max. 0.00150
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01001, max. 0.01218
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:17.976 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:17.978 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:17.978 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:17.978 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:17.979 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:17.979 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:19.294 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:19.355 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:19.359 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:19.362 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:19.365 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:19.368 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:19.371 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01088, max. 0.01086
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04833, max. 0.05248
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04780, max. 0.04525
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00127, max. 0.00146
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01189, max. 0.01025
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01080, max. 0.01048
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04686, max. 0.05004
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05783, max. 0.04997
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00128, max. 0.00142
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00915, max. 0.01094
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:19.660 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:19.662 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:19.662 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:19.662 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:19.662 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:19.662 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:21.092 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:21.152 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:21.156 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:21.159 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:21.162 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:21.165 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:21.169 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01113, max. 0.01113
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05107, max. 0.05487
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04959, max. 0.04505
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00142, max. 0.00171
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01364, max. 0.01285
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01081, max. 0.01017
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04742, max. 0.05127
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05860, max. 0.04637
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00124, max. 0.00125
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01021, max. 0.00974
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:21.456 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:21.458 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:21.458 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:21.458 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:21.459 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:21.459 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:22.792 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:22.853 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:22.857 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:22.860 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:22.863 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:22.866 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:22.869 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01183, max. 0.01143
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05846, max. 0.05782
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06013, max. 0.05428
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00153, max. 0.00210
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01393, max. 0.01916
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01744, max. 0.01498
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06694, max. 0.06799
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07007, max. 0.06274
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00161, max. 0.00277
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01729, max. 0.02237
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:23.156 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:23.157 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:23.157 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:23.158 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:23.158 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:23.158 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:24.508 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:24.568 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:24.572 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:24.575 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:24.578 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:24.581 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:24.584 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01081, max. 0.01081
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04644, max. 0.05045
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04401, max. 0.04366
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00130, max. 0.00122
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00924, max. 0.00987
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01081, max. 0.01037
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04644, max. 0.05045
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05649, max. 0.04685
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00125, max. 0.00115
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00921, max. 0.00975
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:24.874 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:24.875 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:24.876 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:24.876 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:24.876 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:24.876 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:26.209 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:26.270 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:26.273 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:26.276 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:26.280 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:26.283 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:26.286 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01087, max. 0.01084
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04748, max. 0.05213
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04564, max. 0.04431
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00127, max. 0.00142
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01241, max. 0.01054
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01079, max. 0.01047
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04603, max. 0.04970
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05660, max. 0.04641
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00127, max. 0.00138
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00929, max. 0.01012
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:26.574 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:26.576 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:26.576 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:26.576 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:26.577 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:26.577 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:27.993 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:28.054 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:28.057 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:28.060 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:28.063 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:28.066 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:28.070 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01076, max. 0.01075
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04568, max. 0.04886
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04986, max. 0.04918
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00124, max. 0.00130
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00916, max. 0.01102
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01076, max. 0.01075
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04568, max. 0.04886
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05398, max. 0.04918
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00121, max. 0.00141
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00937, max. 0.01093
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:28.360 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:28.362 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:28.362 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:28.362 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:28.362 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:28.363 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:29.690 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:29.751 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:29.755 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:29.758 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:29.761 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:29.764 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:29.767 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01117, max. 0.01121
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04538, max. 0.04809
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06175, max. 0.05345
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00118, max. 0.00143
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00923, max. 0.01206
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01507, max. 0.01603
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06229, max. 0.06330
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07482, max. 0.06358
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00157, max. 0.00336
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00008, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01410, max. 0.02108
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:30.055 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:30.057 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:30.057 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:30.057 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:30.057 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:30.057 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:31.403 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:31.464 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:31.467 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:31.470 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:31.473 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:31.476 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:31.480 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01138, max. 0.01138
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04889, max. 0.05201
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06059, max. 0.04928
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00137, max. 0.00207
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01316, max. 0.01295
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01177, max. 0.01253
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05461, max. 0.05416
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06986, max. 0.05908
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00147, max. 0.00268
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01313, max. 0.01736
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:31.766 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:31.768 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:31.768 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:31.768 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:31.769 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:31.769 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:33.112 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:33.173 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:33.176 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:33.179 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:33.182 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:33.185 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:33.188 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01111, max. 0.01111
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04566, max. 0.04857
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06185, max. 0.05615
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00122, max. 0.00150
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01001, max. 0.01251
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01114, max. 0.01114
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04607, max. 0.04911
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06581, max. 0.05423
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00122, max. 0.00161
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01067, max. 0.01210
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:33.476 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:33.478 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:33.478 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:33.478 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:33.479 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:33.479 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:34.805 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:34.862 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:34.865 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:34.867 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:34.870 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:34.872 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:34.874 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01218, max. 0.01221
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04604, max. 0.04754
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05732, max. 0.05967
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00125, max. 0.00174
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01107, max. 0.01437
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01218, max. 0.01221
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04604, max. 0.04754
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05302, max. 0.05967
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00128, max. 0.00158
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01051, max. 0.01429
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:35.155 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:35.156 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:35.156 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:35.156 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:35.157 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:35.157 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:35.515 INFO: node_feats: torch.Size([18, 2304])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:53:35.662 INFO: node_feats: torch.Size([18, 256])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:53:35.665 INFO: node_feats_out:torch.Size([18, 256]), torch.Size([2560]), 18
2024-11-06 08:53:35.667 INFO: linear_node_feats: torch.Size([18, 256])
2024-11-06 08:53:35.670 INFO: q_node_feats: torch.Size([18, 256])
2024-11-06 08:53:35.672 INFO: v_node_feats: torch.Size([18, 256])
2024-11-06 08:53:35.674 INFO: k_node_feats: torch.Size([18, 256])
q_vector from forward: torch.Size([18, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01218, max. 0.01221
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04604, max. 0.04754
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05686, max. 0.05967
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00125, max. 0.00172
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00926, max. 0.01434
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([18, 256])
2024-11-06 08:53:35.815 INFO: long_range_embedding: torch.Size([18, 256])
2024-11-06 08:53:35.817 INFO: long_range_energy: torch.Size([18, 1]), torch.Size([1])
2024-11-06 08:53:35.817 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-11-06 08:53:35.817 INFO: total_energy: torch.Size([1])
2024-11-06 08:53:35.817 INFO: node_energy_contributions: torch.Size([18, 4])
2024-11-06 08:53:35.817 INFO: node_energy: torch.Size([18])
2024-11-06 08:53:36.006 INFO: Epoch 48: head: default, loss=  0.0410, MAE_E_per_atom=     3.4 meV, MAE_F=    12.8 meV / A
2024-11-06 08:53:36.535 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:36.596 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:36.599 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:36.603 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:36.606 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:36.609 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:36.612 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01160, max. 0.01214
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05528, max. 0.05595
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06725, max. 0.05823
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00154, max. 0.00248
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01332, max. 0.01841
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01065, max. 0.01062
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04702, max. 0.05159
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04768, max. 0.04585
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00127, max. 0.00154
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01114, max. 0.01010
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:36.900 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:36.902 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:36.902 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:36.902 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:36.903 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:36.903 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:38.233 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:38.295 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:38.298 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:38.301 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:38.304 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:38.308 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:38.311 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01654, max. 0.01532
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06486, max. 0.06757
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.07112, max. 0.06293
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00167, max. 0.00304
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01555, max. 0.02284
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01055, max. 0.01024
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04582, max. 0.05032
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05639, max. 0.04970
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00126, max. 0.00134
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00874, max. 0.01099
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:38.604 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:38.606 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:38.606 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:38.606 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:38.606 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:38.606 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:39.944 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:40.005 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:40.009 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:40.012 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:40.015 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:40.018 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:40.021 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01058, max. 0.01057
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04611, max. 0.05144
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04818, max. 0.04395
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00128, max. 0.00120
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00899, max. 0.00976
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01058, max. 0.01014
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04611, max. 0.05144
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05641, max. 0.04607
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00128, max. 0.00115
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00880, max. 0.00987
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:40.309 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:40.311 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:40.311 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:40.311 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:40.311 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:40.311 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:41.638 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:41.699 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:41.703 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:41.706 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:41.709 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:41.712 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:41.715 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01090, max. 0.01090
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05046, max. 0.05652
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04783, max. 0.04447
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00149, max. 0.00152
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01360, max. 0.01419
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01058, max. 0.01053
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04637, max. 0.05226
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05104, max. 0.04360
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00130, max. 0.00125
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00925, max. 0.00960
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:42.004 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:42.006 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:42.006 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:42.006 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:42.006 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:42.006 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:43.340 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:43.495 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:43.499 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:43.502 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:43.505 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:43.508 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:43.511 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01056, max. 0.00995
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04704, max. 0.05342
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05461, max. 0.04421
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00131, max. 0.00121
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01036, max. 0.00966
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01057, max. 0.01056
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04657, max. 0.05282
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04150, max. 0.04149
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00132, max. 0.00124
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00912, max. 0.00961
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:43.801 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:43.803 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:43.803 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:43.803 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:43.803 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:43.804 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:45.139 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:45.200 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:45.204 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:45.207 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:45.210 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:45.213 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:45.216 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01166, max. 0.01140
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06001, max. 0.06106
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06138, max. 0.05468
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00166, max. 0.00192
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01479, max. 0.02273
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01053, max. 0.01049
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04670, max. 0.05306
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05003, max. 0.04170
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00132, max. 0.00127
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00943, max. 0.00992
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:45.505 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:45.506 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:45.507 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:45.507 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:45.507 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:45.507 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:46.836 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:46.896 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:46.900 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:46.903 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:46.906 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:46.909 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:46.912 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01046, max. 0.01045
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04607, max. 0.05224
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04172, max. 0.04111
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00130, max. 0.00122
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00901, max. 0.00937
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01078, max. 0.01078
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05065, max. 0.05659
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04743, max. 0.04394
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00148, max. 0.00148
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01374, max. 0.01485
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:47.203 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:47.205 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:47.205 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:47.205 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:47.205 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:47.205 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:48.537 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:48.598 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:48.601 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:48.604 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:48.607 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:48.610 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:48.613 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01673, max. 0.01407
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06656, max. 0.06969
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06988, max. 0.06174
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00170, max. 0.00267
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01760, max. 0.02368
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01045, max. 0.01043
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04685, max. 0.05356
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04222, max. 0.04237
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00130, max. 0.00131
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01162, max. 0.01023
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:48.902 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:48.903 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:48.903 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:48.904 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:48.904 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:48.904 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:50.245 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:50.305 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:50.308 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:50.311 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:50.315 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:50.318 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:50.321 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01029, max. 0.00999
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04442, max. 0.04976
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05139, max. 0.04191
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00123, max. 0.00119
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00862, max. 0.00978
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01029, max. 0.00987
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04442, max. 0.04976
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05302, max. 0.04377
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00123, max. 0.00110
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00854, max. 0.00989
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:50.629 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:50.631 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:50.631 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:50.631 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:50.631 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:50.632 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:52.057 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:52.118 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:52.121 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:52.124 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:52.127 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:52.131 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:52.134 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01022, max. 0.01021
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04377, max. 0.04870
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04727, max. 0.04442
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00119, max. 0.00111
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00873, max. 0.01042
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01020, max. 0.00962
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04420, max. 0.04924
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05508, max. 0.04444
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00118, max. 0.00115
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00892, max. 0.00998
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:52.423 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:52.425 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:52.425 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:52.425 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:52.425 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:52.425 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:53.763 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:53.824 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:53.827 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:53.831 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:53.834 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:53.837 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:53.840 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01040, max. 0.01040
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04664, max. 0.05200
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04928, max. 0.04362
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00132, max. 0.00148
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01204, max. 0.01222
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01019, max. 0.01017
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04463, max. 0.05035
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04214, max. 0.04060
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00119, max. 0.00120
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01036, max. 0.00961
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:54.129 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:54.131 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:54.131 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:54.131 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:54.132 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:54.132 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:55.458 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:55.520 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:55.523 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:55.526 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:55.529 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:55.533 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:55.536 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01003, max. 0.00999
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04309, max. 0.04809
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05049, max. 0.04560
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00113, max. 0.00109
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00870, max. 0.01020
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01001, max. 0.00944
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04351, max. 0.04864
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05415, max. 0.04359
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00112, max. 0.00110
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00863, max. 0.00986
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:55.825 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:55.827 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:55.827 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:55.827 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:55.827 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:55.827 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:57.156 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:57.217 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:57.220 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:57.223 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:57.226 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:57.230 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:57.233 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00993, max. 0.00992
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04290, max. 0.04841
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04492, max. 0.04175
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00111, max. 0.00104
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00840, max. 0.00958
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01052, max. 0.01047
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05221, max. 0.05490
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05711, max. 0.05106
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00136, max. 0.00187
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01263, max. 0.01860
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:57.521 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:57.523 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:57.523 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:57.524 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:57.524 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:57.524 INFO: node_energy: torch.Size([36])
2024-11-06 08:53:58.863 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:58.924 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:53:58.928 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:53:58.931 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:53:58.934 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:53:58.937 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:53:58.940 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00985, max. 0.00944
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04284, max. 0.04895
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05081, max. 0.04197
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00112, max. 0.00100
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00822, max. 0.00902
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00985, max. 0.00985
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04284, max. 0.04895
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03992, max. 0.04115
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00112, max. 0.00106
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00824, max. 0.00899
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:53:59.228 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:53:59.230 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:53:59.230 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:53:59.230 INFO: total_energy: torch.Size([2])
2024-11-06 08:53:59.230 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:53:59.231 INFO: node_energy: torch.Size([36])
2024-11-06 08:54:00.655 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:54:00.716 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:54:00.719 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:54:00.722 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:54:00.725 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:54:00.728 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:54:00.731 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01522, max. 0.01305
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06095, max. 0.06543
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06106, max. 0.05542
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00157, max. 0.00233
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01555, max. 0.02155
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00979, max. 0.00950
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04278, max. 0.04938
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04850, max. 0.03988
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00112, max. 0.00110
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00833, max. 0.00836
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:54:01.019 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:54:01.021 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:54:01.021 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:54:01.021 INFO: total_energy: torch.Size([2])
2024-11-06 08:54:01.022 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:54:01.022 INFO: node_energy: torch.Size([36])
2024-11-06 08:54:02.360 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:54:02.416 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:54:02.419 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:54:02.422 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:54:02.424 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:54:02.426 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:54:02.429 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01043, max. 0.01046
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04538, max. 0.04870
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05316, max. 0.05149
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00123, max. 0.00143
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00961, max. 0.01215
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01043, max. 0.01046
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04538, max. 0.04870
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05045, max. 0.05149
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00123, max. 0.00128
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00926, max. 0.01209
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:54:02.709 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:54:02.710 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:54:02.710 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:54:02.711 INFO: total_energy: torch.Size([2])
2024-11-06 08:54:02.711 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:54:02.711 INFO: node_energy: torch.Size([36])
2024-11-06 08:54:03.069 INFO: node_feats: torch.Size([18, 2304])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:54:03.121 INFO: node_feats: torch.Size([18, 256])
energy graph: torch.Size([1]) torch.Size([18])
2024-11-06 08:54:03.124 INFO: node_feats_out:torch.Size([18, 256]), torch.Size([2560]), 18
2024-11-06 08:54:03.127 INFO: linear_node_feats: torch.Size([18, 256])
2024-11-06 08:54:03.129 INFO: q_node_feats: torch.Size([18, 256])
2024-11-06 08:54:03.131 INFO: v_node_feats: torch.Size([18, 256])
2024-11-06 08:54:03.134 INFO: k_node_feats: torch.Size([18, 256])
q_vector from forward: torch.Size([18, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01043, max. 0.01046
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04538, max. 0.04870
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05377, max. 0.05149
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00123, max. 0.00142
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00888, max. 0.01215
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([18, 256])
2024-11-06 08:54:03.273 INFO: long_range_embedding: torch.Size([18, 256])
2024-11-06 08:54:03.274 INFO: long_range_energy: torch.Size([18, 1]), torch.Size([1])
2024-11-06 08:54:03.274 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-11-06 08:54:03.275 INFO: total_energy: torch.Size([1])
2024-11-06 08:54:03.275 INFO: node_energy_contributions: torch.Size([18, 4])
2024-11-06 08:54:03.275 INFO: node_energy: torch.Size([18])
2024-11-06 08:54:03.462 INFO: Epoch 51: head: default, loss=  0.0400, MAE_E_per_atom=     3.2 meV, MAE_F=    12.7 meV / A
2024-11-06 08:54:04.008 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:54:04.069 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:54:04.073 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:54:04.076 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:54:04.079 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:54:04.082 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:54:04.085 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01045, max. 0.01045
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04740, max. 0.05415
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04332, max. 0.04044
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00132, max. 0.00132
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01293, max. 0.01429
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00981, max. 0.00980
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04325, max. 0.05025
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03864, max. 0.03801
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00115, max. 0.00110
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00837, max. 0.00830
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:54:04.374 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:54:04.376 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:54:04.376 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:54:04.376 INFO: total_energy: torch.Size([2])
2024-11-06 08:54:04.377 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:54:04.377 INFO: node_energy: torch.Size([36])
2024-11-06 08:54:05.698 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:54:05.759 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:54:05.763 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:54:05.766 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:54:05.769 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:54:05.772 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:54:05.776 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01001, max. 0.00998
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04511, max. 0.05333
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.03831, max. 0.03930
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00123, max. 0.00124
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01150, max. 0.01019
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00984, max. 0.00984
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04365, max. 0.05093
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04227, max. 0.03716
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00119, max. 0.00111
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00863, max. 0.00867
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:54:06.071 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:54:06.073 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:54:06.073 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:54:06.073 INFO: total_energy: torch.Size([2])
2024-11-06 08:54:06.074 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:54:06.074 INFO: node_energy: torch.Size([36])
2024-11-06 08:54:07.403 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:54:07.464 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:54:07.468 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:54:07.471 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:54:07.474 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:54:07.477 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:54:07.480 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00989, max. 0.00960
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04386, max. 0.05134
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04798, max. 0.03960
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00123, max. 0.00119
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00856, max. 0.00855
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00989, max. 0.00949
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04386, max. 0.05134
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04972, max. 0.04175
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00123, max. 0.00109
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00847, max. 0.00850
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:54:07.869 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:54:07.871 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:54:07.871 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:54:07.872 INFO: total_energy: torch.Size([2])
2024-11-06 08:54:07.872 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:54:07.872 INFO: node_energy: torch.Size([36])
2024-11-06 08:54:09.197 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:54:09.258 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:54:09.261 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:54:09.265 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:54:09.268 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:54:09.271 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:54:09.274 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01661, max. 0.01362
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.06549, max. 0.06942
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.06971, max. 0.06097
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00169, max. 0.00234
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00007, max. 0.00006
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01788, max. 0.02354
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01102, max. 0.01076
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.05630, max. 0.05864
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05448, max. 0.05017
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00156, max. 0.00171
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00006, max. 0.00005
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01357, max. 0.02102
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:54:09.563 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:54:09.564 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:54:09.564 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:54:09.565 INFO: total_energy: torch.Size([2])
2024-11-06 08:54:09.565 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:54:09.565 INFO: node_energy: torch.Size([36])
2024-11-06 08:54:10.913 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:54:10.974 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:54:10.978 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:54:10.981 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:54:10.984 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:54:10.987 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:54:10.990 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01000, max. 0.00996
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04418, max. 0.05129
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04769, max. 0.04026
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00125, max. 0.00119
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00882, max. 0.00882
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.00998, max. 0.00942
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04466, max. 0.05191
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05129, max. 0.04154
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00125, max. 0.00115
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00966, max. 0.00857
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:54:11.280 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:54:11.281 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:54:11.281 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:54:11.282 INFO: total_energy: torch.Size([2])
2024-11-06 08:54:11.282 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:54:11.282 INFO: node_energy: torch.Size([36])
2024-11-06 08:54:12.619 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:54:12.680 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:54:12.684 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:54:12.687 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:54:12.690 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:54:12.693 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:54:12.696 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01038, max. 0.01038
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04862, max. 0.05544
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04666, max. 0.04276
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00144, max. 0.00141
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.01312, max. 0.01481
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01003, max. 0.01002
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04428, max. 0.05121
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04052, max. 0.04016
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00126, max. 0.00117
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00004, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00862, max. 0.00859
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:54:12.985 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:54:12.987 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:54:12.987 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:54:12.987 INFO: total_energy: torch.Size([2])
2024-11-06 08:54:12.987 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:54:12.987 INFO: node_energy: torch.Size([36])
2024-11-06 08:54:14.311 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:54:14.371 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:54:14.375 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:54:14.378 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:54:14.381 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:54:14.384 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:54:14.387 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01003, max. 0.00973
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04442, max. 0.05111
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04926, max. 0.04005
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00125, max. 0.00120
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00875, max. 0.00851
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01001, max. 0.00944
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04493, max. 0.05175
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05228, max. 0.04235
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00125, max. 0.00114
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00967, max. 0.00856
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:54:14.678 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:54:14.680 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:54:14.680 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:54:14.680 INFO: total_energy: torch.Size([2])
2024-11-06 08:54:14.680 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:54:14.681 INFO: node_energy: torch.Size([36])
2024-11-06 08:54:16.009 INFO: node_feats: torch.Size([36, 2304])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:54:16.164 INFO: node_feats: torch.Size([36, 256])
energy graph: torch.Size([2]) torch.Size([36])
2024-11-06 08:54:16.168 INFO: node_feats_out:torch.Size([36, 256]), torch.Size([2560]), 36
2024-11-06 08:54:16.171 INFO: linear_node_feats: torch.Size([36, 256])
2024-11-06 08:54:16.174 INFO: q_node_feats: torch.Size([36, 256])
2024-11-06 08:54:16.177 INFO: v_node_feats: torch.Size([36, 256])
2024-11-06 08:54:16.180 INFO: k_node_feats: torch.Size([36, 256])
q_vector from forward: torch.Size([36, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01003, max. 0.00962
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04459, max. 0.05110
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.05138, max. 0.04296
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00124, max. 0.00110
weighted_values: torch.Size([18, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(False, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. nan, max. nan
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) False
new_sum_term: torch.Size([18, 256]) 18
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(False, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. nan, max. nan
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from value or key: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([18, 256, 8, 15, 15])
term from query: torch.Size([18, 256, 983]) torch.Size([18, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([18, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.01003, max. 0.00999
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.04459, max. 0.05109
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.04923, max. 0.04216
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00124, max. 0.00118
weighted_values: torch.Size([18, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00005, max. 0.00004
optimized_inverse: torch.Size([18, 3]) torch.Size([18, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([18, 983, 256]) True
new_sum_term: torch.Size([18, 256]) 18
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.00898, max. 0.00879
node_feats_list: torch.Size([18, 983, 256]) torch.Size([18, 256])
results: torch.Size([36, 256])
2024-11-06 08:54:16.469 INFO: long_range_embedding: torch.Size([36, 256])
2024-11-06 08:54:16.470 INFO: long_range_energy: torch.Size([36, 1]), torch.Size([2])
2024-11-06 08:54:16.471 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-11-06 08:54:16.471 INFO: total_energy: torch.Size([2])
2024-11-06 08:54:16.471 INFO: node_energy_contributions: torch.Size([36, 4])
2024-11-06 08:54:16.471 INFO: node_energy: torch.Size([36])
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/autograd/graph.py:825: UserWarning: Error detected in MulBackward0. Traceback of forward call that caused the error:
  File "/trace/group/mcgaughey/hariharr/mace_exploration/fourier_attention_mace/mace/./mace/cli/run_train.py", line 783, in <module>
    main()
  File "/trace/group/mcgaughey/hariharr/mace_exploration/fourier_attention_mace/mace/./mace/cli/run_train.py", line 64, in main
    run(args)
  File "/trace/group/mcgaughey/hariharr/mace_exploration/fourier_attention_mace/mace/./mace/cli/run_train.py", line 577, in run
    tools.train(
  File "/trace/group/mcgaughey/hariharr/mace_exploration/fourier_attention_mace/mace/mace/tools/train.py", line 219, in train
    train_one_epoch(
  File "/trace/group/mcgaughey/hariharr/mace_exploration/fourier_attention_mace/mace/mace/tools/train.py", line 341, in train_one_epoch
    _, opt_metrics = take_step(
  File "/trace/group/mcgaughey/hariharr/mace_exploration/fourier_attention_mace/mace/mace/tools/train.py", line 371, in take_step
    output = model(
  File "/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/trace/group/mcgaughey/hariharr/mace_exploration/fourier_attention_mace/mace/mace/modules/models.py", line 623, in forward
    long_range_embedding = self.ewald_potential(q_node_feats, v_node_feats, k_node_feats, data)
  File "/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/trace/group/mcgaughey/hariharr/mace_exploration/fourier_attention_mace/mace/mace/modules/ewald_mace.py", line 95, in forward
    real_space_weighted_values = self.compute_inverse_transform_optimized(r[mask], weighted_values, box[i])
  File "/trace/group/mcgaughey/hariharr/mace_exploration/fourier_attention_mace/mace/mace/modules/ewald_mace.py", line 399, in compute_inverse_transform_optimized
    term_before_sum = q.unsqueeze(2) * eikx_valid.unsqueeze(2).unsqueeze(3) * eiky_valid.unsqueeze(2).unsqueeze(3) * eikz_valid.unsqueeze(2).unsqueeze(3)
 (Triggered internally at ../torch/csrc/autograd/python_anomaly_mode.cpp:110.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Traceback (most recent call last):
  File "/trace/group/mcgaughey/hariharr/mace_exploration/fourier_attention_mace/mace/./mace/cli/run_train.py", line 783, in <module>
    main()
  File "/trace/group/mcgaughey/hariharr/mace_exploration/fourier_attention_mace/mace/./mace/cli/run_train.py", line 64, in main
    run(args)
  File "/trace/group/mcgaughey/hariharr/mace_exploration/fourier_attention_mace/mace/./mace/cli/run_train.py", line 577, in run
    tools.train(
  File "/trace/group/mcgaughey/hariharr/mace_exploration/fourier_attention_mace/mace/mace/tools/train.py", line 219, in train
    train_one_epoch(
  File "/trace/group/mcgaughey/hariharr/mace_exploration/fourier_attention_mace/mace/mace/tools/train.py", line 341, in train_one_epoch
    _, opt_metrics = take_step(
  File "/trace/group/mcgaughey/hariharr/mace_exploration/fourier_attention_mace/mace/mace/tools/train.py", line 371, in take_step
    output = model(
  File "/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/trace/group/mcgaughey/hariharr/mace_exploration/fourier_attention_mace/mace/mace/modules/models.py", line 649, in forward
    forces, virials, stress, hessian = get_outputs(
  File "/trace/group/mcgaughey/hariharr/mace_exploration/fourier_attention_mace/mace/mace/modules/utils.py", line 193, in get_outputs
    compute_forces(
  File "/trace/group/mcgaughey/hariharr/mace_exploration/fourier_attention_mace/mace/mace/modules/utils.py", line 27, in compute_forces
    gradient = torch.autograd.grad(
  File "/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/autograd/__init__.py", line 496, in grad
    result = _engine_run_backward(
  File "/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Function 'MulBackward0' returned nan values in its 1th output.
