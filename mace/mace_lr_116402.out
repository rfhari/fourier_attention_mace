/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))
/trace/group/mcgaughey/hariharr/mace_exploration/fourier_attention_mace/mace/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(f=model_path, map_location=device)
/trace/group/mcgaughey/hariharr/mace_exploration/fourier_attention_mace/mace/plot_binding_curve.py:72: DeprecationWarning: Please use atoms.calc = calc
  all_list[i].set_calculator(calculator)
No dtype selected, switching to float64 to match model dtype.
energy graph: torch.Size([1]) torch.Size([20])
energy graph: torch.Size([1]) torch.Size([20])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02266, max. 0.02265
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.07594, max. 0.07226
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10515, max. 0.10223
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00380, max. 0.00290
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00010
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02556, max. 0.02651
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
energy graph: torch.Size([1]) torch.Size([20])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02278, max. 0.02277
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.07584, max. 0.07396
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10850, max. 0.09933
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00429, max. 0.00300
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00010
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02647, max. 0.02644
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
energy graph: torch.Size([1]) torch.Size([20])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02299, max. 0.02298
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.07574, max. 0.07538
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10765, max. 0.09460
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00461, max. 0.00313
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00010
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02618, max. 0.02681
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
energy graph: torch.Size([1]) torch.Size([20])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02296, max. 0.02295
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.07583, max. 0.07595
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10504, max. 0.09465
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00482, max. 0.00312
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00010
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02553, max. 0.02632
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
energy graph: torch.Size([1]) torch.Size([20])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02294, max. 0.02293
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.07604, max. 0.07748
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10503, max. 0.10238
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00479, max. 0.00318
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00010
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02533, max. 0.02636
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
energy graph: torch.Size([1]) torch.Size([20])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02294, max. 0.02293
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.07617, max. 0.07756
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10508, max. 0.10626
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00482, max. 0.00331
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00011
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02516, max. 0.02644
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
energy graph: torch.Size([1]) torch.Size([20])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02294, max. 0.02293
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.07619, max. 0.07676
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10507, max. 0.10623
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00485, max. 0.00334
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00011
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02531, max. 0.02639
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
energy graph: torch.Size([1]) torch.Size([20])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02294, max. 0.02293
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.07619, max. 0.07983
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10507, max. 0.10201
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00484, max. 0.00324
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00010
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02555, max. 0.02630
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
energy graph: torch.Size([1]) torch.Size([20])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02294, max. 0.02293
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.07619, max. 0.08208
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10507, max. 0.10165
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00484, max. 0.00324
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00010
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02567, max. 0.02626
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
energy graph: torch.Size([1]) torch.Size([20])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02294, max. 0.02293
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.07619, max. 0.08310
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10507, max. 0.10759
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00476, max. 0.00340
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00011
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02560, max. 0.02629
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
energy graph: torch.Size([1]) torch.Size([20])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02294, max. 0.02293
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.07619, max. 0.08242
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10507, max. 0.10787
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00461, max. 0.00347
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00011
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02544, max. 0.02634
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
energy graph: torch.Size([1]) torch.Size([20])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02294, max. 0.02293
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.07619, max. 0.08013
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10547, max. 0.10348
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00457, max. 0.00343
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00011
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02539, max. 0.02634
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
energy graph: torch.Size([1]) torch.Size([20])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02294, max. 0.02293
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.07619, max. 0.07750
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10604, max. 0.10715
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00473, max. 0.00326
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00011
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02558, max. 0.02628
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
finished plotting
