/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))
from run_train args: Namespace(config=None, name='dimer-cc-lr-remove-adjusted-mace', seed=123, work_dir='.', log_dir='./logs', model_dir=None, checkpoints_dir=None, results_dir=None, downloads_dir=None, device='cuda', default_dtype='float64', distributed=False, log_level='INFO', error_table='PerAtomMAE', model='MACE', r_max=6.0, radial_type='bessel', num_radial_basis=8, num_cutoff_basis=5, pair_repulsion=False, distance_transform='None', interaction='RealAgnosticResidualInteractionBlock', interaction_first='RealAgnosticResidualInteractionBlock', max_ell=3, correlation=3, num_interactions=2, MLP_irreps='16x0e', radial_MLP='[64, 64, 64]', hidden_irreps=None, num_channels=256, max_L=2, gate='silu', scaling='rms_forces_scaling', avg_num_neighbors=1, compute_avg_num_neighbors=True, compute_stress=False, compute_forces=True, train_file='./custom_dataset/dimer_cc/dimers_cc_train.xyz', valid_file='./custom_dataset/dimer_cc/dimers_cc_test.xyz', valid_fraction=0.1, test_file='./custom_dataset/dimer_cc/dimers_cc_test.xyz', test_dir=None, multi_processed_test=False, num_workers=0, pin_memory=True, atomic_numbers=None, mean=None, std=None, statistics_file=None, E0s='average', foundation_filter_elements=True, heads=None, multiheads_finetuning=True, weight_pt_head=1.0, num_samples_pt=1000, subselect_pt='random', pt_train_file=None, pt_valid_file=None, keep_isolated_atoms=False, energy_key='energy', forces_key='forces', virials_key='REF_virials', stress_key='REF_stress', dipole_key='REF_dipole', charges_key='REF_charges', loss='weighted', forces_weight=100.0, swa_forces_weight=100.0, energy_weight=10.0, swa_energy_weight=1000.0, virials_weight=1.0, swa_virials_weight=10.0, stress_weight=1.0, swa_stress_weight=10.0, dipole_weight=1.0, swa_dipole_weight=1.0, config_type_weights='{"Default":1.0}', huber_delta=0.01, optimizer='adam', beta=0.9, batch_size=2, valid_batch_size=2, lr=0.001, swa_lr=0.001, weight_decay=5e-07, amsgrad=True, scheduler='ReduceLROnPlateau', lr_factor=0.8, scheduler_patience=5, lr_scheduler_gamma=0.9993, swa=True, start_swa=600, ema=True, ema_decay=0.99, max_num_epochs=800, patience=15, foundation_model=None, foundation_model_readout=True, eval_interval=3, keep_checkpoints=False, save_all_checkpoints=False, restart_latest=False, save_cpu=False, clip_grad=10.0, wandb=False, wandb_dir=None, wandb_project='', wandb_entity='', wandb_name='', wandb_log_hypers=['num_channels', 'max_L', 'correlation', 'lr', 'swa_lr', 'weight_decay', 'batch_size', 'max_num_epochs', 'start_swa', 'energy_weight', 'forces_weight'])
args model: MACE
2024-10-31 23:40:05.907 INFO: ===========VERIFYING SETTINGS===========
2024-10-31 23:40:05.908 INFO: MACE version: 0.3.7
2024-10-31 23:40:06.102 INFO: CUDA version: 11.8, CUDA device: 0
2024-10-31 23:40:06.223 INFO: ===========LOADING INPUT DATA===========
2024-10-31 23:40:06.223 INFO: Using heads: ['default']
2024-10-31 23:40:06.223 INFO: =============    Processing head default     ===========
2024-10-31 23:40:06.243 WARNING: Since ASE version 3.23.0b1, using energy_key 'energy' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting 'energy' to 'REF_energy'. You need to use --energy_key='REF_energy' to specify the chosen key name.
2024-10-31 23:40:06.244 WARNING: Since ASE version 3.23.0b1, using forces_key 'forces' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting 'forces' to 'REF_forces'. You need to use --forces_key='REF_forces' to specify the chosen key name.
2024-10-31 23:40:06.246 INFO: Training set [10 configs, 10 energy, 600 forces] loaded from './custom_dataset/dimer_cc/dimers_cc_train.xyz'
2024-10-31 23:40:06.250 WARNING: Since ASE version 3.23.0b1, using energy_key 'energy' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting 'energy' to 'REF_energy'. You need to use --energy_key='REF_energy' to specify the chosen key name.
2024-10-31 23:40:06.251 WARNING: Since ASE version 3.23.0b1, using forces_key 'forces' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting 'forces' to 'REF_forces'. You need to use --forces_key='REF_forces' to specify the chosen key name.
2024-10-31 23:40:06.252 INFO: Validation set [3 configs, 3 energy, 180 forces] loaded from './custom_dataset/dimer_cc/dimers_cc_test.xyz'
2024-10-31 23:40:06.255 WARNING: Since ASE version 3.23.0b1, using energy_key 'energy' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting 'energy' to 'REF_energy'. You need to use --energy_key='REF_energy' to specify the chosen key name.
2024-10-31 23:40:06.256 WARNING: Since ASE version 3.23.0b1, using forces_key 'forces' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting 'forces' to 'REF_forces'. You need to use --forces_key='REF_forces' to specify the chosen key name.
2024-10-31 23:40:06.256 INFO: Test set (3 configs) loaded from './custom_dataset/dimer_cc/dimers_cc_test.xyz':
2024-10-31 23:40:06.257 INFO: Default_Default: 3 configs, 3 energy, 180 forces
2024-10-31 23:40:06.257 INFO: Total number of configurations: train=10, valid=3, tests=[Default_Default: 3],
2024-10-31 23:40:06.257 INFO: Atomic Numbers used: [1, 6, 7, 8]
2024-10-31 23:40:06.257 INFO: Isolated Atomic Energies (E0s) not in training file, using command line argument
2024-10-31 23:40:06.257 INFO: Computing average Atomic Energies using least squares regression
2024-10-31 23:40:06.257 INFO: Atomic Energies used (z: eV) for head default: {1: -945.5562149299105, 6: -343.8386236108764, 7: -257.8789677081574, 8: -171.91931180543827}
2024-10-31 23:40:06.278 INFO: Computing average number of neighbors
2024-10-31 23:40:06.473 INFO: Average number of neighbors: 11.2
2024-10-31 23:40:06.473 INFO: During training the following quantities will be reported: energy, forces
2024-10-31 23:40:06.473 INFO: ===========MODEL DETAILS===========
2024-10-31 23:40:06.481 INFO: Building model
2024-10-31 23:40:06.481 INFO: Message passing with 256 channels and max_L=2 (256x0e+256x1o+256x2e)
2024-10-31 23:40:06.481 INFO: 2 layers, each with correlation order: 3 (body order: 4) and spherical harmonics up to: l=3
2024-10-31 23:40:06.481 INFO: 8 radial and 5 basis functions
2024-10-31 23:40:06.481 INFO: Radial cutoff: 6.0 A (total receptive field for each atom: 12.0 A)
2024-10-31 23:40:06.481 INFO: Distance transform for radial basis functions: None
2024-10-31 23:40:06.481 INFO: Hidden irreps: 256x0e+256x1o+256x2e
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/trace/group/mcgaughey/hariharr/miniconda3/envs/mace/lib/python3.9/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
in the linear readout block 256x0e 16x0e 1
2024-10-31 23:40:09.361 INFO: Total number of parameters: 3929376
2024-10-31 23:40:09.361 INFO: 
2024-10-31 23:40:09.361 INFO: ===========OPTIMIZER INFORMATION===========
2024-10-31 23:40:09.361 INFO: Using ADAM as parameter optimizer
2024-10-31 23:40:09.361 INFO: Batch size: 2
2024-10-31 23:40:09.361 INFO: Using Exponential Moving Average with decay: 0.99
2024-10-31 23:40:09.361 INFO: Number of gradient updates: 4000
2024-10-31 23:40:09.361 INFO: Learning rate: 0.001, weight decay: 5e-07
2024-10-31 23:40:09.361 INFO: WeightedEnergyForcesLoss(energy_weight=10.000, forces_weight=100.000)
2024-10-31 23:40:09.362 INFO: Stage Two (after 600 epochs) with loss function: WeightedEnergyForcesLoss(energy_weight=1000.000, forces_weight=100.000), with energy weight : 1000.0, forces weight : 100.0 and learning rate : 0.001
2024-10-31 23:40:09.488 INFO: Using gradient clipping with tolerance=10.000
2024-10-31 23:40:09.488 INFO: 
2024-10-31 23:40:09.488 INFO: ===========TRAINING===========
2024-10-31 23:40:09.488 INFO: Started training, reporting errors on validation set
2024-10-31 23:40:09.488 INFO: Loss metrics on validation set
2024-10-31 23:40:09.794 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:09.873 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:09.880 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:40:09.885 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:40:09.890 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:40:09.895 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:40:09.900 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02344, max. 0.02344
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.09969, max. 0.10118
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10374, max. 0.08714
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00634, max. 0.00495
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00009
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02644, max. 0.02387
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02344, max. 0.02344
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.09790, max. 0.09923
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10374, max. 0.08714
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00634, max. 0.00493
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00009
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02635, max. 0.02382
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:40:10.294 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:40:10.299 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:40:10.300 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:40:10.300 INFO: total_energy: torch.Size([2])
2024-10-31 23:40:10.300 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:40:10.300 INFO: node_energy: torch.Size([40])
2024-10-31 23:40:10.937 INFO: node_feats: torch.Size([20, 2304])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:40:11.024 INFO: node_feats: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:40:11.031 INFO: node_feats_out:torch.Size([20, 256]), torch.Size([2560]), 20
2024-10-31 23:40:11.037 INFO: linear_node_feats: torch.Size([20, 256])
2024-10-31 23:40:11.043 INFO: q_node_feats: torch.Size([20, 256])
2024-10-31 23:40:11.049 INFO: v_node_feats: torch.Size([20, 256])
2024-10-31 23:40:11.055 INFO: k_node_feats: torch.Size([20, 256])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02344, max. 0.02344
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.09420, max. 0.09911
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10374, max. 0.08714
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00634, max. 0.00487
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00009
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02631, max. 0.02371
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
2024-10-31 23:40:11.195 INFO: long_range_embedding: torch.Size([20, 256])
2024-10-31 23:40:11.202 INFO: long_range_energy: torch.Size([20, 1]), torch.Size([1])
2024-10-31 23:40:11.202 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-10-31 23:40:11.202 INFO: total_energy: torch.Size([1])
2024-10-31 23:40:11.202 INFO: node_energy_contributions: torch.Size([20, 4])
2024-10-31 23:40:11.202 INFO: node_energy: torch.Size([20])
2024-10-31 23:40:11.435 INFO: Initial: head: default, loss=  0.1137, MAE_E_per_atom=    43.1 meV, MAE_F=    30.8 meV / A
2024-10-31 23:40:12.624 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:12.688 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:12.692 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:40:12.696 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:40:12.699 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:40:12.703 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:40:12.706 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02344, max. 0.02344
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.09311, max. 0.09911
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10374, max. 0.08714
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00634, max. 0.00477
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00009
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02639, max. 0.02400
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02344, max. 0.02344
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.09705, max. 0.09911
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10374, max. 0.08714
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00634, max. 0.00485
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00009
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02650, max. 0.02352
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:40:12.987 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:40:12.992 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:40:12.992 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:40:12.992 INFO: total_energy: torch.Size([2])
2024-10-31 23:40:12.992 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:40:12.992 INFO: node_energy: torch.Size([40])
2024-10-31 23:40:16.756 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:16.819 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:16.823 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:40:16.826 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:40:16.829 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:40:16.832 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:40:16.835 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02306, max. 0.02306
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.08974, max. 0.09831
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10251, max. 0.08801
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00627, max. 0.00464
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00009
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02729, max. 0.02454
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02306, max. 0.02306
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.08792, max. 0.09876
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10258, max. 0.08831
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00604, max. 0.00457
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00010, max. 0.00009
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02697, max. 0.02442
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:40:17.259 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:40:17.263 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:40:17.263 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:40:17.263 INFO: total_energy: torch.Size([2])
2024-10-31 23:40:17.264 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:40:17.264 INFO: node_energy: torch.Size([40])
2024-10-31 23:40:21.259 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:21.321 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:21.325 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:40:21.328 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:40:21.332 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:40:21.335 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:40:21.338 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02272, max. 0.02272
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.08654, max. 0.09736
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10332, max. 0.08754
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00595, max. 0.00454
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00009
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02647, max. 0.02460
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02273, max. 0.02273
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.09343, max. 0.09812
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10397, max. 0.08909
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00614, max. 0.00457
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00009
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02747, max. 0.02478
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:40:21.627 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:40:21.631 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:40:21.631 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:40:21.632 INFO: total_energy: torch.Size([2])
2024-10-31 23:40:21.632 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:40:21.632 INFO: node_energy: torch.Size([40])
2024-10-31 23:40:27.310 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:27.371 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:27.375 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:40:27.378 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:40:27.381 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:40:27.384 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:40:27.387 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02275, max. 0.02275
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.08472, max. 0.09679
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10532, max. 0.08766
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00564, max. 0.00454
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00009
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02694, max. 0.02488
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02246, max. 0.02246
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.09389, max. 0.09751
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10632, max. 0.09002
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00620, max. 0.00473
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00009
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02853, max. 0.02573
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:40:27.667 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:40:27.671 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:40:27.672 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:40:27.672 INFO: total_energy: torch.Size([2])
2024-10-31 23:40:27.672 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:40:27.672 INFO: node_energy: torch.Size([40])
2024-10-31 23:40:32.084 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:32.145 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:32.148 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:40:32.152 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:40:32.155 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:40:32.158 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:40:32.161 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02225, max. 0.02225
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.10080, max. 0.10163
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10920, max. 0.09099
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00634, max. 0.00489
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00009
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02803, max. 0.02529
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02225, max. 0.02225
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.09395, max. 0.09679
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.10935, max. 0.09104
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00636, max. 0.00468
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00009
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02860, max. 0.02579
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:40:32.443 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:40:32.446 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:40:32.446 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:40:32.446 INFO: total_energy: torch.Size([2])
2024-10-31 23:40:32.447 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:40:32.447 INFO: node_energy: torch.Size([40])
2024-10-31 23:40:36.948 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:37.005 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:37.008 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:40:37.011 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:40:37.013 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:40:37.016 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:40:37.018 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02221, max. 0.02221
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.10167, max. 0.10188
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.11001, max. 0.09126
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00639, max. 0.00480
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00009
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02830, max. 0.02553
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02221, max. 0.02221
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.09976, max. 0.09891
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.11001, max. 0.09126
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00639, max. 0.00468
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00009
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02826, max. 0.02548
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:40:37.289 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:40:37.292 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:40:37.292 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:40:37.292 INFO: total_energy: torch.Size([2])
2024-10-31 23:40:37.292 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:40:37.293 INFO: node_energy: torch.Size([40])
2024-10-31 23:40:37.680 INFO: node_feats: torch.Size([20, 2304])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:40:37.732 INFO: node_feats: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:40:37.735 INFO: node_feats_out:torch.Size([20, 256]), torch.Size([2560]), 20
2024-10-31 23:40:37.738 INFO: linear_node_feats: torch.Size([20, 256])
2024-10-31 23:40:37.740 INFO: q_node_feats: torch.Size([20, 256])
2024-10-31 23:40:37.742 INFO: v_node_feats: torch.Size([20, 256])
2024-10-31 23:40:37.745 INFO: k_node_feats: torch.Size([20, 256])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02221, max. 0.02221
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.09605, max. 0.09686
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.11001, max. 0.09126
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00639, max. 0.00445
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00009
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02837, max. 0.02535
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
2024-10-31 23:40:37.883 INFO: long_range_embedding: torch.Size([20, 256])
2024-10-31 23:40:37.885 INFO: long_range_energy: torch.Size([20, 1]), torch.Size([1])
2024-10-31 23:40:37.885 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-10-31 23:40:37.886 INFO: total_energy: torch.Size([1])
2024-10-31 23:40:37.886 INFO: node_energy_contributions: torch.Size([20, 4])
2024-10-31 23:40:37.886 INFO: node_energy: torch.Size([20])
2024-10-31 23:40:38.093 INFO: Epoch 0: head: default, loss=  0.0665, MAE_E_per_atom=    44.8 meV, MAE_F=    20.7 meV / A
2024-10-31 23:40:38.601 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:38.662 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:38.666 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:40:38.669 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:40:38.672 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:40:38.675 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:40:38.679 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02210, max. 0.02210
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.09114, max. 0.09670
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.11158, max. 0.09193
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00633, max. 0.00449
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00009
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02933, max. 0.02541
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02211, max. 0.02211
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.09980, max. 0.10172
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.11147, max. 0.09177
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00645, max. 0.00468
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00011, max. 0.00009
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.02922, max. 0.02523
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:40:38.958 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:40:38.961 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:40:38.961 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:40:38.961 INFO: total_energy: torch.Size([2])
2024-10-31 23:40:38.962 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:40:38.962 INFO: node_energy: torch.Size([40])
2024-10-31 23:40:40.524 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:40.586 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:40.590 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:40:40.593 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:40:40.596 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:40:40.599 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:40:40.602 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02202, max. 0.02201
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.09614, max. 0.09724
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.11382, max. 0.09249
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00660, max. 0.00472
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00012, max. 0.00009
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03003, max. 0.02607
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02201, max. 0.02201
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.09786, max. 0.10030
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.11368, max. 0.09244
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00658, max. 0.00446
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00012, max. 0.00009
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03017, max. 0.02567
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:40:40.889 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:40:40.892 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:40:40.892 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:40:40.893 INFO: total_energy: torch.Size([2])
2024-10-31 23:40:40.893 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:40:40.893 INFO: node_energy: torch.Size([40])
2024-10-31 23:40:42.335 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:42.397 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:42.400 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:40:42.403 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:40:42.407 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:40:42.410 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:40:42.413 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02225, max. 0.02225
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.08806, max. 0.09561
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.11416, max. 0.09055
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00609, max. 0.00460
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00012, max. 0.00009
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03112, max. 0.02660
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02196, max. 0.02196
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.09083, max. 0.09556
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.11523, max. 0.09150
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00648, max. 0.00455
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00012, max. 0.00009
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03110, max. 0.02640
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:40:42.694 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:40:42.698 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:40:42.698 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:40:42.698 INFO: total_energy: torch.Size([2])
2024-10-31 23:40:42.698 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:40:42.699 INFO: node_energy: torch.Size([40])
2024-10-31 23:40:44.159 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:44.220 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:44.223 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:40:44.226 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:40:44.229 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:40:44.233 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:40:44.236 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02198, max. 0.02198
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.09644, max. 0.09797
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.11811, max. 0.09493
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00699, max. 0.00464
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00012, max. 0.00010
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03233, max. 0.02780
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02198, max. 0.02198
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.09985, max. 0.10093
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.11820, max. 0.09766
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00686, max. 0.00485
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00012, max. 0.00010
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03201, max. 0.02757
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:40:44.517 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:40:44.521 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:40:44.521 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:40:44.521 INFO: total_energy: torch.Size([2])
2024-10-31 23:40:44.521 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:40:44.522 INFO: node_energy: torch.Size([40])
2024-10-31 23:40:45.970 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:46.032 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:46.035 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:40:46.038 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:40:46.042 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:40:46.045 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:40:46.048 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02202, max. 0.02202
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.10054, max. 0.10139
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.12062, max. 0.10336
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00704, max. 0.00467
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00012, max. 0.00011
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03324, max. 0.02886
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02202, max. 0.02202
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.10727, max. 0.11049
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.12062, max. 0.10671
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00704, max. 0.00508
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00012, max. 0.00011
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03358, max. 0.02911
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:40:46.330 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:40:46.333 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:40:46.333 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:40:46.334 INFO: total_energy: torch.Size([2])
2024-10-31 23:40:46.334 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:40:46.334 INFO: node_energy: torch.Size([40])
2024-10-31 23:40:47.894 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:47.955 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:47.958 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:40:47.961 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:40:47.965 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:40:47.968 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:40:47.971 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02206, max. 0.02206
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.09677, max. 0.09950
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.12285, max. 0.10221
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00703, max. 0.00472
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00013, max. 0.00010
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03456, max. 0.03021
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02206, max. 0.02206
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.10844, max. 0.11193
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.12268, max. 0.10991
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00718, max. 0.00514
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00013, max. 0.00011
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03440, max. 0.02993
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:40:48.254 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:40:48.257 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:40:48.257 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:40:48.258 INFO: total_energy: torch.Size([2])
2024-10-31 23:40:48.258 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:40:48.258 INFO: node_energy: torch.Size([40])
2024-10-31 23:40:49.691 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:49.752 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:49.755 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:40:49.758 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:40:49.762 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:40:49.765 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:40:49.768 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02213, max. 0.02213
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.10733, max. 0.11150
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.12477, max. 0.11189
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00732, max. 0.00495
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00013, max. 0.00011
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03519, max. 0.03075
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02213, max. 0.02213
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.10408, max. 0.10821
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.12477, max. 0.11072
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00732, max. 0.00469
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00013, max. 0.00011
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03503, max. 0.03064
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:40:50.051 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:40:50.055 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:40:50.055 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:40:50.055 INFO: total_energy: torch.Size([2])
2024-10-31 23:40:50.055 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:40:50.055 INFO: node_energy: torch.Size([40])
2024-10-31 23:40:51.500 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:51.562 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:51.566 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:40:51.569 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:40:51.572 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:40:51.575 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:40:51.579 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02245, max. 0.02245
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.09268, max. 0.09605
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.12485, max. 0.10918
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00680, max. 0.00488
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00013, max. 0.00011
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03559, max. 0.03119
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02217, max. 0.02217
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.10047, max. 0.10305
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.12646, max. 0.10862
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00755, max. 0.00487
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00013, max. 0.00011
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03551, max. 0.03102
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:40:51.861 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:40:51.865 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:40:51.865 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:40:51.865 INFO: total_energy: torch.Size([2])
2024-10-31 23:40:51.865 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:40:51.865 INFO: node_energy: torch.Size([40])
2024-10-31 23:40:53.334 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:53.395 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:53.398 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:40:53.402 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:40:53.405 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:40:53.408 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:40:53.411 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02224, max. 0.02224
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.09659, max. 0.10018
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.12920, max. 0.11462
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00741, max. 0.00489
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00013, max. 0.00012
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03649, max. 0.03198
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02226, max. 0.02226
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.10549, max. 0.10759
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.12908, max. 0.11411
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00759, max. 0.00517
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00013, max. 0.00012
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03601, max. 0.03167
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:40:53.693 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:40:53.696 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:40:53.697 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:40:53.697 INFO: total_energy: torch.Size([2])
2024-10-31 23:40:53.697 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:40:53.697 INFO: node_energy: torch.Size([40])
2024-10-31 23:40:55.151 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:55.211 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:55.215 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:40:55.218 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:40:55.221 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:40:55.224 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:40:55.227 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02236, max. 0.02236
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.10616, max. 0.10791
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.13185, max. 0.11962
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00779, max. 0.00500
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00013, max. 0.00012
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03713, max. 0.03292
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02236, max. 0.02236
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.10553, max. 0.10844
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.13199, max. 0.11954
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00781, max. 0.00518
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00013, max. 0.00012
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03715, max. 0.03287
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:40:55.511 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:40:55.514 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:40:55.515 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:40:55.515 INFO: total_energy: torch.Size([2])
2024-10-31 23:40:55.515 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:40:55.515 INFO: node_energy: torch.Size([40])
2024-10-31 23:40:57.093 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:57.155 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:57.159 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:40:57.162 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:40:57.165 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:40:57.168 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:40:57.171 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02243, max. 0.02243
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.10732, max. 0.10922
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.13441, max. 0.12457
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00796, max. 0.00508
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00014, max. 0.00013
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03795, max. 0.03386
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02241, max. 0.02241
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.09875, max. 0.10270
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.13472, max. 0.12521
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00777, max. 0.00503
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00014, max. 0.00013
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03835, max. 0.03408
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:40:57.455 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:40:57.458 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:40:57.458 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:40:57.458 INFO: total_energy: torch.Size([2])
2024-10-31 23:40:57.459 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:40:57.459 INFO: node_energy: torch.Size([40])
2024-10-31 23:40:58.901 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:58.961 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:40:58.965 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:40:58.968 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:40:58.971 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:40:58.974 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:40:58.977 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02251, max. 0.02251
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.10503, max. 0.10852
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.13711, max. 0.12940
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00827, max. 0.00514
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00014, max. 0.00013
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03916, max. 0.03514
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02251, max. 0.02251
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.11329, max. 0.11838
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.13728, max. 0.12994
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00814, max. 0.00531
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00014, max. 0.00013
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03935, max. 0.03533
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:40:59.264 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:40:59.268 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:40:59.268 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:40:59.268 INFO: total_energy: torch.Size([2])
2024-10-31 23:40:59.268 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:40:59.269 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:00.728 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:00.790 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:00.793 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:00.797 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:00.800 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:00.803 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:00.806 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02255, max. 0.02254
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.10371, max. 0.10741
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.14017, max. 0.13472
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00812, max. 0.00523
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00014, max. 0.00014
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04030, max. 0.03653
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02255, max. 0.02255
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.10891, max. 0.11214
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.14005, max. 0.13463
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00832, max. 0.00540
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00014, max. 0.00014
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03970, max. 0.03583
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:41:01.089 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:41:01.093 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:41:01.093 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:41:01.093 INFO: total_energy: torch.Size([2])
2024-10-31 23:41:01.093 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:41:01.093 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:02.550 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:02.613 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:02.616 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:02.620 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:02.623 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:02.626 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:02.629 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02260, max. 0.02260
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.11193, max. 0.11724
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.14250, max. 0.13932
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00845, max. 0.00512
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00015, max. 0.00014
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04075, max. 0.03707
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02287, max. 0.02287
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.09863, max. 0.10197
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.14055, max. 0.13874
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00778, max. 0.00533
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00014, max. 0.00014
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04078, max. 0.03712
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:41:02.909 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:41:02.913 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:41:02.913 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:41:02.913 INFO: total_energy: torch.Size([2])
2024-10-31 23:41:02.913 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:41:02.913 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:04.369 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:04.430 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:04.433 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:04.437 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:04.440 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:04.443 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:04.446 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02266, max. 0.02266
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.11259, max. 0.11524
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.14555, max. 0.14459
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00864, max. 0.00570
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00015, max. 0.00015
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04137, max. 0.03795
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02266, max. 0.02266
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.11917, max. 0.12401
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.14554, max. 0.14460
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00864, max. 0.00588
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00015, max. 0.00015
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04193, max. 0.03841
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:41:04.852 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:41:04.855 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:41:04.856 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:41:04.856 INFO: total_energy: torch.Size([2])
2024-10-31 23:41:04.856 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:41:04.856 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:06.305 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:06.362 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:06.365 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:06.368 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:06.370 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:06.372 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:06.375 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02258, max. 0.02258
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.11823, max. 0.12210
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.14229, max. 0.13861
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00844, max. 0.00569
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00015, max. 0.00014
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04062, max. 0.03695
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02258, max. 0.02258
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.11617, max. 0.11903
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.14229, max. 0.13861
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00844, max. 0.00552
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00015, max. 0.00014
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04052, max. 0.03689
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:41:06.649 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:41:06.651 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:41:06.652 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:41:06.652 INFO: total_energy: torch.Size([2])
2024-10-31 23:41:06.652 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:41:06.652 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:08.563 INFO: node_feats: torch.Size([20, 2304])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:41:08.616 INFO: node_feats: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:41:08.618 INFO: node_feats_out:torch.Size([20, 256]), torch.Size([2560]), 20
2024-10-31 23:41:08.621 INFO: linear_node_feats: torch.Size([20, 256])
2024-10-31 23:41:08.623 INFO: q_node_feats: torch.Size([20, 256])
2024-10-31 23:41:08.625 INFO: v_node_feats: torch.Size([20, 256])
2024-10-31 23:41:08.628 INFO: k_node_feats: torch.Size([20, 256])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02258, max. 0.02258
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.11191, max. 0.11430
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.14229, max. 0.13900
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00844, max. 0.00493
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00015, max. 0.00014
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04074, max. 0.03704
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
2024-10-31 23:41:08.764 INFO: long_range_embedding: torch.Size([20, 256])
2024-10-31 23:41:08.766 INFO: long_range_energy: torch.Size([20, 1]), torch.Size([1])
2024-10-31 23:41:08.766 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-10-31 23:41:08.767 INFO: total_energy: torch.Size([1])
2024-10-31 23:41:08.767 INFO: node_energy_contributions: torch.Size([20, 4])
2024-10-31 23:41:08.767 INFO: node_energy: torch.Size([20])
2024-10-31 23:41:08.975 INFO: Epoch 3: head: default, loss=  0.1525, MAE_E_per_atom=    41.9 meV, MAE_F=    31.9 meV / A
2024-10-31 23:41:09.046 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:09.106 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:09.109 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:09.113 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:09.116 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:09.119 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:09.122 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02266, max. 0.02266
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.11401, max. 0.11944
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.14802, max. 0.14873
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00877, max. 0.00527
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00015, max. 0.00015
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04266, max. 0.03912
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02266, max. 0.02266
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.10880, max. 0.11260
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.14781, max. 0.14817
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00890, max. 0.00546
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00015, max. 0.00015
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04244, max. 0.03906
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:41:09.399 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:41:09.403 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:41:09.403 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:41:09.403 INFO: total_energy: torch.Size([2])
2024-10-31 23:41:09.403 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:41:09.403 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:10.839 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:10.900 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:10.903 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:10.906 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:10.910 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:10.913 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:10.916 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02264, max. 0.02264
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.11403, max. 0.11631
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.15012, max. 0.15207
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00886, max. 0.00590
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00015, max. 0.00016
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04309, max. 0.03931
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02262, max. 0.02262
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.10382, max. 0.10827
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.15097, max. 0.15324
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00870, max. 0.00554
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00015, max. 0.00016
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04350, max. 0.03967
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:41:11.202 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:41:11.205 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:41:11.205 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:41:11.206 INFO: total_energy: torch.Size([2])
2024-10-31 23:41:11.206 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:41:11.206 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:12.654 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:12.715 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:12.719 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:12.722 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:12.725 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:12.728 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:12.731 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02264, max. 0.02264
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.11328, max. 0.11605
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.15249, max. 0.15571
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00900, max. 0.00585
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00016, max. 0.00016
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04410, max. 0.04010
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02264, max. 0.02264
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.11408, max. 0.11613
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.15235, max. 0.15579
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00898, max. 0.00575
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00016, max. 0.00016
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04423, max. 0.04014
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:41:13.013 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:41:13.016 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:41:13.016 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:41:13.016 INFO: total_energy: torch.Size([2])
2024-10-31 23:41:13.017 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:41:13.017 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:14.459 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:14.520 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:14.523 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:14.527 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:14.530 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:14.533 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:14.536 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02262, max. 0.02262
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.11939, max. 0.12405
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.15401, max. 0.15853
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00906, max. 0.00594
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00016, max. 0.00016
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04512, max. 0.04090
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02290, max. 0.02290
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.10275, max. 0.10700
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.15201, max. 0.15740
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00832, max. 0.00575
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00016, max. 0.00016
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04511, max. 0.04124
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:41:14.816 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:41:14.820 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:41:14.820 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:41:14.820 INFO: total_energy: torch.Size([2])
2024-10-31 23:41:14.820 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:41:14.821 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:16.381 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:16.442 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:16.446 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:16.449 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:16.452 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:16.455 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:16.458 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02265, max. 0.02265
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12276, max. 0.12637
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.15616, max. 0.16224
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00918, max. 0.00640
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00016, max. 0.00017
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04605, max. 0.04173
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02264, max. 0.02264
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.10905, max. 0.11183
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.15648, max. 0.16225
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00899, max. 0.00596
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00016, max. 0.00017
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04625, max. 0.04198
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:41:16.740 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:41:16.743 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:41:16.744 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:41:16.744 INFO: total_energy: torch.Size([2])
2024-10-31 23:41:16.744 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:41:16.744 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:18.211 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:18.272 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:18.276 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:18.279 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:18.282 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:18.285 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:18.288 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02268, max. 0.02268
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.11511, max. 0.11666
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.15819, max. 0.16553
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00931, max. 0.00618
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00016, max. 0.00017
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04663, max. 0.04238
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02268, max. 0.02268
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12082, max. 0.12474
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.15805, max. 0.16561
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00929, max. 0.00616
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00016, max. 0.00017
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04697, max. 0.04248
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:41:18.567 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:41:18.571 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:41:18.571 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:41:18.571 INFO: total_energy: torch.Size([2])
2024-10-31 23:41:18.571 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:41:18.572 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:20.008 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:20.069 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:20.073 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:20.076 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:20.079 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:20.082 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:20.085 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02269, max. 0.02269
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.11702, max. 0.11740
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.15935, max. 0.16798
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00935, max. 0.00649
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00016, max. 0.00017
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04702, max. 0.04261
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02269, max. 0.02269
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.11228, max. 0.11395
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.15911, max. 0.16745
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00948, max. 0.00599
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00016, max. 0.00017
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04717, max. 0.04275
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:41:20.367 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:41:20.371 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:41:20.371 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:41:20.371 INFO: total_energy: torch.Size([2])
2024-10-31 23:41:20.371 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:41:20.371 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:21.814 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:21.875 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:21.879 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:21.882 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:21.885 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:21.888 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:21.891 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02270, max. 0.02270
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.11130, max. 0.11306
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.16038, max. 0.16972
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00922, max. 0.00632
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00016, max. 0.00017
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04777, max. 0.04310
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02271, max. 0.02271
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12414, max. 0.12586
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.16006, max. 0.16971
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00942, max. 0.00674
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00016, max. 0.00017
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04758, max. 0.04289
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:41:22.171 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:41:22.175 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:41:22.175 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:41:22.175 INFO: total_energy: torch.Size([2])
2024-10-31 23:41:22.175 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:41:22.175 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:23.624 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:23.685 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:23.688 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:23.692 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:23.695 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:23.698 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:23.701 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02302, max. 0.02302
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.10640, max. 0.11092
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.15874, max. 0.16961
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00864, max. 0.00623
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00016, max. 0.00017
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04764, max. 0.04311
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02275, max. 0.02275
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.11794, max. 0.12031
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.16072, max. 0.17141
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00947, max. 0.00589
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00016, max. 0.00017
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04777, max. 0.04282
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:41:24.084 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:41:24.088 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:41:24.088 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:41:24.088 INFO: total_energy: torch.Size([2])
2024-10-31 23:41:24.089 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:41:24.089 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:25.544 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:25.604 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:25.608 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:25.611 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:25.614 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:25.617 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:25.620 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02283, max. 0.02283
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.11009, max. 0.11381
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.16368, max. 0.17632
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00946, max. 0.00644
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00018
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04860, max. 0.04365
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02285, max. 0.02285
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.11752, max. 0.11956
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.16213, max. 0.17465
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00960, max. 0.00659
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00018
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04847, max. 0.04345
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:41:25.904 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:41:25.907 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:41:25.907 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:41:25.908 INFO: total_energy: torch.Size([2])
2024-10-31 23:41:25.908 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:41:25.908 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:27.373 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:27.435 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:27.438 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:27.441 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:27.444 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:27.447 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:27.451 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02327, max. 0.02327
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.10852, max. 0.11343
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.16181, max. 0.17639
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00888, max. 0.00645
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00018
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04950, max. 0.04461
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02298, max. 0.02298
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.11423, max. 0.11587
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.16413, max. 0.17858
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00958, max. 0.00670
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00018
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04993, max. 0.04477
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:41:27.731 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:41:27.734 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:41:27.734 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:41:27.735 INFO: total_energy: torch.Size([2])
2024-10-31 23:41:27.735 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:41:27.735 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:29.179 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:29.241 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:29.244 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:29.247 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:29.250 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:29.253 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:29.256 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02322, max. 0.02322
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12054, max. 0.12143
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.16630, max. 0.18414
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01002, max. 0.00620
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00019
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05169, max. 0.04621
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02322, max. 0.02322
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12453, max. 0.12450
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.16630, max. 0.18414
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01002, max. 0.00680
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00019
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05175, max. 0.04628
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:41:29.539 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:41:29.543 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:41:29.543 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:41:29.543 INFO: total_energy: torch.Size([2])
2024-10-31 23:41:29.543 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:41:29.543 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:30.985 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:31.046 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:31.050 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:31.053 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:31.056 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:31.059 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:31.062 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02337, max. 0.02337
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12117, max. 0.12347
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.16811, max. 0.18826
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01018, max. 0.00728
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00019
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05275, max. 0.04735
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02337, max. 0.02337
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.11866, max. 0.11780
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.16787, max. 0.18774
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01030, max. 0.00658
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00019
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05284, max. 0.04741
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:41:31.342 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:41:31.345 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:41:31.345 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:41:31.345 INFO: total_energy: torch.Size([2])
2024-10-31 23:41:31.345 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:41:31.346 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:32.791 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:32.961 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:32.965 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:32.968 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:32.972 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:32.975 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:32.978 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02350, max. 0.02350
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12181, max. 0.12269
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.16922, max. 0.19111
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01034, max. 0.00710
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00020
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05367, max. 0.04809
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02350, max. 0.02350
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12103, max. 0.12421
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.16909, max. 0.19117
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01033, max. 0.00713
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00020
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05364, max. 0.04788
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:41:33.261 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:41:33.264 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:41:33.264 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:41:33.265 INFO: total_energy: torch.Size([2])
2024-10-31 23:41:33.265 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:41:33.265 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:34.718 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:34.779 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:34.783 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:34.786 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:34.789 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:34.792 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:34.795 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02361, max. 0.02361
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12934, max. 0.12894
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.16920, max. 0.19251
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01040, max. 0.00755
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00020
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05408, max. 0.04806
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02358, max. 0.02358
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.11645, max. 0.12073
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17115, max. 0.19436
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01026, max. 0.00702
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00020
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05393, max. 0.04797
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:41:35.077 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:41:35.080 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:41:35.081 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:41:35.081 INFO: total_energy: torch.Size([2])
2024-10-31 23:41:35.081 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:41:35.081 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:36.535 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:36.593 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:36.596 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:36.598 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:36.601 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:36.603 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:36.605 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02330, max. 0.02330
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12812, max. 0.12801
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.16572, max. 0.18401
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01006, max. 0.00728
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00019
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05159, max. 0.04615
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02330, max. 0.02330
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12576, max. 0.12555
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.16572, max. 0.18401
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01006, max. 0.00703
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00019
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05157, max. 0.04610
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:41:36.878 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:41:36.881 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:41:36.881 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:41:36.881 INFO: total_energy: torch.Size([2])
2024-10-31 23:41:36.881 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:41:36.881 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:37.267 INFO: node_feats: torch.Size([20, 2304])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:41:37.320 INFO: node_feats: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:41:37.323 INFO: node_feats_out:torch.Size([20, 256]), torch.Size([2560]), 20
2024-10-31 23:41:37.325 INFO: linear_node_feats: torch.Size([20, 256])
2024-10-31 23:41:37.327 INFO: q_node_feats: torch.Size([20, 256])
2024-10-31 23:41:37.330 INFO: v_node_feats: torch.Size([20, 256])
2024-10-31 23:41:37.332 INFO: k_node_feats: torch.Size([20, 256])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02330, max. 0.02330
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12105, max. 0.11943
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.16572, max. 0.18401
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01006, max. 0.00632
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00019
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05161, max. 0.04605
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
2024-10-31 23:41:37.467 INFO: long_range_embedding: torch.Size([20, 256])
2024-10-31 23:41:37.470 INFO: long_range_energy: torch.Size([20, 1]), torch.Size([1])
2024-10-31 23:41:37.470 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-10-31 23:41:37.470 INFO: total_energy: torch.Size([1])
2024-10-31 23:41:37.470 INFO: node_energy_contributions: torch.Size([20, 4])
2024-10-31 23:41:37.471 INFO: node_energy: torch.Size([20])
2024-10-31 23:41:37.680 INFO: Epoch 6: head: default, loss=  0.2810, MAE_E_per_atom=    42.6 meV, MAE_F=    43.5 meV / A
2024-10-31 23:41:37.750 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:37.810 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:37.813 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:37.817 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:37.820 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:37.823 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:37.826 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02371, max. 0.02371
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12027, max. 0.12121
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17007, max. 0.19444
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01030, max. 0.00729
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00020
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05459, max. 0.04837
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02372, max. 0.02372
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12774, max. 0.12557
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.16974, max. 0.19443
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01049, max. 0.00710
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00020
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05445, max. 0.04810
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:41:38.102 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:41:38.106 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:41:38.106 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:41:38.106 INFO: total_energy: torch.Size([2])
2024-10-31 23:41:38.106 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:41:38.106 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:39.542 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:39.603 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:39.606 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:39.609 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:39.612 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:39.615 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:39.618 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02381, max. 0.02381
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12545, max. 0.12637
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.16999, max. 0.19551
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01054, max. 0.00656
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00020
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05455, max. 0.04801
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02381, max. 0.02381
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12292, max. 0.12662
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.16999, max. 0.19551
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01054, max. 0.00738
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00020
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05455, max. 0.04814
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:41:39.902 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:41:39.906 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:41:39.907 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:41:39.907 INFO: total_energy: torch.Size([2])
2024-10-31 23:41:39.907 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:41:39.907 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:41.352 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:41.414 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:41.418 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:41.421 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:41.424 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:41.427 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:41.430 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02381, max. 0.02381
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.11845, max. 0.12271
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17160, max. 0.19705
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01040, max. 0.00718
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00020
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05398, max. 0.04743
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02383, max. 0.02383
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12241, max. 0.12200
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.16936, max. 0.19475
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01064, max. 0.00684
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00020
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05417, max. 0.04770
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:41:41.825 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:41:41.828 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:41:41.829 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:41:41.829 INFO: total_energy: torch.Size([2])
2024-10-31 23:41:41.829 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:41:41.829 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:43.285 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:43.346 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:43.350 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:43.353 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:43.356 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:43.359 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:43.362 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02391, max. 0.02391
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12330, max. 0.12788
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.16961, max. 0.19591
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01058, max. 0.00770
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00020
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05428, max. 0.04765
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02391, max. 0.02391
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12457, max. 0.12587
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.16972, max. 0.19588
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01061, max. 0.00735
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00020
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05433, max. 0.04773
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:41:43.643 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:41:43.647 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:41:43.647 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:41:43.647 INFO: total_energy: torch.Size([2])
2024-10-31 23:41:43.647 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:41:43.647 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:45.092 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:45.153 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:45.157 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:45.160 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:45.163 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:45.166 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:45.169 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02397, max. 0.02397
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13068, max. 0.13197
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.16899, max. 0.19552
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01061, max. 0.00776
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00020
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05412, max. 0.04710
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02425, max. 0.02425
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.11586, max. 0.12231
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.16677, max. 0.19139
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00944, max. 0.00698
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00020
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05331, max. 0.04648
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:41:45.450 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:41:45.453 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:41:45.454 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:41:45.454 INFO: total_energy: torch.Size([2])
2024-10-31 23:41:45.454 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:41:45.454 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:46.938 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:46.998 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:47.002 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:47.005 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:47.008 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:47.011 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:47.014 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02406, max. 0.02406
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12323, max. 0.12403
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.16968, max. 0.19684
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01051, max. 0.00752
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00020
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05454, max. 0.04729
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02407, max. 0.02407
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13113, max. 0.13265
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.16937, max. 0.19679
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01069, max. 0.00779
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00020
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05447, max. 0.04720
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:41:47.295 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:41:47.298 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:41:47.299 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:41:47.299 INFO: total_energy: torch.Size([2])
2024-10-31 23:41:47.299 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:41:47.299 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:48.737 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:48.798 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:48.801 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:48.805 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:48.808 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:48.811 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:48.814 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02417, max. 0.02417
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13087, max. 0.12866
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.16977, max. 0.19798
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01076, max. 0.00719
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00020
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05464, max. 0.04706
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02417, max. 0.02417
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12429, max. 0.12974
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.16979, max. 0.19796
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01074, max. 0.00780
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00020
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05478, max. 0.04750
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:41:49.096 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:41:49.099 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:41:49.100 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:41:49.100 INFO: total_energy: torch.Size([2])
2024-10-31 23:41:49.100 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:41:49.100 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:50.651 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:50.712 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:50.716 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:50.719 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:50.722 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:50.725 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:50.729 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02449, max. 0.02449
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.11792, max. 0.12462
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.16730, max. 0.19334
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00953, max. 0.00701
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00020
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05346, max. 0.04598
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02421, max. 0.02421
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12891, max. 0.12962
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.16965, max. 0.19815
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01079, max. 0.00673
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00020
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05435, max. 0.04662
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:41:51.009 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:41:51.013 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:41:51.013 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:41:51.013 INFO: total_energy: torch.Size([2])
2024-10-31 23:41:51.013 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:41:51.013 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:52.466 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:52.527 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:52.530 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:52.533 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:52.536 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:52.540 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:52.543 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02430, max. 0.02430
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12723, max. 0.13075
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17045, max. 0.20009
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01088, max. 0.00761
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00020
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05478, max. 0.04701
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02430, max. 0.02430
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12716, max. 0.12895
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17058, max. 0.20005
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01089, max. 0.00740
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00020
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05491, max. 0.04733
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:41:52.823 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:41:52.827 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:41:52.827 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:41:52.827 INFO: total_energy: torch.Size([2])
2024-10-31 23:41:52.827 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:41:52.827 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:54.268 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:54.329 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:54.333 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:54.336 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:54.339 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:54.342 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:54.345 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02436, max. 0.02436
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12645, max. 0.12783
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17049, max. 0.20063
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01101, max. 0.00709
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00021
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05472, max. 0.04693
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02434, max. 0.02434
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12288, max. 0.12754
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17275, max. 0.20260
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01076, max. 0.00731
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00021
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05427, max. 0.04634
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:41:54.626 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:41:54.629 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:41:54.630 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:41:54.630 INFO: total_energy: torch.Size([2])
2024-10-31 23:41:54.630 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:41:54.630 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:56.102 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:56.163 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:56.166 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:56.170 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:56.173 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:56.176 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:56.179 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02446, max. 0.02446
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12366, max. 0.12840
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17358, max. 0.20486
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01083, max. 0.00731
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00021
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05473, max. 0.04664
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02448, max. 0.02448
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13309, max. 0.13561
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17152, max. 0.20343
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01100, max. 0.00781
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00021
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05516, max. 0.04707
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:41:56.461 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:41:56.464 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:41:56.464 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:41:56.464 INFO: total_energy: torch.Size([2])
2024-10-31 23:41:56.465 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:41:56.465 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:57.908 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:57.970 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:57.973 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:57.976 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:57.979 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:57.982 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:57.986 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02461, max. 0.02461
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13380, max. 0.13167
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17284, max. 0.20653
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01109, max. 0.00711
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00021
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05562, max. 0.04734
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02461, max. 0.02461
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12931, max. 0.13301
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17284, max. 0.20653
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01109, max. 0.00764
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00021
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05579, max. 0.04767
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:41:58.271 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:41:58.274 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:41:58.274 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:41:58.275 INFO: total_energy: torch.Size([2])
2024-10-31 23:41:58.275 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:41:58.275 INFO: node_energy: torch.Size([40])
2024-10-31 23:41:59.862 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:59.923 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:41:59.926 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:41:59.930 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:41:59.933 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:41:59.936 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:41:59.939 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02467, max. 0.02467
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12711, max. 0.13319
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17347, max. 0.20810
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01112, max. 0.00782
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00021
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05576, max. 0.04781
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02467, max. 0.02467
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12822, max. 0.13035
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17325, max. 0.20764
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01121, max. 0.00714
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00021
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05556, max. 0.04755
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:00.221 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:00.224 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:00.224 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:00.224 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:00.225 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:00.225 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:01.672 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:01.734 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:01.737 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:01.741 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:01.744 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:01.747 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:01.750 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02472, max. 0.02472
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13207, max. 0.13290
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17347, max. 0.20872
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01113, max. 0.00677
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00021
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05492, max. 0.04662
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02500, max. 0.02500
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12123, max. 0.12857
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17084, max. 0.20252
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00969, max. 0.00707
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00017, max. 0.00021
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05371, max. 0.04559
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:02.030 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:02.033 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:02.034 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:02.034 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:02.034 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:02.034 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:03.488 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:03.550 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:03.553 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:03.556 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:03.559 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:03.562 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:03.565 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02481, max. 0.02481
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12987, max. 0.13282
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17437, max. 0.21063
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01118, max. 0.00729
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05526, max. 0.04721
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02480, max. 0.02480
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12804, max. 0.12993
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17452, max. 0.21081
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01100, max. 0.00764
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05497, max. 0.04669
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:03.848 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:03.851 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:03.851 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:03.851 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:03.851 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:03.852 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:05.309 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:05.367 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:05.370 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:05.372 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:05.375 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:05.377 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:05.379 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02447, max. 0.02447
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13357, max. 0.13738
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17206, max. 0.20423
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01097, max. 0.00786
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00021
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05493, max. 0.04722
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02447, max. 0.02447
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13084, max. 0.13478
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17206, max. 0.20423
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01097, max. 0.00763
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00021
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05490, max. 0.04715
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:05.653 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:05.655 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:05.656 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:05.656 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:05.656 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:05.656 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:06.048 INFO: node_feats: torch.Size([20, 2304])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:42:06.101 INFO: node_feats: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:42:06.104 INFO: node_feats_out:torch.Size([20, 256]), torch.Size([2560]), 20
2024-10-31 23:42:06.107 INFO: linear_node_feats: torch.Size([20, 256])
2024-10-31 23:42:06.109 INFO: q_node_feats: torch.Size([20, 256])
2024-10-31 23:42:06.111 INFO: v_node_feats: torch.Size([20, 256])
2024-10-31 23:42:06.114 INFO: k_node_feats: torch.Size([20, 256])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02447, max. 0.02447
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12579, max. 0.13000
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17206, max. 0.20423
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01097, max. 0.00704
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00021
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05477, max. 0.04691
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
2024-10-31 23:42:06.252 INFO: long_range_embedding: torch.Size([20, 256])
2024-10-31 23:42:06.255 INFO: long_range_energy: torch.Size([20, 1]), torch.Size([1])
2024-10-31 23:42:06.255 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-10-31 23:42:06.255 INFO: total_energy: torch.Size([1])
2024-10-31 23:42:06.255 INFO: node_energy_contributions: torch.Size([20, 4])
2024-10-31 23:42:06.255 INFO: node_energy: torch.Size([20])
2024-10-31 23:42:06.465 INFO: Epoch 9: head: default, loss=  0.4837, MAE_E_per_atom=    41.0 meV, MAE_F=    55.1 meV / A
2024-10-31 23:42:06.535 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:06.596 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:06.600 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:06.603 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:06.606 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:06.609 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:06.612 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02488, max. 0.02488
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12601, max. 0.13067
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17670, max. 0.21317
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01096, max. 0.00727
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05434, max. 0.04586
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02491, max. 0.02491
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12815, max. 0.13491
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17475, max. 0.21208
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01117, max. 0.00780
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05520, max. 0.04697
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:06.889 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:06.892 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:06.892 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:06.893 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:06.893 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:06.893 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:08.465 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:08.526 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:08.529 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:08.533 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:08.536 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:08.539 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:08.542 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02503, max. 0.02503
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13096, max. 0.13382
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17547, max. 0.21363
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01121, max. 0.00728
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05526, max. 0.04685
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02503, max. 0.02503
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12995, max. 0.13223
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17512, max. 0.21318
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01128, max. 0.00719
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05499, max. 0.04648
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:08.826 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:08.830 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:08.830 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:08.830 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:08.830 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:08.830 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:10.279 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:10.341 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:10.345 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:10.348 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:10.351 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:10.354 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:10.358 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02512, max. 0.02512
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13637, max. 0.13479
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17534, max. 0.21398
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01118, max. 0.00696
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05448, max. 0.04571
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02512, max. 0.02512
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13203, max. 0.13635
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17534, max. 0.21398
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01118, max. 0.00765
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05478, max. 0.04589
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:10.638 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:10.641 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:10.641 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:10.641 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:10.641 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:10.642 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:12.085 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:12.145 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:12.149 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:12.152 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:12.155 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:12.158 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:12.161 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02514, max. 0.02514
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13624, max. 0.14040
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17459, max. 0.21239
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01111, max. 0.00771
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05356, max. 0.04499
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02541, max. 0.02541
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12337, max. 0.12996
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17173, max. 0.20510
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00959, max. 0.00719
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00021
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05196, max. 0.04318
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:12.439 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:12.443 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:12.443 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:12.443 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:12.443 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:12.443 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:13.892 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:13.953 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:13.956 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:13.960 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:13.963 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:13.966 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:13.969 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02519, max. 0.02519
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13443, max. 0.13382
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17476, max. 0.21222
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01108, max. 0.00683
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05282, max. 0.04437
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02518, max. 0.02518
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12997, max. 0.13183
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17536, max. 0.21242
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01092, max. 0.00770
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05285, max. 0.04463
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:14.248 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:14.252 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:14.252 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:14.252 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:14.252 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:14.253 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:15.825 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:15.886 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:15.889 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:15.892 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:15.895 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:15.898 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:15.901 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02549, max. 0.02549
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12379, max. 0.12987
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17191, max. 0.20400
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00949, max. 0.00724
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00021
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05042, max. 0.04205
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02522, max. 0.02522
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12941, max. 0.13807
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17485, max. 0.21163
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01102, max. 0.00782
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05255, max. 0.04399
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:16.180 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:16.183 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:16.184 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:16.184 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:16.184 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:16.184 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:17.627 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:17.687 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:17.690 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:17.693 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:17.696 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:17.699 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:17.702 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02531, max. 0.02531
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13704, max. 0.14241
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17575, max. 0.21271
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01101, max. 0.00773
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05198, max. 0.04387
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02531, max. 0.02531
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13126, max. 0.13264
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17560, max. 0.21222
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01109, max. 0.00728
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05206, max. 0.04386
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:17.983 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:17.986 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:17.986 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:17.986 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:17.987 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:17.987 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:19.430 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:19.491 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:19.494 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:19.497 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:19.500 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:19.503 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:19.506 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02536, max. 0.02536
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13522, max. 0.13458
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17639, max. 0.21298
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01097, max. 0.00688
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05133, max. 0.04330
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02535, max. 0.02535
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13077, max. 0.13357
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17722, max. 0.21317
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01080, max. 0.00776
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05133, max. 0.04353
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:19.785 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:19.788 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:19.789 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:19.789 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:19.789 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:19.789 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:21.234 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:21.295 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:21.298 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:21.301 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:21.304 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:21.307 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:21.310 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02539, max. 0.02539
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13738, max. 0.13781
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17697, max. 0.21305
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01090, max. 0.00688
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05062, max. 0.04287
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02539, max. 0.02539
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13322, max. 0.13995
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17697, max. 0.21305
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01090, max. 0.00772
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05105, max. 0.04295
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:21.589 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:21.593 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:21.593 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:21.593 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:21.593 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:21.593 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:23.032 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:23.093 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:23.097 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:23.100 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:23.103 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:23.106 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:23.109 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02532, max. 0.02532
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12940, max. 0.13175
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17881, max. 0.21230
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01051, max. 0.00727
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04881, max. 0.04144
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02535, max. 0.02535
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13190, max. 0.13691
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17717, max. 0.21186
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01081, max. 0.00721
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05013, max. 0.04224
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:23.388 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:23.391 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:23.392 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:23.392 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:23.392 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:23.392 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:24.964 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:25.025 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:25.028 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:25.031 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:25.035 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:25.038 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:25.041 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02533, max. 0.02533
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12941, max. 0.13196
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17934, max. 0.21207
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01042, max. 0.00725
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04810, max. 0.04088
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02534, max. 0.02534
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13027, max. 0.13437
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17781, max. 0.21193
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01056, max. 0.00772
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04880, max. 0.04154
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:25.320 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:25.323 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:25.323 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:25.323 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:25.323 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:25.324 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:26.769 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:26.830 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:26.834 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:26.837 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:26.840 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:26.843 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:26.846 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02568, max. 0.02568
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12435, max. 0.12882
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17600, max. 0.20467
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00917, max. 0.00734
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00021
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04696, max. 0.03945
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02543, max. 0.02543
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13196, max. 0.13755
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17913, max. 0.21329
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01071, max. 0.00716
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04954, max. 0.04181
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:27.127 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:27.131 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:27.131 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:27.131 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:27.131 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:27.132 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:28.586 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:28.646 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:28.650 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:28.653 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:28.656 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:28.659 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:28.662 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02556, max. 0.02556
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13565, max. 0.13649
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18110, max. 0.21629
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01072, max. 0.00687
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04953, max. 0.04197
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02556, max. 0.02556
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13016, max. 0.14157
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18113, max. 0.21626
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01071, max. 0.00781
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05026, max. 0.04233
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:28.942 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:28.945 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:28.945 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:28.945 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:28.945 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:28.946 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:30.386 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:30.447 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:30.450 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:30.453 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:30.457 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:30.460 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:30.463 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02563, max. 0.02563
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13207, max. 0.13403
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18192, max. 0.21674
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01076, max. 0.00735
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04968, max. 0.04210
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02563, max. 0.02563
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13407, max. 0.14232
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18222, max. 0.21726
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01069, max. 0.00770
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04988, max. 0.04209
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:30.741 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:30.744 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:30.744 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:30.744 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:30.744 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:30.745 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:32.187 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:32.248 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:32.251 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:32.254 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:32.257 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:32.260 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:32.263 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02567, max. 0.02567
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13777, max. 0.14627
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18259, max. 0.21682
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01062, max. 0.00762
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04901, max. 0.04164
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02567, max. 0.02567
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13812, max. 0.14042
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18259, max. 0.21682
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01062, max. 0.00671
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04879, max. 0.04151
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:32.646 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:32.649 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:32.649 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:32.649 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:32.650 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:32.650 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:34.095 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:34.153 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:34.156 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:34.158 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:34.160 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:34.163 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:34.165 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02533, max. 0.02533
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13696, max. 0.14544
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17818, max. 0.21283
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01081, max. 0.00788
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05088, max. 0.04284
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02533, max. 0.02533
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13402, max. 0.14290
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17818, max. 0.21283
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01081, max. 0.00767
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05089, max. 0.04277
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:34.438 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:34.441 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:34.441 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:34.441 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:34.441 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:34.441 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:34.836 INFO: node_feats: torch.Size([20, 2304])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:42:34.889 INFO: node_feats: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:42:34.892 INFO: node_feats_out:torch.Size([20, 256]), torch.Size([2560]), 20
2024-10-31 23:42:34.894 INFO: linear_node_feats: torch.Size([20, 256])
2024-10-31 23:42:34.897 INFO: q_node_feats: torch.Size([20, 256])
2024-10-31 23:42:34.899 INFO: v_node_feats: torch.Size([20, 256])
2024-10-31 23:42:34.901 INFO: k_node_feats: torch.Size([20, 256])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02533, max. 0.02533
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12876, max. 0.13588
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17818, max. 0.21283
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01081, max. 0.00723
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05059, max. 0.04267
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
2024-10-31 23:42:35.037 INFO: long_range_embedding: torch.Size([20, 256])
2024-10-31 23:42:35.039 INFO: long_range_energy: torch.Size([20, 1]), torch.Size([1])
2024-10-31 23:42:35.039 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-10-31 23:42:35.040 INFO: total_energy: torch.Size([1])
2024-10-31 23:42:35.040 INFO: node_energy_contributions: torch.Size([20, 4])
2024-10-31 23:42:35.040 INFO: node_energy: torch.Size([20])
2024-10-31 23:42:35.248 INFO: Epoch 12: head: default, loss=  0.6153, MAE_E_per_atom=    44.4 meV, MAE_F=    61.3 meV / A
2024-10-31 23:42:35.318 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:35.379 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:35.382 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:35.386 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:35.389 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:35.392 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:35.395 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02565, max. 0.02565
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13745, max. 0.14657
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18235, max. 0.21479
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01051, max. 0.00761
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04774, max. 0.04063
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02564, max. 0.02564
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13270, max. 0.13667
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18252, max. 0.21496
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01033, max. 0.00777
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04750, max. 0.04060
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:35.670 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:35.674 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:35.674 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:35.674 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:35.674 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:35.674 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:37.110 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:37.170 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:37.174 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:37.177 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:37.180 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:37.183 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:37.186 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02587, max. 0.02587
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12496, max. 0.12981
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.17909, max. 0.20361
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00893, max. 0.00744
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00021
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04434, max. 0.03740
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02560, max. 0.02560
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13230, max. 0.13422
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18376, max. 0.21291
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01008, max. 0.00724
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04553, max. 0.03892
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:37.468 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:37.472 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:37.472 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:37.472 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:37.472 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:37.473 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:38.934 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:38.996 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:38.999 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:39.002 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:39.005 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:39.008 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:39.011 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02569, max. 0.02569
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13027, max. 0.14366
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18328, max. 0.21393
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01039, max. 0.00784
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04726, max. 0.03995
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02569, max. 0.02569
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13561, max. 0.13872
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18326, max. 0.21396
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01040, max. 0.00692
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04640, max. 0.03950
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:39.289 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:39.292 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:39.292 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:39.293 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:39.293 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:39.293 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:40.737 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:40.798 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:40.801 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:40.804 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:40.808 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:40.811 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:40.814 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02573, max. 0.02573
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13174, max. 0.13591
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18352, max. 0.21338
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01044, max. 0.00742
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04617, max. 0.03930
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02573, max. 0.02573
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13249, max. 0.14029
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18396, max. 0.21388
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01038, max. 0.00714
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04674, max. 0.03963
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:41.192 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:41.196 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:41.196 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:41.196 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:41.196 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:41.197 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:42.646 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:42.707 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:42.710 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:42.713 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:42.716 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:42.719 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:42.723 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02576, max. 0.02576
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13396, max. 0.14469
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18414, max. 0.21355
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01031, max. 0.00777
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04599, max. 0.03902
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02576, max. 0.02576
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13766, max. 0.14218
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18414, max. 0.21355
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01031, max. 0.00669
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04529, max. 0.03873
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:43.001 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:43.004 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:43.005 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:43.005 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:43.005 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:43.005 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:44.461 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:44.522 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:44.526 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:44.529 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:44.532 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:44.535 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:44.538 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02576, max. 0.02576
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13544, max. 0.13972
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18409, max. 0.21252
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01024, max. 0.00695
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04466, max. 0.03812
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02576, max. 0.02576
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13374, max. 0.14488
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18409, max. 0.21252
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01024, max. 0.00777
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04520, max. 0.03837
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:44.815 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:44.818 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:44.818 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:44.819 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:44.819 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:44.819 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:46.248 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:46.309 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:46.314 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:46.317 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:46.320 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:46.323 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:46.326 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02573, max. 0.02573
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13781, max. 0.14832
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18372, max. 0.21095
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01016, max. 0.00761
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04374, max. 0.03743
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02571, max. 0.02571
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13311, max. 0.13533
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18527, max. 0.21056
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00998, max. 0.00725
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04251, max. 0.03651
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:46.606 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:46.610 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:46.610 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:46.610 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:46.610 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:46.610 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:48.064 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:48.125 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:48.128 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:48.132 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:48.135 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:48.138 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:48.141 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02572, max. 0.02572
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13031, max. 0.14466
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18392, max. 0.21035
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01010, max. 0.00781
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00021
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04395, max. 0.03728
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02572, max. 0.02572
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13657, max. 0.14248
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18390, max. 0.21038
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01011, max. 0.00660
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00021
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04278, max. 0.03668
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:48.418 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:48.421 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:48.421 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:48.421 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:48.421 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:48.422 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:49.967 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:50.028 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:50.031 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:50.034 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:50.037 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:50.040 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:50.043 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02568, max. 0.02568
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13205, max. 0.13792
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18394, max. 0.20961
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00988, max. 0.00776
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00021
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04190, max. 0.03606
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02569, max. 0.02569
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13093, max. 0.14062
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18396, max. 0.20944
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01007, max. 0.00703
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00021
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04304, max. 0.03660
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:50.322 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:50.325 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:50.325 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:50.325 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:50.326 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:50.326 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:51.774 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:51.835 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:51.839 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:51.842 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:51.845 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:51.848 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:51.851 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02591, max. 0.02591
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12397, max. 0.13068
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18098, max. 0.19904
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00881, max. 0.00748
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00018, max. 0.00020
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03918, max. 0.03317
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02568, max. 0.02568
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13036, max. 0.13643
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18361, max. 0.20859
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01014, max. 0.00735
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00021
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04189, max. 0.03577
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:52.130 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:52.133 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:52.133 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:52.133 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:52.134 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:52.134 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:53.604 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:53.664 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:53.668 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:53.671 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:53.674 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:53.677 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:53.680 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02573, max. 0.02573
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13079, max. 0.14075
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18513, max. 0.21081
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01005, max. 0.00696
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04284, max. 0.03636
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02573, max. 0.02573
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13594, max. 0.14264
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18502, max. 0.21085
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01004, max. 0.00648
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04171, max. 0.03572
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:53.961 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:53.964 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:53.964 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:53.964 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:53.964 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:53.965 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:55.395 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:55.455 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:55.459 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:55.462 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:55.465 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:55.468 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:55.471 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02576, max. 0.02576
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13262, max. 0.14534
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18580, max. 0.21213
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01005, max. 0.00764
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04266, max. 0.03617
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02600, max. 0.02600
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12430, max. 0.13092
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18282, max. 0.20180
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00890, max. 0.00748
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00021
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03936, max. 0.03330
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:55.750 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:55.754 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:55.754 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:55.754 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:55.754 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:55.755 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:57.207 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:57.268 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:57.272 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:57.275 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:57.278 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:57.281 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:57.284 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02584, max. 0.02584
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13100, max. 0.14531
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18723, max. 0.21469
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01010, max. 0.00770
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04368, max. 0.03687
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02584, max. 0.02584
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13846, max. 0.14902
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18720, max. 0.21472
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01011, max. 0.00744
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04265, max. 0.03634
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:57.563 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:57.566 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:57.566 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:57.567 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:57.567 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:57.567 INFO: node_energy: torch.Size([40])
2024-10-31 23:42:59.111 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:59.171 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:42:59.175 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:42:59.178 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:42:59.181 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:42:59.184 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:42:59.187 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02589, max. 0.02589
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13471, max. 0.14093
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18815, max. 0.21648
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01014, max. 0.00685
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04277, max. 0.03634
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02589, max. 0.02589
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13236, max. 0.13744
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18780, max. 0.21592
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01034, max. 0.00736
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04301, max. 0.03649
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:42:59.467 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:42:59.470 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:42:59.470 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:42:59.470 INFO: total_energy: torch.Size([2])
2024-10-31 23:42:59.470 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:42:59.471 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:00.922 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:00.982 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:00.986 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:00.989 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:00.992 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:00.995 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:00.998 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02594, max. 0.02594
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13414, max. 0.13889
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18880, max. 0.21783
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01003, max. 0.00772
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04243, max. 0.03624
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02593, max. 0.02592
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13457, max. 0.13601
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.19013, max. 0.21690
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01016, max. 0.00708
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04138, max. 0.03532
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:01.278 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:01.281 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:01.281 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:01.282 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:01.282 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:01.282 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:02.744 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:02.802 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:02.804 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:02.807 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:02.809 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:02.812 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:02.814 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02572, max. 0.02572
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13860, max. 0.15026
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18453, max. 0.21358
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01027, max. 0.00784
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04488, max. 0.03806
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02572, max. 0.02572
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13563, max. 0.14789
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18453, max. 0.21358
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01027, max. 0.00764
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04490, max. 0.03800
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:03.086 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:03.089 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:03.089 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:03.089 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:03.089 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:03.089 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:03.480 INFO: node_feats: torch.Size([20, 2304])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:43:03.533 INFO: node_feats: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:43:03.535 INFO: node_feats_out:torch.Size([20, 256]), torch.Size([2560]), 20
2024-10-31 23:43:03.538 INFO: linear_node_feats: torch.Size([20, 256])
2024-10-31 23:43:03.540 INFO: q_node_feats: torch.Size([20, 256])
2024-10-31 23:43:03.542 INFO: v_node_feats: torch.Size([20, 256])
2024-10-31 23:43:03.545 INFO: k_node_feats: torch.Size([20, 256])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02572, max. 0.02572
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13032, max. 0.14070
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18453, max. 0.21358
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01027, max. 0.00724
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04448, max. 0.03782
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
2024-10-31 23:43:03.679 INFO: long_range_embedding: torch.Size([20, 256])
2024-10-31 23:43:03.682 INFO: long_range_energy: torch.Size([20, 1]), torch.Size([1])
2024-10-31 23:43:03.682 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-10-31 23:43:03.682 INFO: total_energy: torch.Size([1])
2024-10-31 23:43:03.683 INFO: node_energy_contributions: torch.Size([20, 4])
2024-10-31 23:43:03.683 INFO: node_energy: torch.Size([20])
2024-10-31 23:43:03.890 INFO: Epoch 15: head: default, loss=  0.5383, MAE_E_per_atom=    48.8 meV, MAE_F=    57.7 meV / A
2024-10-31 23:43:03.960 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:04.021 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:04.024 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:04.027 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:04.030 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:04.033 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:04.036 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02605, max. 0.02605
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13347, max. 0.13812
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18957, max. 0.21952
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01042, max. 0.00739
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04341, max. 0.03676
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02605, max. 0.02605
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13943, max. 0.14992
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18994, max. 0.22009
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01020, max. 0.00742
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04322, max. 0.03669
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:04.312 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:04.315 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:04.315 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:04.316 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:04.316 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:04.316 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:05.752 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:05.813 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:05.816 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:05.819 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:05.822 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:05.825 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:05.828 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02614, max. 0.02614
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13553, max. 0.13968
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.19078, max. 0.22184
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01012, max. 0.00777
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00023
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04286, max. 0.03655
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02615, max. 0.02615
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13742, max. 0.14428
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.19077, max. 0.22177
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01021, max. 0.00650
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00023
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04299, max. 0.03652
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:06.111 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:06.114 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:06.115 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:06.115 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:06.115 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:06.115 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:07.666 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:07.727 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:07.731 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:07.734 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:07.737 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:07.740 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:07.743 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02625, max. 0.02625
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13439, max. 0.14750
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.19145, max. 0.22316
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01022, max. 0.00772
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00020, max. 0.00023
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04390, max. 0.03698
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02625, max. 0.02625
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13260, max. 0.14703
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.19147, max. 0.22312
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01021, max. 0.00776
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00020, max. 0.00023
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04439, max. 0.03728
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:08.021 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:08.024 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:08.025 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:08.025 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:08.025 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:08.025 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:09.467 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:09.528 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:09.531 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:09.535 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:09.538 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:09.541 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:09.544 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02631, max. 0.02631
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13678, max. 0.13738
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.19283, max. 0.22234
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01029, max. 0.00717
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00020, max. 0.00023
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04118, max. 0.03510
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02633, max. 0.02633
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13588, max. 0.14286
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.19154, max. 0.22345
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01019, max. 0.00697
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00020, max. 0.00023
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04271, max. 0.03618
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:09.821 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:09.825 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:09.825 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:09.825 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:09.825 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:09.825 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:11.276 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:11.336 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:11.340 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:11.343 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:11.346 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:11.349 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:11.352 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02666, max. 0.02666
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12638, max. 0.13310
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.18875, max. 0.21270
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00918, max. 0.00774
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00019, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03927, max. 0.03309
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02643, max. 0.02643
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13262, max. 0.14350
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.19197, max. 0.22405
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01023, max. 0.00744
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00020, max. 0.00023
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04346, max. 0.03662
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:11.632 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:11.636 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:11.636 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:11.636 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:11.636 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:11.636 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:13.102 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:13.163 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:13.166 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:13.170 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:13.173 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:13.176 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:13.179 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02657, max. 0.02657
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13363, max. 0.14850
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.19289, max. 0.22600
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01019, max. 0.00789
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00020, max. 0.00023
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04378, max. 0.03680
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02655, max. 0.02655
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13832, max. 0.13836
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.19414, max. 0.22478
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01035, max. 0.00731
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00020, max. 0.00023
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04086, max. 0.03487
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:13.457 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:13.461 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:13.461 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:13.461 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:13.461 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:13.462 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:14.903 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:14.963 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:14.966 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:14.970 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:14.973 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:14.976 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:14.979 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02673, max. 0.02673
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13382, max. 0.14485
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.19415, max. 0.22820
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01030, max. 0.00764
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00020, max. 0.00023
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04388, max. 0.03699
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02673, max. 0.02673
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.14177, max. 0.15296
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.19404, max. 0.22824
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01024, max. 0.00763
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00020, max. 0.00023
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04277, max. 0.03633
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:15.359 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:15.362 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:15.362 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:15.363 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:15.363 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:15.363 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:16.808 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:16.869 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:16.873 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:16.876 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:16.879 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:16.882 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:16.886 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02686, max. 0.02686
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13652, max. 0.15045
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.19482, max. 0.22953
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01024, max. 0.00801
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00020, max. 0.00023
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04342, max. 0.03664
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02685, max. 0.02685
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13996, max. 0.14280
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.19479, max. 0.22955
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01028, max. 0.00810
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00020, max. 0.00023
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04210, max. 0.03598
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:17.163 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:17.166 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:17.166 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:17.167 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:17.167 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:17.167 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:18.614 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:18.675 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:18.678 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:18.681 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:18.684 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:18.687 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:18.690 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02698, max. 0.02698
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.14008, max. 0.14793
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.19553, max. 0.23066
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01024, max. 0.00684
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00020, max. 0.00024
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04213, max. 0.03591
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02698, max. 0.02698
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13849, max. 0.14605
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.19553, max. 0.23066
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01024, max. 0.00741
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00020, max. 0.00024
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04244, max. 0.03606
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:18.969 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:18.973 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:18.973 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:18.973 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:18.973 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:18.973 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:20.430 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:20.491 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:20.495 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:20.498 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:20.501 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:20.504 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:20.507 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02707, max. 0.02707
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13987, max. 0.14288
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.19549, max. 0.23037
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01057, max. 0.00794
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00020, max. 0.00024
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04216, max. 0.03580
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02730, max. 0.02730
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.12878, max. 0.13592
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.19263, max. 0.21863
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00936, max. 0.00812
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00020, max. 0.00022
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03844, max. 0.03269
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:20.789 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:20.793 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:20.793 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:20.793 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:20.793 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:20.793 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:22.262 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:22.323 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:22.327 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:22.330 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:22.333 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:22.336 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:22.339 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02720, max. 0.02720
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.14174, max. 0.14902
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.19725, max. 0.23322
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01025, max. 0.00694
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00020, max. 0.00024
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04167, max. 0.03557
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02720, max. 0.02720
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13775, max. 0.15231
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.19725, max. 0.23322
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01025, max. 0.00821
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00020, max. 0.00024
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04407, max. 0.03625
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:22.617 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:22.621 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:22.621 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:22.621 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:22.621 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:22.621 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:24.151 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:24.212 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:24.215 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:24.219 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:24.222 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:24.225 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:24.228 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02729, max. 0.02729
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.14307, max. 0.14500
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.19817, max. 0.23466
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01040, max. 0.00837
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00020, max. 0.00024
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04118, max. 0.03529
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02727, max. 0.02727
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.14379, max. 0.14166
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.19944, max. 0.23301
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01059, max. 0.00789
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00020, max. 0.00024
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03988, max. 0.03419
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:24.510 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:24.513 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:24.513 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:24.514 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:24.514 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:24.514 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:25.967 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:26.028 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:26.032 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:26.035 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:26.038 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:26.041 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:26.044 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02743, max. 0.02743
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.14193, max. 0.14851
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.19995, max. 0.23756
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01029, max. 0.00782
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00020, max. 0.00024
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04203, max. 0.03575
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02743, max. 0.02743
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13617, max. 0.14831
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.20005, max. 0.23753
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01040, max. 0.00817
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00020, max. 0.00024
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04674, max. 0.03663
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:26.324 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:26.328 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:26.328 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:26.328 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:26.328 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:26.328 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:27.769 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:27.830 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:27.833 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:27.837 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:27.840 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:27.843 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:27.846 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02776, max. 0.02776
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13071, max. 0.13832
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.19790, max. 0.22663
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00955, max. 0.00844
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00020, max. 0.00023
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03831, max. 0.03324
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02754, max. 0.02754
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.14500, max. 0.15721
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.20128, max. 0.23975
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01032, max. 0.00823
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00021, max. 0.00024
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04213, max. 0.03591
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:28.125 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:28.129 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:28.129 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:28.129 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:28.129 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:28.129 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:29.580 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:29.641 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:29.644 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:29.647 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:29.650 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:29.653 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:29.656 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02767, max. 0.02767
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13797, max. 0.15422
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.20345, max. 0.24337
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01038, max. 0.00848
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00021, max. 0.00025
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04960, max. 0.03716
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02767, max. 0.02767
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.14503, max. 0.14610
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.20298, max. 0.24280
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01076, max. 0.00846
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00021, max. 0.00025
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04427, max. 0.03631
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:29.937 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:29.940 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:29.940 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:29.941 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:29.941 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:29.941 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:31.401 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:31.459 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:31.461 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:31.464 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:31.466 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:31.468 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:31.471 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02680, max. 0.02680
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.14280, max. 0.15612
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.19436, max. 0.22878
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01028, max. 0.00819
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00020, max. 0.00023
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04342, max. 0.03680
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02680, max. 0.02680
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13973, max. 0.15387
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.19436, max. 0.22878
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01048, max. 0.00798
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00020, max. 0.00023
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04348, max. 0.03678
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:31.834 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:31.837 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:31.837 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:31.837 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:31.838 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:31.838 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:32.225 INFO: node_feats: torch.Size([20, 2304])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:43:32.279 INFO: node_feats: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:43:32.281 INFO: node_feats_out:torch.Size([20, 256]), torch.Size([2560]), 20
2024-10-31 23:43:32.284 INFO: linear_node_feats: torch.Size([20, 256])
2024-10-31 23:43:32.286 INFO: q_node_feats: torch.Size([20, 256])
2024-10-31 23:43:32.288 INFO: v_node_feats: torch.Size([20, 256])
2024-10-31 23:43:32.291 INFO: k_node_feats: torch.Size([20, 256])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02680, max. 0.02680
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13423, max. 0.14640
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.19436, max. 0.22878
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01028, max. 0.00766
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00020, max. 0.00023
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04289, max. 0.03647
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
2024-10-31 23:43:32.427 INFO: long_range_embedding: torch.Size([20, 256])
2024-10-31 23:43:32.429 INFO: long_range_energy: torch.Size([20, 1]), torch.Size([1])
2024-10-31 23:43:32.429 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-10-31 23:43:32.429 INFO: total_energy: torch.Size([1])
2024-10-31 23:43:32.430 INFO: node_energy_contributions: torch.Size([20, 4])
2024-10-31 23:43:32.430 INFO: node_energy: torch.Size([20])
2024-10-31 23:43:32.638 INFO: Epoch 18: head: default, loss=  0.5660, MAE_E_per_atom=    49.0 meV, MAE_F=    59.3 meV / A
2024-10-31 23:43:32.708 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:32.769 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:32.772 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:32.775 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:32.778 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:32.781 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:32.785 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02777, max. 0.02777
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.14833, max. 0.14401
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.20636, max. 0.24451
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01085, max. 0.00835
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00021, max. 0.00025
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04062, max. 0.03481
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02802, max. 0.02802
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13186, max. 0.13967
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.20175, max. 0.23291
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00968, max. 0.00863
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00021, max. 0.00024
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03878, max. 0.03374
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:33.064 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:33.072 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:33.072 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:33.072 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:33.072 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:33.072 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:34.525 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:34.585 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:34.589 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:34.592 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:34.595 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:34.598 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:34.601 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02794, max. 0.02794
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.14681, max. 0.15132
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.20760, max. 0.25086
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01050, max. 0.00832
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00021, max. 0.00026
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04585, max. 0.03700
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02794, max. 0.02794
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.14689, max. 0.15937
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.20760, max. 0.25086
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01053, max. 0.00865
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00021, max. 0.00026
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04360, max. 0.03710
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:34.883 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:34.888 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:34.888 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:34.888 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:34.888 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:34.888 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:36.327 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:36.388 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:36.392 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:36.395 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:36.398 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:36.401 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:36.404 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02806, max. 0.02806
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13971, max. 0.15632
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.20953, max. 0.25409
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01055, max. 0.00872
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00021, max. 0.00026
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05291, max. 0.03840
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02806, max. 0.02806
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.14989, max. 0.15327
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.20951, max. 0.25413
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01081, max. 0.00737
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00021, max. 0.00026
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04348, max. 0.03707
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:36.682 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:36.685 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:36.686 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:36.686 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:36.686 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:36.686 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:38.123 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:38.184 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:38.187 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:38.190 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:38.193 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:38.196 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:38.199 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02814, max. 0.02814
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.15132, max. 0.14938
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.21075, max. 0.25620
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01081, max. 0.00892
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00022, max. 0.00026
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04308, max. 0.03687
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02815, max. 0.02815
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.14215, max. 0.15760
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.21086, max. 0.25628
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01059, max. 0.00882
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00022, max. 0.00026
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05237, max. 0.03789
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:38.480 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:38.483 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:38.483 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:38.483 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:38.483 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:38.484 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:40.047 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:40.108 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:40.111 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:40.114 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:40.117 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:40.120 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:40.123 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02824, max. 0.02824
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.14160, max. 0.15245
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.21208, max. 0.25800
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01068, max. 0.00887
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00022, max. 0.00026
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05397, max. 0.03808
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02824, max. 0.02824
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.15015, max. 0.14923
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.21151, max. 0.25740
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01096, max. 0.00899
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00022, max. 0.00026
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04887, max. 0.03726
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:40.403 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:40.406 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:40.406 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:40.407 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:40.407 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:40.407 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:41.866 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:41.927 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:41.930 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:41.933 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:41.937 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:41.940 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:41.943 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02833, max. 0.02833
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.14213, max. 0.15293
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.21287, max. 0.25921
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01067, max. 0.00895
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00022, max. 0.00026
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05488, max. 0.03785
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02833, max. 0.02833
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.15196, max. 0.15470
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.21277, max. 0.25924
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01095, max. 0.00753
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00022, max. 0.00026
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04284, max. 0.03655
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:42.220 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:42.223 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:42.223 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:42.224 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:42.224 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:42.224 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:43.654 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:43.715 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:43.718 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:43.721 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:43.724 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:43.727 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:43.730 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02837, max. 0.02837
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.15340, max. 0.14698
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.21432, max. 0.25737
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01107, max. 0.00894
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00022, max. 0.00026
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04034, max. 0.03527
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02862, max. 0.02862
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13435, max. 0.14293
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.20964, max. 0.24510
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.00988, max. 0.00914
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00021, max. 0.00025
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03833, max. 0.03460
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:44.010 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:44.014 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:44.014 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:44.014 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:44.014 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:44.014 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:45.471 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:45.533 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:45.536 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:45.539 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:45.542 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:45.545 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:45.548 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02852, max. 0.02851
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.14155, max. 0.15899
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.21471, max. 0.26216
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01059, max. 0.00908
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00022, max. 0.00027
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05785, max. 0.03936
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02852, max. 0.02852
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.15231, max. 0.15087
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.21550, max. 0.26156
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01097, max. 0.00926
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00022, max. 0.00027
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05140, max. 0.03668
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:45.825 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:45.828 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:45.828 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:45.829 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:45.829 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:45.829 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:47.275 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:47.335 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:47.341 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:47.344 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:47.347 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:47.350 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:47.353 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02863, max. 0.02863
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.15240, max. 0.15539
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.21584, max. 0.26409
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01065, max. 0.00907
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00022, max. 0.00027
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05219, max. 0.03644
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02863, max. 0.02863
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.14395, max. 0.16052
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.21584, max. 0.26409
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01072, max. 0.00921
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00022, max. 0.00027
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05719, max. 0.03921
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:47.633 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:47.636 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:47.636 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:47.636 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:47.636 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:47.637 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:49.180 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:49.241 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:49.245 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:49.248 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:49.251 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:49.254 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:49.257 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02871, max. 0.02871
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.15015, max. 0.16387
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.21654, max. 0.26501
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01067, max. 0.00952
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00022, max. 0.00027
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04255, max. 0.03629
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02870, max. 0.02870
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.15628, max. 0.15263
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.21638, max. 0.26487
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01095, max. 0.00939
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00022, max. 0.00027
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04314, max. 0.03594
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:49.537 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:49.540 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:49.540 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:49.540 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:49.540 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:49.541 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:51.001 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:51.061 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:51.065 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:51.068 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:51.071 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:51.074 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:51.077 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02876, max. 0.02876
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.15706, max. 0.14908
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.21838, max. 0.26333
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01132, max. 0.00936
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00022, max. 0.00027
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03954, max. 0.03570
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02878, max. 0.02878
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.14450, max. 0.16156
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.21727, max. 0.26585
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01076, max. 0.00935
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00022, max. 0.00027
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05905, max. 0.04059
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:51.356 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:51.359 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:51.359 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:51.360 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:51.360 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:51.360 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:52.799 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:52.859 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:52.863 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:52.866 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:52.869 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:52.872 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:52.875 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02887, max. 0.02887
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.15443, max. 0.15696
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.22075, max. 0.26704
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01070, max. 0.00937
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00023, max. 0.00027
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05481, max. 0.03821
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02887, max. 0.02887
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.15551, max. 0.15305
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.22256, max. 0.26638
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01102, max. 0.00963
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00023, max. 0.00027
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05477, max. 0.03691
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:53.155 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:53.158 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:53.159 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:53.159 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:53.159 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:53.159 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:54.605 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:54.666 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:54.669 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:54.672 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:54.675 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:54.678 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:54.682 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02893, max. 0.02893
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.15841, max. 0.15401
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.21975, max. 0.26773
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01104, max. 0.00959
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00022, max. 0.00027
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04510, max. 0.03597
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02894, max. 0.02894
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.15120, max. 0.16532
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.21899, max. 0.26791
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01071, max. 0.00982
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00022, max. 0.00027
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04156, max. 0.03600
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:54.960 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:54.964 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:54.964 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:54.964 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:54.964 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:54.965 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:56.409 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:56.470 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:56.474 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:56.477 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:56.480 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:56.483 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:56.486 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02900, max. 0.02900
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.15814, max. 0.15862
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.22183, max. 0.26888
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01148, max. 0.00801
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00023, max. 0.00027
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04635, max. 0.03599
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02922, max. 0.02922
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13706, max. 0.14648
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.21625, max. 0.25326
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01036, max. 0.00975
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00022, max. 0.00026
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03868, max. 0.03498
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:56.867 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:56.871 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:56.871 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:56.871 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:56.871 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:56.871 INFO: node_energy: torch.Size([40])
2024-10-31 23:43:58.325 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:58.386 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:43:58.389 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:43:58.392 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:43:58.395 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:43:58.398 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:43:58.402 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02908, max. 0.02908
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.14794, max. 0.15735
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.22582, max. 0.27124
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01075, max. 0.00972
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00023, max. 0.00028
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06335, max. 0.04326
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02908, max. 0.02908
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.14416, max. 0.16253
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.22154, max. 0.27123
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01073, max. 0.00954
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00023, max. 0.00028
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06460, max. 0.04438
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:43:58.681 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:43:58.684 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:43:58.684 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:43:58.684 INFO: total_energy: torch.Size([2])
2024-10-31 23:43:58.685 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:43:58.685 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:00.138 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:00.196 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:00.199 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:00.201 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:00.204 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:00.206 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:00.208 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02814, max. 0.02814
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.15037, max. 0.16385
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.20964, max. 0.25342
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01091, max. 0.00901
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00021, max. 0.00026
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04313, max. 0.03658
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02814, max. 0.02814
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.14541, max. 0.16169
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.20964, max. 0.25342
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01135, max. 0.00877
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00021, max. 0.00026
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04325, max. 0.03661
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:00.479 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:00.482 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:00.482 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:00.482 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:00.482 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:00.482 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:00.872 INFO: node_feats: torch.Size([20, 2304])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:44:00.924 INFO: node_feats: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:44:00.927 INFO: node_feats_out:torch.Size([20, 256]), torch.Size([2560]), 20
2024-10-31 23:44:00.929 INFO: linear_node_feats: torch.Size([20, 256])
2024-10-31 23:44:00.932 INFO: q_node_feats: torch.Size([20, 256])
2024-10-31 23:44:00.934 INFO: v_node_feats: torch.Size([20, 256])
2024-10-31 23:44:00.936 INFO: k_node_feats: torch.Size([20, 256])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02814, max. 0.02814
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13960, max. 0.15382
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.20964, max. 0.25342
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01054, max. 0.00876
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00021, max. 0.00026
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04248, max. 0.03616
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
2024-10-31 23:44:01.071 INFO: long_range_embedding: torch.Size([20, 256])
2024-10-31 23:44:01.074 INFO: long_range_energy: torch.Size([20, 1]), torch.Size([1])
2024-10-31 23:44:01.074 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-10-31 23:44:01.074 INFO: total_energy: torch.Size([1])
2024-10-31 23:44:01.074 INFO: node_energy_contributions: torch.Size([20, 4])
2024-10-31 23:44:01.075 INFO: node_energy: torch.Size([20])
2024-10-31 23:44:01.285 INFO: Epoch 21: head: default, loss=  0.6631, MAE_E_per_atom=    48.9 meV, MAE_F=    63.6 meV / A
2024-10-31 23:44:01.356 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:01.417 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:01.420 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:01.424 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:01.427 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:01.430 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:01.433 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02915, max. 0.02915
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.14597, max. 0.16395
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.22277, max. 0.27304
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01091, max. 0.00968
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00023, max. 0.00028
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06381, max. 0.04416
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02915, max. 0.02915
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.14856, max. 0.15777
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.22743, max. 0.27300
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01078, max. 0.00981
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00023, max. 0.00028
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06429, max. 0.04396
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:01.710 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:01.713 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:01.713 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:01.714 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:01.714 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:01.714 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:03.145 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:03.206 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:03.209 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:03.212 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:03.215 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:03.218 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:03.221 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02922, max. 0.02922
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.15760, max. 0.15921
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.22880, max. 0.27424
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01102, max. 0.00988
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00023, max. 0.00028
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05916, max. 0.04150
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02921, max. 0.02921
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.16123, max. 0.15566
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.22590, max. 0.27400
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01124, max. 0.00990
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00023, max. 0.00028
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04774, max. 0.03597
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:03.501 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:03.504 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:03.504 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:03.505 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:03.505 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:03.505 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:04.949 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:05.010 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:05.013 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:05.017 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:05.020 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:05.023 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:05.026 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02928, max. 0.02928
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.14520, max. 0.16369
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.22469, max. 0.27557
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01095, max. 0.00968
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00023, max. 0.00028
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06737, max. 0.04641
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02925, max. 0.02925
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.16160, max. 0.15166
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.22562, max. 0.27260
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01163, max. 0.01001
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00023, max. 0.00028
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03854, max. 0.03563
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:05.304 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:05.308 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:05.308 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:05.308 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:05.308 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:05.308 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:06.859 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:06.920 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:06.923 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:06.926 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:06.929 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:06.932 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:06.936 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02958, max. 0.02958
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.13879, max. 0.14842
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.22222, max. 0.26125
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01084, max. 0.01026
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00023, max. 0.00027
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03929, max. 0.03494
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02936, max. 0.02936
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.16171, max. 0.16059
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.23084, max. 0.27758
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01187, max. 0.00826
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00024, max. 0.00028
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05003, max. 0.03604
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:07.214 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:07.217 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:07.217 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:07.217 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:07.218 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:07.218 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:08.669 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:08.729 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:08.733 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:08.736 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:08.739 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:08.742 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:08.745 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02947, max. 0.02947
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.16105, max. 0.15651
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.23500, max. 0.28027
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01140, max. 0.01038
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00024, max. 0.00029
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06143, max. 0.04179
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02947, max. 0.02947
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.15518, max. 0.16838
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.22792, max. 0.28096
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01112, max. 0.01061
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00023, max. 0.00029
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04117, max. 0.03619
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:09.025 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:09.029 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:09.029 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:09.029 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:09.029 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:09.029 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:10.486 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:10.547 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:10.550 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:10.553 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:10.556 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:10.559 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:10.563 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02956, max. 0.02956
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.16186, max. 0.15698
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.23683, max. 0.28323
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01147, max. 0.01049
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00024, max. 0.00029
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06245, max. 0.04252
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02956, max. 0.02955
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.16465, max. 0.15740
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.23417, max. 0.28361
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01150, max. 0.01034
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00024, max. 0.00029
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05055, max. 0.03612
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:10.842 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:10.845 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:10.845 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:10.845 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:10.845 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:10.846 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:12.286 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:12.346 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:12.349 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:12.352 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:12.355 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:12.359 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:12.362 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02965, max. 0.02965
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.16533, max. 0.15340
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.23251, max. 0.28382
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01193, max. 0.01049
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00024, max. 0.00029
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.03909, max. 0.03593
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02967, max. 0.02967
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.15687, max. 0.16941
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.23164, max. 0.28716
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01128, max. 0.01087
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00024, max. 0.00029
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04160, max. 0.03638
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:12.641 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:12.644 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:12.644 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:12.645 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:12.645 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:12.645 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:14.094 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:14.253 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:14.257 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:14.260 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:14.263 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:14.266 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:14.269 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02979, max. 0.02979
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.16271, max. 0.16223
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.24075, max. 0.29066
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01153, max. 0.01064
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00025, max. 0.00030
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06507, max. 0.04588
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02979, max. 0.02979
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.14991, max. 0.16737
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.23378, max. 0.29066
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01126, max. 0.01011
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00024, max. 0.00030
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.07112, max. 0.04953
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:14.549 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:14.553 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:14.553 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:14.553 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:14.553 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:14.553 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:15.997 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:16.058 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:16.062 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:16.065 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:16.068 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:16.071 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:16.074 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02988, max. 0.02988
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.15406, max. 0.16129
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.24106, max. 0.29301
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01125, max. 0.01060
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00025, max. 0.00030
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.07256, max. 0.04998
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02988, max. 0.02988
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.16641, max. 0.16300
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.24178, max. 0.29305
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01233, max. 0.00862
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00025, max. 0.00030
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05441, max. 0.03895
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:16.353 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:16.356 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:16.357 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:16.357 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:16.357 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:16.357 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:17.798 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:17.858 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:17.862 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:17.865 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:17.868 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:17.871 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:17.874 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03019, max. 0.03019
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.14147, max. 0.15128
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.23271, max. 0.27725
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01157, max. 0.01098
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00024, max. 0.00028
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04006, max. 0.03520
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02997, max. 0.02996
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.14887, max. 0.16712
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.23658, max. 0.29458
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01155, max. 0.01034
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00024, max. 0.00030
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.07526, max. 0.05217
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:18.153 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:18.157 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:18.157 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:18.157 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:18.157 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:18.157 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:19.627 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:19.688 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:19.692 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:19.695 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:19.698 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:19.701 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:19.704 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03030, max. 0.03030
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.14198, max. 0.15200
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.23449, max. 0.27990
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01171, max. 0.01112
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00024, max. 0.00029
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04019, max. 0.03524
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03008, max. 0.03008
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.15290, max. 0.16886
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.23837, max. 0.29746
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01140, max. 0.01034
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00024, max. 0.00030
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.07457, max. 0.05207
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:19.981 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:19.985 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:19.985 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:19.985 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:19.985 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:19.986 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:21.426 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:21.487 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:21.490 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:21.493 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:21.496 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:21.499 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:21.502 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03021, max. 0.03021
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.16091, max. 0.17220
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.24077, max. 0.30144
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01163, max. 0.01158
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00025, max. 0.00031
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04239, max. 0.03671
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03021, max. 0.03021
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.16922, max. 0.16463
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.24873, max. 0.30144
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01259, max. 0.00887
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00025, max. 0.00031
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05726, max. 0.04110
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:21.783 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:21.786 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:21.786 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:21.787 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:21.787 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:21.787 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:23.329 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:23.390 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:23.394 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:23.397 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:23.400 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:23.403 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:23.407 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03032, max. 0.03032
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.15235, max. 0.16904
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.24295, max. 0.30434
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01184, max. 0.01068
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00025, max. 0.00031
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.07946, max. 0.05529
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03029, max. 0.03029
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.17045, max. 0.15627
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.24525, max. 0.30057
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01233, max. 0.01125
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00025, max. 0.00031
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04119, max. 0.03616
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:23.686 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:23.689 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:23.690 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:23.690 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:23.690 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:23.690 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:25.139 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:25.200 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:25.204 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:25.207 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:25.210 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:25.213 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:25.216 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03044, max. 0.03044
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.15817, max. 0.16406
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.25219, max. 0.30759
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01163, max. 0.01119
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00026, max. 0.00031
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.07905, max. 0.05477
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03043, max. 0.03043
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.17218, max. 0.16176
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.25259, max. 0.30718
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01210, max. 0.01135
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00026, max. 0.00031
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05759, max. 0.03895
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:25.496 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:25.499 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:25.499 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:25.500 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:25.500 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:25.500 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:26.948 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:27.009 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:27.013 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:27.016 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:27.019 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:27.022 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:27.025 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03056, max. 0.03056
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.16908, max. 0.16642
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.25691, max. 0.31082
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01208, max. 0.01165
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00026, max. 0.00032
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.07340, max. 0.05213
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03056, max. 0.03056
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.16987, max. 0.16213
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.25598, max. 0.31009
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01193, max. 0.01160
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00026, max. 0.00032
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.07233, max. 0.04977
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:27.305 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:27.308 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:27.308 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:27.308 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:27.309 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:27.309 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:28.765 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:28.822 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:28.825 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:28.827 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:28.830 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:28.832 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:28.834 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02938, max. 0.02938
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.16129, max. 0.17106
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.22747, max. 0.28049
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01184, max. 0.00987
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00023, max. 0.00029
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04261, max. 0.03603
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02938, max. 0.02938
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.15349, max. 0.16899
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.22729, max. 0.28049
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01236, max. 0.00960
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00023, max. 0.00029
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04275, max. 0.03601
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:29.106 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:29.108 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:29.108 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:29.109 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:29.109 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:29.109 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:29.500 INFO: node_feats: torch.Size([20, 2304])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:44:29.552 INFO: node_feats: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:44:29.555 INFO: node_feats_out:torch.Size([20, 256]), torch.Size([2560]), 20
2024-10-31 23:44:29.557 INFO: linear_node_feats: torch.Size([20, 256])
2024-10-31 23:44:29.560 INFO: q_node_feats: torch.Size([20, 256])
2024-10-31 23:44:29.562 INFO: v_node_feats: torch.Size([20, 256])
2024-10-31 23:44:29.564 INFO: k_node_feats: torch.Size([20, 256])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.02938, max. 0.02938
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.14610, max. 0.16074
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.22729, max. 0.28049
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01124, max. 0.00996
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00023, max. 0.00029
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04180, max. 0.03605
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
2024-10-31 23:44:29.699 INFO: long_range_embedding: torch.Size([20, 256])
2024-10-31 23:44:29.701 INFO: long_range_energy: torch.Size([20, 1]), torch.Size([1])
2024-10-31 23:44:29.701 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-10-31 23:44:29.702 INFO: total_energy: torch.Size([1])
2024-10-31 23:44:29.702 INFO: node_energy_contributions: torch.Size([20, 4])
2024-10-31 23:44:29.702 INFO: node_energy: torch.Size([20])
2024-10-31 23:44:29.909 INFO: Epoch 24: head: default, loss=  0.7118, MAE_E_per_atom=    48.3 meV, MAE_F=    65.2 meV / A
2024-10-31 23:44:29.979 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:30.040 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:30.043 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:30.046 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:30.049 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:30.052 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:30.055 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03090, max. 0.03090
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.14481, max. 0.15599
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.24468, max. 0.29489
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01248, max. 0.01186
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00025, max. 0.00030
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04090, max. 0.03565
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03068, max. 0.03068
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.15584, max. 0.17096
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.25005, max. 0.31344
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01214, max. 0.01103
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00026, max. 0.00032
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.08364, max. 0.05838
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:30.331 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:30.334 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:30.334 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:30.334 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:30.335 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:30.335 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:31.778 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:31.838 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:31.842 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:31.845 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:31.848 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:31.851 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:31.854 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03079, max. 0.03079
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.17193, max. 0.16337
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.26063, max. 0.31593
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01203, max. 0.01187
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00027, max. 0.00032
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.07458, max. 0.05138
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03079, max. 0.03079
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.16014, max. 0.17286
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.25064, max. 0.31667
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01189, max. 0.01095
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00026, max. 0.00032
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.08274, max. 0.05815
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:32.232 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:32.236 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:32.236 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:32.236 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:32.236 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:32.236 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:33.681 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:33.742 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:33.745 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:33.749 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:33.752 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:33.755 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:33.758 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03089, max. 0.03089
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.16667, max. 0.17590
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.25223, max. 0.31931
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01208, max. 0.01251
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00026, max. 0.00033
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04690, max. 0.03734
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03087, max. 0.03087
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.17577, max. 0.15982
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.25883, max. 0.31509
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01277, max. 0.01197
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00027, max. 0.00032
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04444, max. 0.03665
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:34.036 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:34.039 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:34.040 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:34.040 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:34.040 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:34.040 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:35.500 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:35.561 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:35.564 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:35.567 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:35.570 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:35.573 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:35.576 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03100, max. 0.03100
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.17333, max. 0.16888
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.26740, max. 0.32221
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01245, max. 0.01225
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00027, max. 0.00033
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.07789, max. 0.05546
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03100, max. 0.03100
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.17772, max. 0.16475
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.26622, max. 0.32168
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01280, max. 0.01205
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00027, max. 0.00033
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06192, max. 0.04203
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:35.853 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:35.856 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:35.857 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:35.857 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:35.857 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:35.857 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:37.298 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:37.359 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:37.362 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:37.365 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:37.368 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:37.371 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:37.374 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03110, max. 0.03110
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.17759, max. 0.16912
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.26955, max. 0.32492
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01332, max. 0.00957
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00028, max. 0.00033
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06515, max. 0.04705
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03110, max. 0.03110
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.16390, max. 0.16747
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.26579, max. 0.32489
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01244, max. 0.01191
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00027, max. 0.00033
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.08632, max. 0.06008
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:37.655 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:37.658 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:37.658 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:37.659 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:37.659 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:37.659 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:39.111 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:39.172 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:39.175 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:39.178 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:39.181 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:39.184 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:39.187 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03119, max. 0.03119
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.17946, max. 0.16575
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.27073, max. 0.32635
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01305, max. 0.01229
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00028, max. 0.00033
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06351, max. 0.04318
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03119, max. 0.03119
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.17554, max. 0.16559
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.26892, max. 0.32615
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01220, max. 0.01235
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00028, max. 0.00033
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.07858, max. 0.05428
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:39.464 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:39.467 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:39.468 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:39.468 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:39.468 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:39.468 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:41.005 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:41.065 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:41.069 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:41.072 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:41.075 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:41.078 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:41.081 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03129, max. 0.03129
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.17006, max. 0.17806
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.25864, max. 0.32914
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01249, max. 0.01307
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00026, max. 0.00034
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04959, max. 0.03769
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03129, max. 0.03129
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.16177, max. 0.17435
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.26289, max. 0.32909
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01270, max. 0.01164
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00027, max. 0.00034
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.09072, max. 0.06356
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:41.359 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:41.362 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:41.363 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:41.363 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:41.363 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:41.363 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:42.807 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:42.868 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:42.872 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:42.875 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:42.878 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:42.881 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:42.884 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03136, max. 0.03136
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.16579, max. 0.16882
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.27072, max. 0.33047
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01278, max. 0.01219
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00028, max. 0.00034
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.08946, max. 0.06237
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03136, max. 0.03136
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.16603, max. 0.17614
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.25976, max. 0.33051
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01236, max. 0.01145
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00027, max. 0.00034
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.08947, max. 0.06310
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:43.160 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:43.163 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:43.164 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:43.164 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:43.164 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:43.164 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:44.606 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:44.666 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:44.670 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:44.673 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:44.676 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:44.679 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:44.682 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03165, max. 0.03165
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.14818, max. 0.16033
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.25619, max. 0.31124
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01348, max. 0.01284
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00026, max. 0.00032
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04214, max. 0.03602
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03143, max. 0.03143
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.18016, max. 0.17083
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.27681, max. 0.33123
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01358, max. 0.00991
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00028, max. 0.00034
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06834, max. 0.04943
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:44.960 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:44.963 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:44.963 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:44.963 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:44.964 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:44.964 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:46.415 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:46.475 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:46.479 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:46.482 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:46.485 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:46.488 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:46.491 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03148, max. 0.03148
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.18095, max. 0.16319
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.27345, max. 0.32856
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01349, max. 0.01278
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00028, max. 0.00034
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04833, max. 0.03679
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03151, max. 0.03151
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.17771, max. 0.17186
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.27940, max. 0.33334
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01291, max. 0.01297
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00029, max. 0.00034
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.08388, max. 0.05990
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:46.771 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:46.774 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:46.775 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:46.775 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:46.775 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:46.775 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:48.237 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:48.298 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:48.302 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:48.305 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:48.308 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:48.311 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:48.314 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03158, max. 0.03158
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.18192, max. 0.16385
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.27616, max. 0.33102
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01362, max. 0.01291
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00028, max. 0.00034
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04895, max. 0.03684
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03160, max. 0.03160
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.17870, max. 0.17241
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.28209, max. 0.33588
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01305, max. 0.01311
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00029, max. 0.00034
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.08502, max. 0.06076
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:48.594 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:48.597 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:48.597 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:48.597 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:48.598 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:48.598 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:50.129 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:50.189 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:50.193 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:50.196 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:50.199 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:50.202 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:50.205 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03170, max. 0.03170
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.17392, max. 0.18046
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.26643, max. 0.33882
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01302, max. 0.01370
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00027, max. 0.00035
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05289, max. 0.03898
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03170, max. 0.03170
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.16980, max. 0.17816
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.26799, max. 0.33882
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01270, max. 0.01180
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00027, max. 0.00035
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.09409, max. 0.06653
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:50.485 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:50.489 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:50.489 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:50.489 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:50.489 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:50.490 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:51.929 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:51.990 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:51.993 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:51.996 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:51.999 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:52.002 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:52.005 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03178, max. 0.03178
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.18389, max. 0.17271
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.28714, max. 0.34096
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01396, max. 0.01042
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00029, max. 0.00035
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.07214, max. 0.05232
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03178, max. 0.03178
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.18076, max. 0.16892
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.28218, max. 0.34018
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01290, max. 0.01310
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00029, max. 0.00035
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.08536, max. 0.05927
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:52.284 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:52.287 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:52.287 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:52.288 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:52.288 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:52.288 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:53.731 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:53.792 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:53.795 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:53.798 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:53.801 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:53.804 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:53.807 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03185, max. 0.03185
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.16998, max. 0.17140
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.28226, max. 0.34282
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01352, max. 0.01277
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00029, max. 0.00035
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.09628, max. 0.06742
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03185, max. 0.03185
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.16781, max. 0.17751
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.27745, max. 0.34280
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01330, max. 0.01223
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00028, max. 0.00035
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.09877, max. 0.06952
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:54.085 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:54.088 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:54.089 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:54.089 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:54.089 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:54.089 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:55.530 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:55.592 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:55.595 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:55.599 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:55.602 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:55.605 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:55.608 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03192, max. 0.03192
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.18622, max. 0.16960
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.29004, max. 0.34349
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01411, max. 0.01328
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00030, max. 0.00035
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.07047, max. 0.04827
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03215, max. 0.03215
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.15095, max. 0.16362
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.26558, max. 0.32320
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01424, max. 0.01357
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00027, max. 0.00033
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04312, max. 0.03625
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:55.887 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:55.890 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:55.890 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:55.891 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:55.891 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:55.891 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:57.466 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:57.524 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:57.527 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:57.529 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:57.531 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:57.534 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:57.536 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03070, max. 0.03070
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.17306, max. 0.17860
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.25894, max. 0.31235
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01288, max. 0.01089
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00026, max. 0.00032
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04242, max. 0.03705
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03070, max. 0.03070
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.16439, max. 0.17664
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.24822, max. 0.31235
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01345, max. 0.01097
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00025, max. 0.00032
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04261, max. 0.03701
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:57.810 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:57.812 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:57.813 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:57.813 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:57.813 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:57.813 INFO: node_energy: torch.Size([40])
2024-10-31 23:44:58.205 INFO: node_feats: torch.Size([20, 2304])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:44:58.258 INFO: node_feats: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:44:58.260 INFO: node_feats_out:torch.Size([20, 256]), torch.Size([2560]), 20
2024-10-31 23:44:58.263 INFO: linear_node_feats: torch.Size([20, 256])
2024-10-31 23:44:58.265 INFO: q_node_feats: torch.Size([20, 256])
2024-10-31 23:44:58.267 INFO: v_node_feats: torch.Size([20, 256])
2024-10-31 23:44:58.270 INFO: k_node_feats: torch.Size([20, 256])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03070, max. 0.03070
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.15755, max. 0.16798
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.24822, max. 0.31235
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01234, max. 0.01132
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00025, max. 0.00032
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04140, max. 0.03706
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
2024-10-31 23:44:58.404 INFO: long_range_embedding: torch.Size([20, 256])
2024-10-31 23:44:58.407 INFO: long_range_energy: torch.Size([20, 1]), torch.Size([1])
2024-10-31 23:44:58.407 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-10-31 23:44:58.407 INFO: total_energy: torch.Size([1])
2024-10-31 23:44:58.407 INFO: node_energy_contributions: torch.Size([20, 4])
2024-10-31 23:44:58.408 INFO: node_energy: torch.Size([20])
2024-10-31 23:44:58.615 INFO: Epoch 27: head: default, loss=  0.7819, MAE_E_per_atom=    47.4 meV, MAE_F=    67.3 meV / A
2024-10-31 23:44:58.686 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:58.746 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:44:58.749 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:44:58.752 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:44:58.755 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:44:58.758 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:44:58.762 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03203, max. 0.03203
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.18291, max. 0.17018
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.28786, max. 0.34689
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01320, max. 0.01343
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00029, max. 0.00035
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.08833, max. 0.06144
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03203, max. 0.03203
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.18749, max. 0.17015
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.29331, max. 0.34696
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01427, max. 0.01344
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00030, max. 0.00035
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.07136, max. 0.04891
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:44:59.037 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:44:59.040 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:44:59.040 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:44:59.041 INFO: total_energy: torch.Size([2])
2024-10-31 23:44:59.041 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:44:59.041 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:00.480 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:00.541 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:00.544 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:00.547 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:00.550 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:00.553 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:00.556 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03212, max. 0.03212
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.18726, max. 0.16767
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.29144, max. 0.34613
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01437, max. 0.01369
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00030, max. 0.00035
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05252, max. 0.03748
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03215, max. 0.03215
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.18757, max. 0.17447
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.29731, max. 0.35151
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01436, max. 0.01095
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00030, max. 0.00036
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.07583, max. 0.05509
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:00.840 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:00.844 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:00.844 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:00.844 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:00.844 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:00.844 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:02.294 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:02.355 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:02.359 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:02.362 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:02.365 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:02.368 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:02.371 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03227, max. 0.03227
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.17951, max. 0.18342
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.28154, max. 0.35567
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01374, max. 0.01458
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00029, max. 0.00036
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05724, max. 0.04230
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03227, max. 0.03227
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.17402, max. 0.17343
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.29244, max. 0.35563
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01411, max. 0.01327
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00030, max. 0.00036
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.10115, max. 0.07095
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:02.651 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:02.655 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:02.655 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:02.655 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:02.655 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:02.655 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:04.096 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:04.157 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:04.160 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:04.163 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:04.166 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:04.170 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:04.173 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03260, max. 0.03260
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.15495, max. 0.16704
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.27433, max. 0.33703
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01489, max. 0.01423
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00028, max. 0.00034
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04394, max. 0.03725
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03238, max. 0.03238
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.18667, max. 0.17657
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.30348, max. 0.35894
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01419, max. 0.01429
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00031, max. 0.00037
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.09422, max. 0.06764
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:04.452 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:04.455 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:04.455 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:04.455 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:04.456 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:04.456 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:05.910 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:05.970 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:05.974 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:05.977 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:05.980 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:05.983 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:05.986 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03251, max. 0.03251
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.17410, max. 0.18089
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.29341, max. 0.36320
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01405, max. 0.01313
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00030, max. 0.00037
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.10675, max. 0.07538
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03251, max. 0.03251
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.17808, max. 0.18246
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.28753, max. 0.36326
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01354, max. 0.01254
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00029, max. 0.00037
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.10425, max. 0.07405
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:06.368 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:06.372 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:06.372 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:06.372 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:06.372 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:06.372 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:07.831 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:07.892 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:07.895 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:07.898 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:07.901 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:07.905 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:07.908 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03263, max. 0.03263
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.19403, max. 0.17314
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.31005, max. 0.36554
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01510, max. 0.01425
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00032, max. 0.00037
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.07611, max. 0.05228
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03264, max. 0.03264
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.18923, max. 0.17787
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.31017, max. 0.36638
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01455, max. 0.01466
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00032, max. 0.00038
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.09697, max. 0.06969
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:08.185 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:08.188 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:08.188 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:08.188 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:08.189 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:08.189 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:09.625 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:09.685 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:09.689 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:09.692 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:09.695 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:09.698 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:09.701 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03276, max. 0.03276
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.17638, max. 0.18209
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.29902, max. 0.36922
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01430, max. 0.01346
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00031, max. 0.00038
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.10966, max. 0.07750
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03274, max. 0.03274
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.19323, max. 0.17172
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.30744, max. 0.36341
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01516, max. 0.01452
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00032, max. 0.00037
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05600, max. 0.03876
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:09.980 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:09.983 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:09.983 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:09.983 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:09.984 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:09.984 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:11.434 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:11.495 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:11.498 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:11.502 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:11.505 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:11.508 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:11.511 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03290, max. 0.03290
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.19130, max. 0.17455
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.30757, max. 0.37153
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01420, max. 0.01452
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00032, max. 0.00038
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.09688, max. 0.06762
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03312, max. 0.03312
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.15938, max. 0.17019
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.28248, max. 0.34951
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01562, max. 0.01494
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00029, max. 0.00036
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04481, max. 0.03823
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:11.789 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:11.792 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:11.792 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:11.792 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:11.792 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:11.793 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:13.250 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:13.311 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:13.314 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:13.317 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:13.320 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:13.324 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:13.327 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03306, max. 0.03306
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.18705, max. 0.18736
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.30125, max. 0.37678
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01470, max. 0.01575
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00031, max. 0.00039
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06258, max. 0.04636
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03306, max. 0.03306
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.18334, max. 0.18530
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.29996, max. 0.37678
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01404, max. 0.01305
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00031, max. 0.00039
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.11045, max. 0.07861
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:13.605 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:13.608 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:13.608 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:13.608 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:13.608 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:13.609 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:15.160 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:15.221 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:15.224 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:15.228 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:15.231 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:15.234 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:15.237 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03319, max. 0.03319
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.19814, max. 0.17932
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.32571, max. 0.38000
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01539, max. 0.01246
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00033, max. 0.00039
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.08522, max. 0.06215
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03319, max. 0.03319
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.18222, max. 0.17775
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.31483, max. 0.37996
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01539, max. 0.01433
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00032, max. 0.00039
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.11155, max. 0.07858
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:15.517 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:15.520 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:15.520 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:15.520 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:15.521 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:15.521 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:16.975 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:17.035 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:17.039 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:17.042 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:17.045 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:17.048 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:17.051 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03331, max. 0.03331
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.19914, max. 0.17988
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.32881, max. 0.38240
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01549, max. 0.01264
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00034, max. 0.00039
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.08640, max. 0.06304
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03331, max. 0.03331
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.18311, max. 0.17832
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.31775, max. 0.38236
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01556, max. 0.01446
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00033, max. 0.00039
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.11305, max. 0.07968
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:17.329 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:17.333 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:17.333 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:17.333 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:17.333 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:17.333 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:18.763 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:18.823 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:18.827 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:18.830 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:18.833 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:18.836 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:18.839 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03341, max. 0.03341
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.19003, max. 0.18917
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.31009, max. 0.38412
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01517, max. 0.01628
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00032, max. 0.00039
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06522, max. 0.04838
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03341, max. 0.03341
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.18693, max. 0.18716
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.30880, max. 0.38412
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01433, max. 0.01344
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00032, max. 0.00039
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.11503, max. 0.08202
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:19.122 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:19.125 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:19.125 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:19.126 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:19.126 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:19.126 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:20.564 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:20.625 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:20.629 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:20.632 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:20.635 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:20.638 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:20.641 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03349, max. 0.03349
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.19674, max. 0.18229
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.33128, max. 0.38507
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01578, max. 0.01590
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00034, max. 0.00039
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.10679, max. 0.07703
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03348, max. 0.03348
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20170, max. 0.17728
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.33156, max. 0.38408
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01634, max. 0.01537
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00034, max. 0.00039
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.08355, max. 0.05767
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:20.917 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:20.921 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:20.921 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:20.921 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:20.921 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:20.921 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:22.367 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:22.427 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:22.430 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:22.433 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:22.436 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:22.439 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:22.442 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03378, max. 0.03378
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.16537, max. 0.17347
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.29288, max. 0.36237
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01665, max. 0.01587
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00030, max. 0.00037
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04596, max. 0.03973
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03356, max. 0.03356
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.19644, max. 0.17790
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.32361, max. 0.38551
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01505, max. 0.01535
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00033, max. 0.00040
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.10464, max. 0.07333
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:22.720 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:22.723 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:22.724 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:22.724 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:22.724 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:22.724 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:24.279 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:24.341 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:24.344 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:24.347 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:24.350 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:24.353 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:24.356 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03363, max. 0.03363
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20074, max. 0.17646
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.33019, max. 0.38321
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01644, max. 0.01577
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00034, max. 0.00039
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06208, max. 0.04270
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03366, max. 0.03366
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.18527, max. 0.18671
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.32173, max. 0.38973
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01518, max. 0.01471
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00033, max. 0.00040
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.12182, max. 0.08650
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:24.635 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:24.639 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:24.639 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:24.639 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:24.639 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:24.639 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:26.095 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:26.152 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:26.155 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:26.158 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:26.160 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:26.162 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:26.165 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03210, max. 0.03209
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.18610, max. 0.18632
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.29527, max. 0.34797
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01431, max. 0.01207
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00030, max. 0.00036
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04407, max. 0.03921
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03210, max. 0.03209
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.17650, max. 0.18446
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.27233, max. 0.34797
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01472, max. 0.01290
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00028, max. 0.00036
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04400, max. 0.03907
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:26.436 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:26.439 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:26.439 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:26.439 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:26.440 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:26.440 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:26.829 INFO: node_feats: torch.Size([20, 2304])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:45:26.882 INFO: node_feats: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:45:26.884 INFO: node_feats_out:torch.Size([20, 256]), torch.Size([2560]), 20
2024-10-31 23:45:26.887 INFO: linear_node_feats: torch.Size([20, 256])
2024-10-31 23:45:26.889 INFO: q_node_feats: torch.Size([20, 256])
2024-10-31 23:45:26.891 INFO: v_node_feats: torch.Size([20, 256])
2024-10-31 23:45:26.894 INFO: k_node_feats: torch.Size([20, 256])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03210, max. 0.03210
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.17025, max. 0.17537
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.27174, max. 0.34797
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01411, max. 0.01282
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00028, max. 0.00036
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04375, max. 0.04349
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
2024-10-31 23:45:27.028 INFO: long_range_embedding: torch.Size([20, 256])
2024-10-31 23:45:27.031 INFO: long_range_energy: torch.Size([20, 1]), torch.Size([1])
2024-10-31 23:45:27.031 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-10-31 23:45:27.031 INFO: total_energy: torch.Size([1])
2024-10-31 23:45:27.031 INFO: node_energy_contributions: torch.Size([20, 4])
2024-10-31 23:45:27.032 INFO: node_energy: torch.Size([20])
2024-10-31 23:45:27.239 INFO: Epoch 30: head: default, loss=  0.8828, MAE_E_per_atom=    46.4 meV, MAE_F=    70.3 meV / A
2024-10-31 23:45:27.309 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:27.369 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:27.373 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:27.376 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:27.379 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:27.382 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:27.385 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03376, max. 0.03376
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.18642, max. 0.18731
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.32500, max. 0.39370
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01532, max. 0.01489
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00033, max. 0.00040
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.12344, max. 0.08771
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03374, max. 0.03374
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20205, max. 0.17736
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.33394, max. 0.38708
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01661, max. 0.01595
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00034, max. 0.00040
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06291, max. 0.04332
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:27.662 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:27.665 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:27.665 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:27.666 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:27.666 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:27.666 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:29.105 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:29.165 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:29.168 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:29.171 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:29.174 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:29.177 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:29.180 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03386, max. 0.03386
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.19997, max. 0.17959
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.33303, max. 0.39643
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01548, max. 0.01581
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00034, max. 0.00041
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.10845, max. 0.07615
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03386, max. 0.03386
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.19429, max. 0.19158
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.32318, max. 0.39728
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01583, max. 0.01705
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00033, max. 0.00041
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06929, max. 0.05150
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:29.461 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:29.464 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:29.465 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:29.465 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:29.465 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:29.465 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:30.911 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:30.971 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:30.975 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:30.978 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:30.981 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:30.984 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:30.987 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03394, max. 0.03394
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.18849, max. 0.18144
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.33550, max. 0.40042
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01664, max. 0.01527
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00034, max. 0.00041
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.12241, max. 0.08662
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03394, max. 0.03394
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.19281, max. 0.19008
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.32392, max. 0.40046
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01490, max. 0.01405
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00033, max. 0.00041
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.12317, max. 0.08811
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:31.266 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:31.269 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:31.269 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:31.269 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:31.269 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:31.270 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:32.812 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:32.873 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:32.876 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:32.879 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:32.882 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:32.885 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:32.889 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03402, max. 0.03402
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20608, max. 0.18363
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.35060, max. 0.40293
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01625, max. 0.01389
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00036, max. 0.00041
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.09461, max. 0.06925
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03401, max. 0.03401
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20768, max. 0.18001
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.34884, max. 0.40181
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01725, max. 0.01621
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00036, max. 0.00041
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.08891, max. 0.06161
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:33.166 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:33.169 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:33.169 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:33.169 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:33.169 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:33.169 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:34.615 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:34.675 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:34.678 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:34.681 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:34.685 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:34.688 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:34.691 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03409, max. 0.03409
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20332, max. 0.18562
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.35015, max. 0.40550
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01683, max. 0.01695
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00036, max. 0.00042
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.11508, max. 0.08324
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03431, max. 0.03431
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.17041, max. 0.17770
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.30557, max. 0.38027
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01758, max. 0.01676
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00031, max. 0.00039
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04700, max. 0.04116
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:34.970 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:34.973 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:34.973 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:34.974 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:34.974 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:34.974 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:36.446 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:36.507 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:36.511 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:36.514 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:36.517 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:36.520 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:36.523 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03418, max. 0.03418
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.19090, max. 0.18258
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.34220, max. 0.40915
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01704, max. 0.01560
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00035, max. 0.00042
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.12567, max. 0.08901
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03416, max. 0.03416
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20686, max. 0.18074
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.34778, max. 0.40206
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01725, max. 0.01664
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00036, max. 0.00041
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06566, max. 0.04528
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:36.801 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:36.804 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:36.804 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:36.804 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:36.805 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:36.805 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:38.258 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:38.319 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:38.323 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:38.326 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:38.329 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:38.332 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:38.335 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03429, max. 0.03429
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.19909, max. 0.19378
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.33625, max. 0.41311
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01645, max. 0.01782
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00034, max. 0.00042
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.07287, max. 0.05423
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03429, max. 0.03428
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.19643, max. 0.19192
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.33294, max. 0.41311
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01538, max. 0.01454
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00034, max. 0.00042
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.12790, max. 0.09160
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:38.617 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:38.621 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:38.621 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:38.621 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:38.621 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:38.622 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:40.063 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:40.125 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:40.128 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:40.132 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:40.135 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:40.138 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:40.141 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03437, max. 0.03437
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21189, max. 0.18178
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.36076, max. 0.41480
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01784, max. 0.01678
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00037, max. 0.00043
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.09219, max. 0.06398
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03437, max. 0.03437
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21018, max. 0.18553
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.36225, max. 0.41599
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01670, max. 0.01459
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00037, max. 0.00043
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.09839, max. 0.07212
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:40.419 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:40.423 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:40.423 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:40.423 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:40.423 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:40.423 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:41.970 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:42.031 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:42.034 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:42.037 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:42.040 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:42.043 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:42.047 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03446, max. 0.03446
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20636, max. 0.18266
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.34999, max. 0.41790
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01630, max. 0.01670
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00036, max. 0.00043
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.11556, max. 0.08135
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03468, max. 0.03468
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.17377, max. 0.18049
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.31404, max. 0.39269
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01822, max. 0.01738
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00032, max. 0.00040
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04767, max. 0.04206
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:42.324 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:42.327 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:42.328 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:42.328 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:42.328 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:42.328 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:43.783 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:43.844 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:43.847 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:43.850 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:43.853 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:43.856 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:43.859 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03457, max. 0.03457
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.19449, max. 0.19148
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.34730, max. 0.42274
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01639, max. 0.01617
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00036, max. 0.00043
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13479, max. 0.09611
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03457, max. 0.03457
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20876, max. 0.18816
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.36520, max. 0.42282
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01767, max. 0.01778
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00038, max. 0.00043
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.12123, max. 0.08786
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:44.138 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:44.141 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:44.141 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:44.142 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:44.142 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:44.142 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:45.593 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:45.653 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:45.656 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:45.659 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:45.662 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:45.665 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:45.671 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03469, max. 0.03469
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20979, max. 0.18871
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.36812, max. 0.42572
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01784, max. 0.01795
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00038, max. 0.00044
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.12249, max. 0.08879
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03469, max. 0.03469
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20302, max. 0.19574
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.34736, max. 0.42572
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01702, max. 0.01849
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00036, max. 0.00044
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.07601, max. 0.05664
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:45.951 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:45.954 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:45.954 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:45.954 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:45.954 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:45.955 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:47.383 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:47.444 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:47.447 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:47.450 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:47.453 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:47.456 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:47.459 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03479, max. 0.03479
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.19631, max. 0.19240
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.35215, max. 0.42760
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01665, max. 0.01645
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00036, max. 0.00044
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13740, max. 0.09805
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03474, max. 0.03474
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21265, max. 0.18462
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.36487, max. 0.42000
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01816, max. 0.01754
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00038, max. 0.00043
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06952, max. 0.04806
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:47.742 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:47.745 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:47.745 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:47.746 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:47.746 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:47.746 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:49.197 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:49.359 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:49.364 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:49.367 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:49.370 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:49.373 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:49.377 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03488, max. 0.03488
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21659, max. 0.18409
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.37490, max. 0.42853
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01866, max. 0.01750
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00038, max. 0.00044
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.09676, max. 0.06732
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03491, max. 0.03491
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21483, max. 0.18814
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.37638, max. 0.42981
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01718, max. 0.01551
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00039, max. 0.00044
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.10360, max. 0.07605
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:49.658 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:49.661 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:49.662 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:49.662 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:49.662 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:49.662 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:51.116 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:51.177 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:51.180 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:51.184 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:51.187 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:51.190 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:51.193 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03519, max. 0.03519
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.17811, max. 0.18326
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.32304, max. 0.40486
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01907, max. 0.01814
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00033, max. 0.00042
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04854, max. 0.04315
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03503, max. 0.03503
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.19769, max. 0.18615
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.36294, max. 0.43193
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01835, max. 0.01659
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00037, max. 0.00044
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13594, max. 0.09660
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:51.471 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:51.475 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:51.475 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:51.475 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:51.475 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:51.475 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:52.927 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:52.988 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:52.991 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:52.994 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:52.997 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:53.000 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:53.003 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03518, max. 0.03518
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20432, max. 0.19593
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.35350, max. 0.43528
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01664, max. 0.01605
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00036, max. 0.00045
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13858, max. 0.09957
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03518, max. 0.03518
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21186, max. 0.18569
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.36616, max. 0.43438
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01715, max. 0.01754
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00038, max. 0.00045
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.12268, max. 0.08659
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:53.282 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:53.285 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:53.285 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:53.286 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:53.286 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:53.286 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:54.742 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:54.799 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:54.802 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:54.805 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:54.807 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:54.809 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:54.812 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03344, max. 0.03344
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.19925, max. 0.19367
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.33368, max. 0.38573
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01636, max. 0.01344
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00034, max. 0.00040
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04667, max. 0.04268
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03344, max. 0.03344
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.19062, max. 0.19191
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.30785, max. 0.38573
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01606, max. 0.01494
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00032, max. 0.00040
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04662, max. 0.04253
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:55.084 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:55.087 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:55.087 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:55.087 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:55.088 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:55.088 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:55.474 INFO: node_feats: torch.Size([20, 2304])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:45:55.527 INFO: node_feats: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:45:55.529 INFO: node_feats_out:torch.Size([20, 256]), torch.Size([2560]), 20
2024-10-31 23:45:55.532 INFO: linear_node_feats: torch.Size([20, 256])
2024-10-31 23:45:55.534 INFO: q_node_feats: torch.Size([20, 256])
2024-10-31 23:45:55.537 INFO: v_node_feats: torch.Size([20, 256])
2024-10-31 23:45:55.539 INFO: k_node_feats: torch.Size([20, 256])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03344, max. 0.03344
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.18368, max. 0.18240
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.29729, max. 0.38573
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01600, max. 0.01437
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00030, max. 0.00040
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04635, max. 0.05084
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
2024-10-31 23:45:55.675 INFO: long_range_embedding: torch.Size([20, 256])
2024-10-31 23:45:55.678 INFO: long_range_energy: torch.Size([20, 1]), torch.Size([1])
2024-10-31 23:45:55.678 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-10-31 23:45:55.678 INFO: total_energy: torch.Size([1])
2024-10-31 23:45:55.678 INFO: node_energy_contributions: torch.Size([20, 4])
2024-10-31 23:45:55.679 INFO: node_energy: torch.Size([20])
2024-10-31 23:45:55.889 INFO: Epoch 33: head: default, loss=  0.9970, MAE_E_per_atom=    45.4 meV, MAE_F=    73.1 meV / A
2024-10-31 23:45:55.969 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:56.029 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:56.033 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:56.036 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:56.039 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:56.042 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:56.045 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03546, max. 0.03546
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.18010, max. 0.18464
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.32698, max. 0.41039
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01946, max. 0.01849
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00034, max. 0.00042
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04896, max. 0.04358
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03532, max. 0.03532
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20750, max. 0.19822
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.36088, max. 0.43798
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01776, max. 0.01931
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00037, max. 0.00045
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.07995, max. 0.05965
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:56.320 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:56.323 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:56.323 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:56.324 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:56.324 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:56.324 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:57.765 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:57.825 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:57.828 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:57.831 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:57.834 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:57.838 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:57.840 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03547, max. 0.03547
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20659, max. 0.19714
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.35964, max. 0.44181
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01701, max. 0.01651
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00037, max. 0.00045
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.14152, max. 0.10177
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03547, max. 0.03547
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21424, max. 0.18686
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.37266, max. 0.44090
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01748, max. 0.01786
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00038, max. 0.00045
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.12520, max. 0.08845
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:45:58.240 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:45:58.244 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:45:58.244 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:45:58.245 INFO: total_energy: torch.Size([2])
2024-10-31 23:45:58.245 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:45:58.245 INFO: node_energy: torch.Size([40])
2024-10-31 23:45:59.692 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:59.753 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:45:59.757 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:45:59.760 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:45:59.763 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:45:59.766 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:45:59.769 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03561, max. 0.03561
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21687, max. 0.19276
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.38877, max. 0.44500
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01910, max. 0.01914
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00040, max. 0.00046
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13150, max. 0.09557
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03558, max. 0.03558
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22239, max. 0.18679
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.39178, max. 0.44360
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01960, max. 0.01831
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00040, max. 0.00046
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.10170, max. 0.07092
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:00.048 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:00.051 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:00.051 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:00.051 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:00.052 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:00.052 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:01.508 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:01.570 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:01.573 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:01.577 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:01.580 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:01.583 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:01.586 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03576, max. 0.03576
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20318, max. 0.19083
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.37883, max. 0.44810
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01931, max. 0.01733
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00039, max. 0.00046
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.14301, max. 0.10184
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03575, max. 0.03575
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22169, max. 0.19170
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.39663, max. 0.44814
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01787, max. 0.01679
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00041, max. 0.00046
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.11031, max. 0.08113
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:01.864 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:01.867 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:01.867 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:01.868 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:01.868 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:01.868 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:03.315 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:03.375 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:03.379 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:03.382 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:03.385 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:03.388 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:03.391 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03577, max. 0.03577
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22082, max. 0.18968
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.38922, max. 0.44220
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01950, max. 0.01894
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00040, max. 0.00045
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.07503, max. 0.05201
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03588, max. 0.03588
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20444, max. 0.19669
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.37493, max. 0.45054
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01785, max. 0.01773
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00038, max. 0.00046
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.14870, max. 0.10643
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:03.671 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:03.675 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:03.675 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:03.675 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:03.675 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:03.675 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:05.141 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:05.201 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:05.205 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:05.208 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:05.211 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:05.214 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:05.217 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03602, max. 0.03602
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22362, max. 0.19276
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.40251, max. 0.45343
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01808, max. 0.01720
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00041, max. 0.00047
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.11236, max. 0.08266
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03599, max. 0.03599
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22551, max. 0.18830
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.40100, max. 0.45195
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02013, max. 0.01879
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00041, max. 0.00046
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.10442, max. 0.07286
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:05.494 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:05.498 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:05.498 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:05.498 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:05.498 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:05.498 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:07.046 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:07.106 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:07.110 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:07.113 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:07.116 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:07.119 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:07.122 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03630, max. 0.03630
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.18566, max. 0.18858
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.34024, max. 0.42740
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02058, max. 0.01988
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00035, max. 0.00044
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05011, max. 0.04492
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03615, max. 0.03615
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21174, max. 0.19987
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.37305, max. 0.45641
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01786, max. 0.01759
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00038, max. 0.00047
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.14833, max. 0.10680
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:07.402 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:07.405 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:07.406 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:07.406 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:07.406 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:07.406 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:08.862 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:08.923 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:08.927 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:08.930 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:08.933 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:08.936 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:08.939 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03631, max. 0.03631
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22222, max. 0.19845
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.40416, max. 0.46051
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02002, max. 0.02005
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00042, max. 0.00047
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13807, max. 0.10047
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03631, max. 0.03631
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20703, max. 0.19496
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.39028, max. 0.46047
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02004, max. 0.01797
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00040, max. 0.00047
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.14852, max. 0.10588
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:09.218 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:09.222 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:09.222 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:09.222 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:09.222 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:09.222 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:10.670 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:10.731 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:10.734 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:10.737 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:10.740 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:10.743 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:10.746 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03634, max. 0.03634
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22519, max. 0.19241
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.40234, max. 0.45494
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02020, max. 0.01994
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00041, max. 0.00047
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.07787, max. 0.05404
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03645, max. 0.03645
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20857, max. 0.19886
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.38668, max. 0.46370
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01846, max. 0.01842
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00040, max. 0.00048
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.15469, max. 0.11085
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:11.026 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:11.029 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:11.029 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:11.030 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:11.030 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:11.030 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:12.478 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:12.538 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:12.541 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:12.544 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:12.547 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:12.550 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:12.553 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03661, max. 0.03661
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22289, max. 0.19458
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.39826, max. 0.46638
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01889, max. 0.01914
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00041, max. 0.00048
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13505, max. 0.09563
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03661, max. 0.03661
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21761, max. 0.20350
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.38996, max. 0.46733
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01924, max. 0.02104
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00040, max. 0.00048
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.08783, max. 0.06566
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:12.834 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:12.838 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:12.838 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:12.838 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:12.838 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:12.838 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:14.296 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:14.356 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:14.360 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:14.363 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:14.366 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:14.369 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:14.372 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03673, max. 0.03673
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22563, max. 0.20213
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.41415, max. 0.47036
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02061, max. 0.02063
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00043, max. 0.00048
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.14227, max. 0.10361
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03670, max. 0.03670
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23145, max. 0.19353
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.41871, max. 0.46875
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02112, max. 0.01966
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00043, max. 0.00048
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.10952, max. 0.07656
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:14.650 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:14.653 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:14.653 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:14.653 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:14.654 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:14.654 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:16.187 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:16.248 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:16.251 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:16.254 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:16.257 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:16.260 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:16.263 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03701, max. 0.03701
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.19037, max. 0.19245
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.35801, max. 0.44315
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02158, max. 0.02120
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00037, max. 0.00046
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05108, max. 0.04611
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03686, max. 0.03686
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21100, max. 0.19920
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.40229, max. 0.47339
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02080, max. 0.01866
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00041, max. 0.00049
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.15430, max. 0.11015
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:16.554 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:16.558 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:16.558 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:16.558 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:16.558 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:16.558 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:18.015 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:18.076 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:18.080 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:18.083 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:18.086 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:18.089 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:18.092 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03701, max. 0.03701
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22614, max. 0.19783
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.40844, max. 0.47679
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01945, max. 0.01961
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00042, max. 0.00049
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13897, max. 0.09854
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03702, max. 0.03702
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22092, max. 0.20701
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.39985, max. 0.47776
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01974, max. 0.02162
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00041, max. 0.00049
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.09058, max. 0.06777
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:18.371 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:18.375 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:18.375 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:18.375 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:18.375 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:18.375 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:19.821 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:19.882 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:19.886 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:19.889 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:19.892 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:19.895 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:19.898 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03714, max. 0.03714
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21933, max. 0.20381
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.39449, max. 0.48145
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01920, max. 0.01936
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00040, max. 0.00050
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.15923, max. 0.11496
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03714, max. 0.03714
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21367, max. 0.20152
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.40227, max. 0.48137
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01925, max. 0.01931
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00041, max. 0.00050
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.16253, max. 0.11672
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:20.175 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:20.178 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:20.178 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:20.178 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:20.178 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:20.179 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:21.618 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:21.679 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:21.682 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:21.685 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:21.688 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:21.691 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:21.694 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03726, max. 0.03726
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23382, max. 0.20098
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.43391, max. 0.48391
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01918, max. 0.01936
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00045, max. 0.00050
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.12302, max. 0.09073
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03715, max. 0.03715
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23161, max. 0.19800
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.42238, max. 0.47443
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02126, max. 0.02151
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00044, max. 0.00049
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.08226, max. 0.05720
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:21.974 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:21.977 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:21.977 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:21.978 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:21.978 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:21.978 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:23.550 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:23.607 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:23.609 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:23.612 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:23.614 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:23.616 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:23.619 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03485, max. 0.03485
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21299, max. 0.20105
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.37481, max. 0.42522
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01862, max. 0.01584
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00039, max. 0.00044
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04924, max. 0.04607
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03485, max. 0.03485
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20485, max. 0.19939
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.34611, max. 0.42522
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01755, max. 0.01715
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00035, max. 0.00044
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04921, max. 0.04591
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:23.888 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:23.891 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:23.891 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:23.891 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:23.892 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:23.892 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:24.282 INFO: node_feats: torch.Size([20, 2304])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:46:24.334 INFO: node_feats: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:46:24.337 INFO: node_feats_out:torch.Size([20, 256]), torch.Size([2560]), 20
2024-10-31 23:46:24.339 INFO: linear_node_feats: torch.Size([20, 256])
2024-10-31 23:46:24.341 INFO: q_node_feats: torch.Size([20, 256])
2024-10-31 23:46:24.344 INFO: v_node_feats: torch.Size([20, 256])
2024-10-31 23:46:24.346 INFO: k_node_feats: torch.Size([20, 256])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03485, max. 0.03485
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.19806, max. 0.18944
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.32759, max. 0.42522
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01809, max. 0.01602
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00034, max. 0.00044
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.04890, max. 0.05846
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
2024-10-31 23:46:24.481 INFO: long_range_embedding: torch.Size([20, 256])
2024-10-31 23:46:24.483 INFO: long_range_energy: torch.Size([20, 1]), torch.Size([1])
2024-10-31 23:46:24.484 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-10-31 23:46:24.484 INFO: total_energy: torch.Size([1])
2024-10-31 23:46:24.484 INFO: node_energy_contributions: torch.Size([20, 4])
2024-10-31 23:46:24.484 INFO: node_energy: torch.Size([20])
2024-10-31 23:46:24.693 INFO: Epoch 36: head: default, loss=  1.1570, MAE_E_per_atom=    44.6 meV, MAE_F=    76.9 meV / A
2024-10-31 23:46:24.763 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:24.823 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:24.826 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:24.830 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:24.833 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:24.836 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:24.839 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03739, max. 0.03739
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22891, max. 0.20073
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.41772, max. 0.48602
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01999, max. 0.02004
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00043, max. 0.00050
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.14277, max. 0.10136
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03739, max. 0.03739
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21548, max. 0.20316
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.40783, max. 0.48692
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01956, max. 0.01971
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00042, max. 0.00050
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.16545, max. 0.11891
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:25.114 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:25.118 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:25.118 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:25.118 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:25.118 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:25.118 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:26.553 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:26.614 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:26.617 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:26.620 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:26.623 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:26.626 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:26.629 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03749, max. 0.03749
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23162, max. 0.20843
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.43255, max. 0.48894
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02174, max. 0.02165
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00045, max. 0.00050
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.15033, max. 0.10970
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03749, max. 0.03749
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22195, max. 0.20513
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.40209, max. 0.48894
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01970, max. 0.02001
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00041, max. 0.00050
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.16319, max. 0.11792
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:26.918 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:26.922 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:26.922 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:26.922 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:26.922 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:26.922 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:28.360 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:28.421 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:28.424 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:28.427 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:28.431 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:28.434 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:28.437 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03758, max. 0.03758
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21602, max. 0.20475
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.41860, max. 0.49010
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02187, max. 0.02001
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00043, max. 0.00050
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.16224, max. 0.11605
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03755, max. 0.03755
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23811, max. 0.19977
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.44002, max. 0.48836
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02238, max. 0.02086
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00045, max. 0.00050
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.11607, max. 0.08135
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:28.715 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:28.719 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:28.719 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:28.719 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:28.719 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:28.719 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:30.164 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:30.225 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:30.229 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:30.232 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:30.235 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:30.238 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:30.241 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03768, max. 0.03768
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22551, max. 0.21189
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.41465, max. 0.49151
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02056, max. 0.02251
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00043, max. 0.00051
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.09493, max. 0.07110
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03768, max. 0.03768
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23655, max. 0.20376
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.44325, max. 0.49151
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01951, max. 0.02010
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00046, max. 0.00051
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.12651, max. 0.09336
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:30.518 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:30.521 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:30.521 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:30.522 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:30.522 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:30.522 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:32.073 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:32.134 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:32.137 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:32.140 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:32.143 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:32.146 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:32.150 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03790, max. 0.03790
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.19608, max. 0.19825
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.38077, max. 0.46059
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02288, max. 0.02297
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00039, max. 0.00047
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05231, max. 0.04783
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03764, max. 0.03764
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23432, max. 0.20081
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.43193, max. 0.48254
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02189, max. 0.02242
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00045, max. 0.00050
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.08486, max. 0.05905
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:32.430 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:32.433 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:32.433 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:32.434 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:32.434 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:32.434 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:33.906 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:33.966 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:33.969 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:33.972 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:33.975 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:33.978 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:33.981 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03785, max. 0.03785
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22452, max. 0.20634
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.40922, max. 0.49566
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02020, max. 0.02067
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00042, max. 0.00051
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.16714, max. 0.12083
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03785, max. 0.03785
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23155, max. 0.20361
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.42740, max. 0.49466
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02062, max. 0.02055
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00044, max. 0.00051
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.14707, max. 0.10449
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:34.259 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:34.262 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:34.262 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:34.263 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:34.263 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:34.263 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:35.704 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:35.764 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:35.768 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:35.771 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:35.774 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:35.777 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:35.780 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03792, max. 0.03792
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24065, max. 0.20224
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.44871, max. 0.49651
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02294, max. 0.02158
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00046, max. 0.00051
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.11887, max. 0.08338
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03785, max. 0.03785
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23606, max. 0.20245
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.43741, max. 0.48830
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02218, max. 0.02288
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00045, max. 0.00050
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.08603, max. 0.05991
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:36.059 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:36.063 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:36.063 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:36.063 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:36.063 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:36.063 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:37.521 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:37.582 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:37.585 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:37.588 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:37.591 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:37.594 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:37.597 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03808, max. 0.03808
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22848, max. 0.21510
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.42408, max. 0.50208
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02107, max. 0.02306
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00044, max. 0.00052
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.09766, max. 0.07320
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03808, max. 0.03808
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21926, max. 0.20829
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.42926, max. 0.50205
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02260, max. 0.02097
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00044, max. 0.00052
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.16754, max. 0.11998
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:37.876 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:37.879 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:37.880 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:37.880 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:37.880 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:37.880 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:39.317 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:39.378 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:39.381 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:39.384 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:39.387 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:39.390 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:39.393 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03818, max. 0.03818
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22090, max. 0.20866
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.42420, max. 0.50489
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02047, max. 0.02123
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00044, max. 0.00052
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.17411, max. 0.12534
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03833, max. 0.03833
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.19880, max. 0.20166
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.39218, max. 0.47227
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02355, max. 0.02394
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00040, max. 0.00049
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05358, max. 0.04864
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:39.671 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:39.675 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:39.675 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:39.675 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:39.675 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:39.675 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:41.229 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:41.290 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:41.293 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:41.297 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:41.300 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:41.303 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:41.306 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03830, max. 0.03830
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24161, max. 0.20831
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.45953, max. 0.50875
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02027, max. 0.02135
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00047, max. 0.00052
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13198, max. 0.09755
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03830, max. 0.03830
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23765, max. 0.21500
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.45146, max. 0.50875
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02295, max. 0.02273
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00047, max. 0.00052
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.15851, max. 0.11587
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:41.585 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:41.589 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:41.589 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:41.589 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:41.589 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:41.590 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:43.039 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:43.099 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:43.103 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:43.106 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:43.109 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:43.112 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:43.115 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03841, max. 0.03841
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23615, max. 0.20804
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.44189, max. 0.51076
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02145, max. 0.02171
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00045, max. 0.00053
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.15222, max. 0.10835
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03830, max. 0.03830
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24003, max. 0.20622
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.44958, max. 0.50134
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02281, max. 0.02388
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00046, max. 0.00052
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.08836, max. 0.06163
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:43.392 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:43.396 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:43.396 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:43.396 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:43.396 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:43.396 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:44.838 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:44.898 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:44.902 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:44.905 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:44.908 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:44.911 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:44.914 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03853, max. 0.03853
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22939, max. 0.20986
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.42399, max. 0.51531
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02120, max. 0.02209
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00043, max. 0.00053
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.17455, max. 0.12645
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03854, max. 0.03854
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22278, max. 0.21194
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.43996, max. 0.51528
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02330, max. 0.02193
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00045, max. 0.00053
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.17236, max. 0.12362
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:45.193 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:45.197 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:45.197 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:45.197 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:45.197 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:45.197 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:46.636 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:46.697 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:46.700 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:46.703 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:46.707 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:46.710 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:46.713 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03866, max. 0.03866
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24040, max. 0.21791
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.45983, max. 0.51771
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02348, max. 0.02320
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00047, max. 0.00053
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.16186, max. 0.11840
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03866, max. 0.03866
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22412, max. 0.21226
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.43449, max. 0.51762
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02103, max. 0.02221
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00045, max. 0.00053
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.17912, max. 0.12910
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:46.989 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:46.992 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:46.992 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:46.993 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:46.993 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:46.993 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:48.436 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:48.496 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:48.499 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:48.502 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:48.505 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:48.508 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:48.511 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03893, max. 0.03892
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20231, max. 0.20587
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.40628, max. 0.48528
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02442, max. 0.02520
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00042, max. 0.00050
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05522, max. 0.04958
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03877, max. 0.03877
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24502, max. 0.21134
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.47029, max. 0.51909
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02080, max. 0.02224
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00048, max. 0.00054
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13549, max. 0.10021
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:48.789 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:48.792 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:48.792 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:48.792 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:48.793 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:48.793 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:50.344 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:50.404 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:50.408 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:50.411 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:50.414 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:50.417 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:50.420 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03890, max. 0.03890
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23463, max. 0.22165
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.44268, max. 0.52180
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02209, max. 0.02416
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00045, max. 0.00054
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.10259, max. 0.07700
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03887, max. 0.03887
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24832, max. 0.20947
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.47243, max. 0.51980
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02433, max. 0.02347
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00049, max. 0.00054
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.12512, max. 0.08796
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:50.699 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:50.702 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:50.702 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:50.702 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:50.703 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:50.703 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:52.146 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:52.203 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:52.206 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:52.208 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:52.210 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:52.213 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:52.215 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03652, max. 0.03651
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22624, max. 0.20761
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.41472, max. 0.46338
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02090, max. 0.01826
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00043, max. 0.00048
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05176, max. 0.04934
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03652, max. 0.03652
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21781, max. 0.20604
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.38338, max. 0.46338
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01993, max. 0.01931
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00039, max. 0.00048
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05174, max. 0.04918
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:52.486 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:52.489 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:52.489 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:52.489 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:52.489 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:52.490 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:52.880 INFO: node_feats: torch.Size([20, 2304])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:46:52.932 INFO: node_feats: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:46:52.935 INFO: node_feats_out:torch.Size([20, 256]), torch.Size([2560]), 20
2024-10-31 23:46:52.937 INFO: linear_node_feats: torch.Size([20, 256])
2024-10-31 23:46:52.940 INFO: q_node_feats: torch.Size([20, 256])
2024-10-31 23:46:52.942 INFO: v_node_feats: torch.Size([20, 256])
2024-10-31 23:46:52.944 INFO: k_node_feats: torch.Size([20, 256])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03652, max. 0.03652
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21153, max. 0.20245
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.36780, max. 0.46338
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02023, max. 0.01862
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00038, max. 0.00048
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05141, max. 0.06580
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
2024-10-31 23:46:53.080 INFO: long_range_embedding: torch.Size([20, 256])
2024-10-31 23:46:53.083 INFO: long_range_energy: torch.Size([20, 1]), torch.Size([1])
2024-10-31 23:46:53.083 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-10-31 23:46:53.083 INFO: total_energy: torch.Size([1])
2024-10-31 23:46:53.083 INFO: node_energy_contributions: torch.Size([20, 4])
2024-10-31 23:46:53.083 INFO: node_energy: torch.Size([20])
2024-10-31 23:46:53.291 INFO: Epoch 39: head: default, loss=  1.3136, MAE_E_per_atom=    44.0 meV, MAE_F=    80.2 meV / A
2024-10-31 23:46:53.361 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:53.421 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:53.424 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:53.427 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:53.430 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:53.433 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:53.436 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03902, max. 0.03902
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23537, max. 0.22241
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.44511, max. 0.52438
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02223, max. 0.02431
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00046, max. 0.00054
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.10329, max. 0.07755
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03902, max. 0.03902
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24272, max. 0.22039
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.46701, max. 0.52438
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02399, max. 0.02365
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00048, max. 0.00054
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.16492, max. 0.12070
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:53.714 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:53.717 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:53.717 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:53.717 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:53.717 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:53.718 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:55.144 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:55.204 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:55.208 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:55.211 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:55.214 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:55.217 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:55.220 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03910, max. 0.03910
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24053, max. 0.21253
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.45661, max. 0.52505
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02240, max. 0.02307
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00047, max. 0.00054
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.15787, max. 0.11251
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03910, max. 0.03910
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22598, max. 0.21549
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.45048, max. 0.52606
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02410, max. 0.02303
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00046, max. 0.00054
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.17752, max. 0.12743
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:55.499 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:55.502 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:55.502 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:55.503 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:55.503 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:55.503 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:56.946 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:57.006 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:57.010 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:57.013 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:57.016 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:57.019 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:57.022 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03908, max. 0.03908
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24495, max. 0.21079
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.46533, max. 0.51624
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02382, max. 0.02547
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00048, max. 0.00053
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.09191, max. 0.06418
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03919, max. 0.03919
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24743, max. 0.21391
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.47897, max. 0.52730
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02127, max. 0.02307
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00049, max. 0.00054
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13874, max. 0.10269
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:57.299 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:57.303 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:57.303 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:57.303 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:57.303 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:57.303 INFO: node_energy: torch.Size([40])
2024-10-31 23:46:58.848 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:58.909 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:46:58.913 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:46:58.916 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:46:58.919 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:46:58.922 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:46:58.925 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03926, max. 0.03926
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.25024, max. 0.21118
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.47988, max. 0.52703
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02491, max. 0.02426
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00049, max. 0.00054
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.12779, max. 0.08993
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03951, max. 0.03951
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20511, max. 0.20829
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.41786, max. 0.49431
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02519, max. 0.02632
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00043, max. 0.00051
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05682, max. 0.05046
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:46:59.201 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:46:59.205 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:46:59.205 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:46:59.205 INFO: total_energy: torch.Size([2])
2024-10-31 23:46:59.205 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:46:59.205 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:00.660 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:00.720 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:00.724 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:00.727 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:00.730 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:00.733 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:00.736 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03941, max. 0.03941
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22883, max. 0.21690
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.44833, max. 0.53252
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02190, max. 0.02372
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00046, max. 0.00055
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.18652, max. 0.13463
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03941, max. 0.03941
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23505, max. 0.21559
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.43984, max. 0.53261
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02241, max. 0.02382
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00045, max. 0.00055
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.18290, max. 0.13270
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:01.015 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:01.019 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:01.019 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:01.019 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:01.019 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:01.019 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:02.468 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:02.528 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:02.532 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:02.535 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:02.538 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:02.541 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:02.544 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03953, max. 0.03953
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22952, max. 0.21761
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.45034, max. 0.53470
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02203, max. 0.02395
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00046, max. 0.00055
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.18749, max. 0.13535
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03953, max. 0.03953
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24564, max. 0.22344
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.47693, max. 0.53479
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02475, max. 0.02427
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00049, max. 0.00055
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.16954, max. 0.12421
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:02.821 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:02.825 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:02.825 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:02.825 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:02.825 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:02.825 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:04.250 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:04.310 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:04.314 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:04.317 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:04.320 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:04.323 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:04.326 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03963, max. 0.03963
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24350, max. 0.21561
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.46700, max. 0.53488
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02312, max. 0.02414
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00048, max. 0.00055
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.16183, max. 0.11542
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03952, max. 0.03952
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24774, max. 0.21338
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.47428, max. 0.52452
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02438, max. 0.02641
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00049, max. 0.00054
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.09376, max. 0.06550
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:04.604 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:04.607 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:04.607 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:04.608 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:04.608 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:04.608 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:06.058 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:06.119 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:06.122 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:06.125 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:06.128 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:06.131 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:06.134 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03975, max. 0.03975
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22956, max. 0.21932
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.46170, max. 0.53781
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02500, max. 0.02434
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00048, max. 0.00056
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.18283, max. 0.13136
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03975, max. 0.03975
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23707, max. 0.21755
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.44481, max. 0.53785
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02283, max. 0.02449
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00046, max. 0.00056
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.18546, max. 0.13458
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:06.511 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:06.514 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:06.514 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:06.514 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:06.515 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:06.515 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:07.954 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:08.014 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:08.017 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:08.020 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:08.024 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:08.027 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:08.030 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04042, max. 0.04042
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20808, max. 0.21175
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.42978, max. 0.50279
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02596, max. 0.02753
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00044, max. 0.00052
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05795, max. 0.05132
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03987, max. 0.03987
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.25171, max. 0.21763
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49262, max. 0.53878
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02197, max. 0.02438
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00056
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.14298, max. 0.10590
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:08.306 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:08.310 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:08.310 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:08.310 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:08.310 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:08.310 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:09.755 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:09.815 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:09.818 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:09.821 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:09.824 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:09.827 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:09.830 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03996, max. 0.03996
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.25519, max. 0.21583
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49494, max. 0.53901
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02585, max. 0.02567
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00056
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13117, max. 0.09233
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03999, max. 0.03999
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24118, max. 0.22858
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.46349, max. 0.54119
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02337, max. 0.02551
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00048, max. 0.00056
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.10825, max. 0.08136
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:10.110 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:10.113 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:10.114 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:10.114 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:10.114 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:10.114 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:11.570 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:11.631 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:11.634 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:11.637 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:11.640 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:11.643 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:11.647 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04010, max. 0.04010
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23274, max. 0.22076
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.45893, max. 0.54348
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02261, max. 0.02508
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00047, max. 0.00056
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.19145, max. 0.13824
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04010, max. 0.04010
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.25348, max. 0.21903
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49803, max. 0.54358
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02222, max. 0.02487
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00056
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.14449, max. 0.10705
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:11.924 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:11.927 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:11.927 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:11.927 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:11.927 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:11.928 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:13.352 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:13.412 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:13.415 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:13.418 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:13.421 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:13.424 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:13.427 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04018, max. 0.04018
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.25653, max. 0.21707
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49939, max. 0.54297
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02614, max. 0.02611
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00056
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13226, max. 0.09313
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04019, max. 0.04019
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24245, max. 0.22992
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.46753, max. 0.54519
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02361, max. 0.02575
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00048, max. 0.00056
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.10927, max. 0.08216
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:13.707 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:13.711 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:13.711 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:13.711 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:13.711 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:13.711 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:15.253 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:15.314 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:15.318 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:15.321 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:15.324 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:15.327 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:15.330 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04026, max. 0.04026
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24746, max. 0.21927
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.47984, max. 0.54575
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02394, max. 0.02546
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00049, max. 0.00056
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.16584, max. 0.11832
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04026, max. 0.04026
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24005, max. 0.22049
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.45444, max. 0.54684
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02349, max. 0.02557
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00047, max. 0.00057
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.18933, max. 0.13747
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:15.608 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:15.611 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:15.612 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:15.612 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:15.612 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:15.612 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:17.056 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:17.116 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:17.119 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:17.122 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:17.125 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:17.128 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:17.131 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04034, max. 0.04034
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23280, max. 0.22254
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.47113, max. 0.54789
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02578, max. 0.02559
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00049, max. 0.00057
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.18709, max. 0.13449
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04058, max. 0.04058
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.25204, max. 0.21748
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48778, max. 0.53579
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02523, max. 0.02793
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00055
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.09606, max. 0.06711
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:17.409 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:17.412 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:17.412 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:17.412 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:17.413 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:17.413 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:18.863 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:18.923 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:18.927 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:18.930 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:18.933 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:18.936 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:18.939 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04132, max. 0.04132
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21077, max. 0.21471
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.44191, max. 0.51211
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02675, max. 0.02886
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00046, max. 0.00053
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05908, max. 0.05233
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04043, max. 0.04043
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.25097, max. 0.22931
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49339, max. 0.54949
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02596, max. 0.02557
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00057
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.17580, max. 0.12889
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:19.218 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:19.222 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:19.222 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:19.222 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:19.222 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:19.222 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:20.680 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:20.737 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:20.740 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:20.742 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:20.744 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:20.747 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:20.749 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03808, max. 0.03808
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23797, max. 0.21321
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.45097, max. 0.49755
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02308, max. 0.02062
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00046, max. 0.00051
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05393, max. 0.05231
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03808, max. 0.03808
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22910, max. 0.21172
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.41742, max. 0.49755
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02225, max. 0.02131
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00043, max. 0.00051
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05394, max. 0.05214
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:21.020 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:21.022 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:21.022 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:21.023 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:21.023 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:21.023 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:21.412 INFO: node_feats: torch.Size([20, 2304])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:47:21.464 INFO: node_feats: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:47:21.467 INFO: node_feats_out:torch.Size([20, 256]), torch.Size([2560]), 20
2024-10-31 23:47:21.469 INFO: linear_node_feats: torch.Size([20, 256])
2024-10-31 23:47:21.472 INFO: q_node_feats: torch.Size([20, 256])
2024-10-31 23:47:21.474 INFO: v_node_feats: torch.Size([20, 256])
2024-10-31 23:47:21.476 INFO: k_node_feats: torch.Size([20, 256])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03808, max. 0.03808
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22339, max. 0.21409
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.40439, max. 0.49755
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02229, max. 0.02155
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00042, max. 0.00051
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05358, max. 0.07223
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
2024-10-31 23:47:21.612 INFO: long_range_embedding: torch.Size([20, 256])
2024-10-31 23:47:21.614 INFO: long_range_energy: torch.Size([20, 1]), torch.Size([1])
2024-10-31 23:47:21.614 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-10-31 23:47:21.615 INFO: total_energy: torch.Size([1])
2024-10-31 23:47:21.615 INFO: node_energy_contributions: torch.Size([20, 4])
2024-10-31 23:47:21.615 INFO: node_energy: torch.Size([20])
2024-10-31 23:47:21.823 INFO: Epoch 42: head: default, loss=  1.4593, MAE_E_per_atom=    43.4 meV, MAE_F=    82.7 meV / A
2024-10-31 23:47:21.892 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:21.952 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:21.955 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:21.958 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:21.961 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:21.964 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:21.968 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04092, max. 0.04092
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.25339, max. 0.21873
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49203, max. 0.53993
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02548, max. 0.02841
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00056
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.09671, max. 0.06756
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04075, max. 0.04075
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.25855, max. 0.21893
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.50657, max. 0.54998
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02664, max. 0.02693
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00052, max. 0.00057
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13416, max. 0.09453
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:22.243 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:22.246 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:22.246 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:22.246 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:22.246 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:22.247 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:23.686 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:23.842 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:23.846 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:23.849 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:23.852 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:23.855 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:23.858 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04065, max. 0.04065
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24229, max. 0.22283
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.46300, max. 0.55531
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02403, max. 0.02645
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00048, max. 0.00057
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.19255, max. 0.13992
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04065, max. 0.04065
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23490, max. 0.22461
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.47713, max. 0.55528
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02622, max. 0.02631
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00049, max. 0.00057
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.18967, max. 0.13645
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:24.141 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:24.144 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:24.144 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:24.144 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:24.145 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:24.145 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:25.584 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:25.644 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:25.648 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:25.651 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:25.654 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:25.657 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:25.660 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04075, max. 0.04074
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23641, max. 0.22456
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.46997, max. 0.55726
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02333, max. 0.02658
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00048, max. 0.00058
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.19701, max. 0.14275
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04074, max. 0.04074
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.25064, max. 0.22226
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49019, max. 0.55626
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02461, max. 0.02658
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00058
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.16902, max. 0.12071
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:25.938 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:25.941 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:25.941 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:25.942 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:25.942 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:25.942 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:27.383 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:27.443 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:27.447 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:27.450 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:27.453 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:27.456 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:27.459 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04202, max. 0.04202
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21294, max. 0.21752
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.45114, max. 0.52043
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02734, max. 0.02984
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00047, max. 0.00054
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06002, max. 0.05290
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04084, max. 0.04084
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.25831, max. 0.22309
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.51376, max. 0.55880
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02296, max. 0.02651
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00053, max. 0.00058
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.14933, max. 0.11076
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:27.735 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:27.738 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:27.739 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:27.739 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:27.739 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:27.739 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:29.186 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:29.246 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:29.250 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:29.253 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:29.256 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:29.259 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:29.262 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04095, max. 0.04095
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.25499, max. 0.23345
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.50520, max. 0.56133
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02673, max. 0.02673
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00052, max. 0.00058
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.17983, max. 0.13197
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04095, max. 0.04095
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24750, max. 0.23519
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48294, max. 0.56133
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02453, max. 0.02668
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00058
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.11317, max. 0.08518
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:29.541 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:29.544 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:29.544 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:29.544 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:29.544 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:29.545 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:30.997 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:31.057 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:31.060 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:31.063 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:31.066 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:31.069 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:31.073 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04104, max. 0.04104
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24464, max. 0.22532
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.47154, max. 0.56306
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02455, max. 0.02733
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00049, max. 0.00058
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.19562, max. 0.14223
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04177, max. 0.04177
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.25709, max. 0.22210
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.50312, max. 0.55025
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02613, max. 0.02957
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00052, max. 0.00057
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.09860, max. 0.06894
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:31.442 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:31.445 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:31.446 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:31.446 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:31.446 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:31.446 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:32.889 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:32.949 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:32.952 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:32.955 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:32.958 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:32.961 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:32.964 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04113, max. 0.04113
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.26035, max. 0.22485
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.52043, max. 0.56493
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02331, max. 0.02717
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00054, max. 0.00058
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.15140, max. 0.11237
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04113, max. 0.04113
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.25611, max. 0.23455
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.50894, max. 0.56493
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02701, max. 0.02714
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00053, max. 0.00058
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.18140, max. 0.13318
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:33.244 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:33.247 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:33.247 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:33.247 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:33.247 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:33.248 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:34.685 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:34.745 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:34.749 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:34.752 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:34.755 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:34.758 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:34.761 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04121, max. 0.04121
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23916, max. 0.22748
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.47839, max. 0.56616
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02420, max. 0.02765
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00049, max. 0.00059
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.20107, max. 0.14607
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04121, max. 0.04121
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.25356, max. 0.22517
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.50001, max. 0.56513
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02528, max. 0.02765
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00052, max. 0.00059
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.17241, max. 0.12324
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:35.038 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:35.041 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:35.041 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:35.041 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:35.041 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:35.042 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:36.488 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:36.549 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:36.553 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:36.556 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:36.559 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:36.562 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:36.565 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04129, max. 0.04129
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24930, max. 0.23692
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48934, max. 0.56704
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02497, max. 0.02710
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00059
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.11504, max. 0.08663
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04202, max. 0.04202
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.26371, max. 0.22351
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.52318, max. 0.56461
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02776, max. 0.02864
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00054, max. 0.00058
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13837, max. 0.09763
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:36.841 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:36.845 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:36.845 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:36.845 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:36.845 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:36.845 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:38.289 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:38.349 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:38.353 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:38.356 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:38.359 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:38.362 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:38.365 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04285, max. 0.04285
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21566, max. 0.21978
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.46249, max. 0.52831
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02813, max. 0.03110
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00048, max. 0.00055
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06141, max. 0.05360
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04136, max. 0.04136
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23882, max. 0.22882
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48960, max. 0.56783
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02726, max. 0.02793
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00059
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.19545, max. 0.14077
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:38.644 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:38.647 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:38.647 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:38.647 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:38.648 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:38.648 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:40.212 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:40.272 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:40.276 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:40.279 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:40.282 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:40.285 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:40.288 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04145, max. 0.04145
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.25005, max. 0.23759
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49209, max. 0.56971
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02517, max. 0.02743
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00059
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.11586, max. 0.08728
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04145, max. 0.04145
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.25753, max. 0.23595
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.51420, max. 0.56971
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02749, max. 0.02785
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00053, max. 0.00059
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.18404, max. 0.13520
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:40.567 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:40.571 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:40.571 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:40.571 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:40.571 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:40.571 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:42.037 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:42.098 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:42.101 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:42.104 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:42.107 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:42.110 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:42.113 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04158, max. 0.04158
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.25474, max. 0.22658
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.50512, max. 0.56971
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02572, max. 0.02835
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00052, max. 0.00059
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.17467, max. 0.12494
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04151, max. 0.04151
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24078, max. 0.22901
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48328, max. 0.57075
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02480, max. 0.02835
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00059
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.20376, max. 0.14815
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:42.392 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:42.396 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:42.396 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:42.396 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:42.396 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:42.397 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:43.839 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:43.900 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:43.903 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:43.906 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:43.909 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:43.912 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:43.916 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04158, max. 0.04158
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23971, max. 0.22989
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49312, max. 0.57145
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02760, max. 0.02846
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00059
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.19721, max. 0.14212
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04316, max. 0.04316
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21669, max. 0.22041
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.46696, max. 0.53137
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02847, max. 0.03165
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00048, max. 0.00055
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06196, max. 0.05392
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:44.194 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:44.197 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:44.197 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:44.197 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:44.197 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:44.198 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:45.654 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:45.715 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:45.718 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:45.721 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:45.724 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:45.727 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:45.730 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04257, max. 0.04257
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.26548, max. 0.22485
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.53005, max. 0.57064
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02833, max. 0.02952
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00055, max. 0.00059
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.14038, max. 0.09914
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04270, max. 0.04270
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.26002, max. 0.22446
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.51394, max. 0.55972
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02699, max. 0.03107
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00053, max. 0.00058
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.10118, max. 0.07084
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:46.007 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:46.011 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:46.011 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:46.011 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:46.011 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:46.011 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:47.461 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:47.521 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:47.524 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:47.528 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:47.531 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:47.534 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:47.537 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04176, max. 0.04176
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24873, max. 0.22954
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48667, max. 0.57571
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02563, max. 0.02904
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00060
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.20186, max. 0.14709
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04176, max. 0.04176
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.26362, max. 0.22815
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.53261, max. 0.57571
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02457, max. 0.02863
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00055, max. 0.00060
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.15555, max. 0.11559
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:47.915 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:47.919 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:47.919 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:47.919 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:47.919 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:47.919 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:49.362 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:49.419 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:49.422 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:49.424 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:49.426 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:49.429 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:49.431 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03950, max. 0.03950
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24814, max. 0.21802
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48316, max. 0.52730
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02511, max. 0.02286
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00054
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05768, max. 0.05488
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03950, max. 0.03950
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23867, max. 0.21661
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.44782, max. 0.52730
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02443, max. 0.02309
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00046, max. 0.00054
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05584, max. 0.05470
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:49.702 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:49.705 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:49.705 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:49.705 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:49.705 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:49.706 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:50.095 INFO: node_feats: torch.Size([20, 2304])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:47:50.147 INFO: node_feats: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:47:50.150 INFO: node_feats_out:torch.Size([20, 256]), torch.Size([2560]), 20
2024-10-31 23:47:50.152 INFO: linear_node_feats: torch.Size([20, 256])
2024-10-31 23:47:50.155 INFO: q_node_feats: torch.Size([20, 256])
2024-10-31 23:47:50.157 INFO: v_node_feats: torch.Size([20, 256])
2024-10-31 23:47:50.159 INFO: k_node_feats: torch.Size([20, 256])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03950, max. 0.03950
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23370, max. 0.22409
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.43679, max. 0.52730
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02419, max. 0.02451
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00045, max. 0.00054
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05546, max. 0.07763
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
2024-10-31 23:47:50.295 INFO: long_range_embedding: torch.Size([20, 256])
2024-10-31 23:47:50.297 INFO: long_range_energy: torch.Size([20, 1]), torch.Size([1])
2024-10-31 23:47:50.297 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-10-31 23:47:50.298 INFO: total_energy: torch.Size([1])
2024-10-31 23:47:50.298 INFO: node_energy_contributions: torch.Size([20, 4])
2024-10-31 23:47:50.298 INFO: node_energy: torch.Size([20])
2024-10-31 23:47:50.505 INFO: Epoch 45: head: default, loss=  1.6022, MAE_E_per_atom=    43.0 meV, MAE_F=    85.2 meV / A
2024-10-31 23:47:50.506 INFO: Stopping optimization after 15 epochs without improvement and starting Stage Two
2024-10-31 23:47:50.506 INFO: Changing loss based on Stage Two Weights
2024-10-31 23:47:50.579 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:50.639 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:50.643 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:50.646 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:50.649 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:50.652 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:50.655 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04301, max. 0.04301
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.26141, max. 0.22569
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.51825, max. 0.56390
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02724, max. 0.03155
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00054, max. 0.00058
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.10180, max. 0.07131
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04184, max. 0.04184
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.25989, max. 0.23822
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.52202, max. 0.57750
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02811, max. 0.02881
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00054, max. 0.00060
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.18700, max. 0.13749
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:50.930 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:50.933 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:50.933 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:50.934 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:50.934 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:50.934 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:52.370 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:52.431 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:52.435 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:52.438 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:52.441 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:52.444 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:52.447 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04233, max. 0.04233
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.25444, max. 0.24253
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.50670, max. 0.56582
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02635, max. 0.02932
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00052, max. 0.00059
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.11927, max. 0.09012
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04233, max. 0.04233
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.26599, max. 0.23001
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.54095, max. 0.56582
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02551, max. 0.02971
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00056, max. 0.00059
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.15798, max. 0.11776
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:52.726 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:52.730 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:52.730 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:52.730 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:52.730 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:52.730 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:54.168 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:54.228 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:54.232 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:54.235 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:54.238 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:54.241 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:54.244 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04280, max. 0.04280
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24343, max. 0.23303
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49519, max. 0.63495
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02674, max. 0.03062
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00066
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.21187, max. 0.15331
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04280, max. 0.04280
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.25010, max. 0.23138
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49469, max. 0.63506
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02677, max. 0.03070
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00066
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.20774, max. 0.15093
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:54.520 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:54.523 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:54.523 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:54.523 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:54.523 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:54.524 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:55.969 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:56.030 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:56.033 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:56.036 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:56.039 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:56.042 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:56.045 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04375, max. 0.04375
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.26651, max. 0.22719
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.53476, max. 0.69954
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02963, max. 0.03187
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00055, max. 0.00073
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.14471, max. 0.10090
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04395, max. 0.04395
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22424, max. 0.22772
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48335, max. 0.65779
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02998, max. 0.03418
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00068
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06470, max. 0.06465
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:56.324 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:56.327 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:56.327 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:56.327 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:56.328 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:56.328 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:57.881 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:57.941 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:57.944 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:57.947 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:57.951 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:57.954 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:57.957 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04313, max. 0.04313
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23628, max. 0.23020
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.50245, max. 0.68421
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02893, max. 0.03088
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00052, max. 0.00071
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.20578, max. 0.14750
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04313, max. 0.04313
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.25251, max. 0.22606
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.50118, max. 0.68290
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02775, max. 0.03091
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00052, max. 0.00071
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.18199, max. 0.12945
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:47:58.233 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:47:58.237 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:47:58.237 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:47:58.237 INFO: total_energy: torch.Size([2])
2024-10-31 23:47:58.237 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:47:58.237 INFO: node_energy: torch.Size([40])
2024-10-31 23:47:59.696 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:59.756 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:47:59.760 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:47:59.763 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:47:59.766 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:47:59.769 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:47:59.772 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04212, max. 0.04212
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24859, max. 0.22983
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48752, max. 0.64656
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02683, max. 0.03063
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00067
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.20937, max. 0.15272
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04212, max. 0.04212
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24236, max. 0.23050
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48950, max. 0.64644
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02659, max. 0.03058
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00067
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.21399, max. 0.15535
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:00.047 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:00.050 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:00.051 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:00.051 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:00.051 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:00.051 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:01.483 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:01.543 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:01.546 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:01.549 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:01.552 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:01.555 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:01.558 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04154, max. 0.04154
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24627, max. 0.22219
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48247, max. 0.61445
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02700, max. 0.03030
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00064
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.18285, max. 0.13194
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04135, max. 0.04135
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24507, max. 0.22434
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.47242, max. 0.61584
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02614, max. 0.02956
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00049, max. 0.00064
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.11836, max. 0.08945
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:01.836 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:01.839 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:01.839 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:01.840 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:01.840 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:01.840 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:03.282 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:03.344 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:03.347 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:03.350 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:03.353 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:03.356 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:03.359 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04182, max. 0.04182
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.25244, max. 0.20947
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49680, max. 0.60202
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02922, max. 0.03053
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00062
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.14731, max. 0.10554
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04090, max. 0.04090
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22923, max. 0.22490
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48104, max. 0.60520
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02867, max. 0.02988
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00063
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.20558, max. 0.14980
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:03.637 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:03.640 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:03.640 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:03.640 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:03.641 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:03.641 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:05.186 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:05.247 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:05.251 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:05.254 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:05.257 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:05.260 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:05.263 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04137, max. 0.04137
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24398, max. 0.20384
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.46743, max. 0.57347
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02763, max. 0.03145
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00048, max. 0.00059
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.10751, max. 0.07697
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04169, max. 0.04169
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21422, max. 0.21220
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.46064, max. 0.54328
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02925, max. 0.03214
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00047, max. 0.00056
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06778, max. 0.05502
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:05.539 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:05.542 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:05.542 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:05.543 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:05.543 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:05.543 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:07.007 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:07.068 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:07.072 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:07.075 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:07.078 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:07.081 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:07.084 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04040, max. 0.04040
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24315, max. 0.22470
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.47676, max. 0.55635
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02821, max. 0.02904
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00049, max. 0.00058
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.19055, max. 0.14246
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04040, max. 0.04040
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24688, max. 0.21377
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48675, max. 0.55635
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02467, max. 0.02903
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00058
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.15754, max. 0.11874
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:07.363 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:07.366 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:07.366 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:07.366 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:07.366 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:07.367 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:08.821 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:08.882 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:08.885 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:08.888 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:08.891 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:08.894 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:08.897 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03997, max. 0.03997
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23513, max. 0.21456
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.45908, max. 0.56075
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02617, max. 0.02930
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00047, max. 0.00058
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.18258, max. 0.13387
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03985, max. 0.03985
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24308, max. 0.21756
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.45584, max. 0.56231
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02597, max. 0.02859
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00047, max. 0.00058
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.11478, max. 0.08737
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:09.173 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:09.176 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:09.176 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:09.177 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:09.177 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:09.177 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:10.611 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:10.671 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:10.674 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:10.677 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:10.681 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:10.684 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:10.686 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03928, max. 0.03928
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23682, max. 0.22015
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.46453, max. 0.59197
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02777, max. 0.02836
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00048, max. 0.00061
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.18602, max. 0.13882
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03928, max. 0.03928
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24246, max. 0.22447
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.46697, max. 0.59197
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02600, max. 0.02885
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00048, max. 0.00061
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.20391, max. 0.15115
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:10.966 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:10.970 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:10.970 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:10.970 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:10.970 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:10.971 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:12.409 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:12.470 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:12.473 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:12.476 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:12.479 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:12.482 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:12.485 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03960, max. 0.03960
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23619, max. 0.20237
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.47652, max. 0.63442
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02377, max. 0.02796
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00049, max. 0.00066
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.14960, max. 0.11203
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03976, max. 0.03976
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21122, max. 0.20434
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.45957, max. 0.58260
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02831, max. 0.03058
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00047, max. 0.00060
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06693, max. 0.05445
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:12.862 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:12.865 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:12.865 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:12.866 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:12.866 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:12.866 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:14.316 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:14.376 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:14.380 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:14.383 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:14.386 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:14.389 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:14.392 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03976, max. 0.03976
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23233, max. 0.20023
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49576, max. 0.66428
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02778, max. 0.02850
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00069
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.14046, max. 0.10058
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03968, max. 0.03968
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22783, max. 0.19888
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48628, max. 0.64804
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02621, max. 0.02970
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00067
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.10274, max. 0.07324
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:14.670 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:14.673 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:14.673 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:14.674 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:14.674 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:14.674 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:16.123 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:16.183 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:16.187 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:16.190 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:16.193 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:16.196 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:16.199 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03952, max. 0.03952
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22684, max. 0.21315
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.50420, max. 0.67573
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02454, max. 0.02766
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00052, max. 0.00070
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.20049, max. 0.14664
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03952, max. 0.03952
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22671, max. 0.20745
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.50432, max. 0.67585
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02694, max. 0.02769
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00052, max. 0.00070
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.19394, max. 0.14124
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:16.477 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:16.480 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:16.481 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:16.481 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:16.481 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:16.481 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:17.924 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:17.981 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:17.984 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:17.986 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:17.989 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:17.991 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:17.993 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03996, max. 0.03996
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24713, max. 0.21790
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48134, max. 0.57010
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02669, max. 0.02494
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00059
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05980, max. 0.05689
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03996, max. 0.03996
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.24264, max. 0.21657
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.44536, max. 0.57010
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02610, max. 0.02429
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00046, max. 0.00059
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05877, max. 0.05672
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:18.262 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:18.265 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:18.265 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:18.265 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:18.265 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:18.266 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:18.656 INFO: node_feats: torch.Size([20, 2304])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:48:18.709 INFO: node_feats: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:48:18.712 INFO: node_feats_out:torch.Size([20, 256]), torch.Size([2560]), 20
2024-10-31 23:48:18.714 INFO: linear_node_feats: torch.Size([20, 256])
2024-10-31 23:48:18.717 INFO: q_node_feats: torch.Size([20, 256])
2024-10-31 23:48:18.719 INFO: v_node_feats: torch.Size([20, 256])
2024-10-31 23:48:18.721 INFO: k_node_feats: torch.Size([20, 256])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03996, max. 0.03996
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23288, max. 0.21845
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.44381, max. 0.57010
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02570, max. 0.02690
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00046, max. 0.00059
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.05896, max. 0.08325
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
2024-10-31 23:48:18.855 INFO: long_range_embedding: torch.Size([20, 256])
2024-10-31 23:48:18.858 INFO: long_range_energy: torch.Size([20, 1]), torch.Size([1])
2024-10-31 23:48:18.858 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-10-31 23:48:18.858 INFO: total_energy: torch.Size([1])
2024-10-31 23:48:18.858 INFO: node_energy_contributions: torch.Size([20, 4])
2024-10-31 23:48:18.858 INFO: node_energy: torch.Size([20])
2024-10-31 23:48:19.066 INFO: Epoch 603: head: default, loss=  2.8061, MAE_E_per_atom=    41.1 meV, MAE_F=    86.3 meV / A
2024-10-31 23:48:19.601 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:19.661 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:19.665 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:19.668 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:19.671 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:19.674 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:19.677 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03911, max. 0.03911
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22397, max. 0.19449
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49295, max. 0.65695
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02564, max. 0.02879
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00068
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.10162, max. 0.07783
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03923, max. 0.03923
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22598, max. 0.19966
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.50478, max. 0.67682
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02563, max. 0.02737
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00052, max. 0.00070
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.17144, max. 0.12424
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:19.955 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:19.958 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:19.958 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:19.959 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:19.959 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:19.959 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:21.397 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:21.457 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:21.461 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:21.464 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:21.467 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:21.470 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:21.473 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03878, max. 0.03878
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21192, max. 0.19554
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.46771, max. 0.60577
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02707, max. 0.02886
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00048, max. 0.00063
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06606, max. 0.06626
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03861, max. 0.03861
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22559, max. 0.20545
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49665, max. 0.66287
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02385, max. 0.02633
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00069
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.10321, max. 0.08675
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:21.757 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:21.760 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:21.760 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:21.760 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:21.761 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:21.761 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:23.312 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:23.373 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:23.377 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:23.380 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:23.383 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:23.386 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:23.390 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03791, max. 0.03791
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22640, max. 0.21320
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48466, max. 0.64287
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02452, max. 0.02644
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00066
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.19170, max. 0.14210
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03791, max. 0.03791
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21921, max. 0.19411
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48466, max. 0.64287
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02191, max. 0.02593
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00066
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.14095, max. 0.10598
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:23.669 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:23.672 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:23.673 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:23.673 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:23.673 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:23.673 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:25.113 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:25.173 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:25.177 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:25.180 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:25.183 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:25.186 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:25.189 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03745, max. 0.03745
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21727, max. 0.19021
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.47928, max. 0.63209
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02659, max. 0.02625
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00065
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13881, max. 0.10112
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03749, max. 0.03749
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22099, max. 0.20598
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48131, max. 0.63701
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02384, max. 0.02598
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00066
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.19649, max. 0.14548
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:25.469 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:25.473 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:25.473 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:25.473 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:25.473 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:25.473 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:26.915 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:26.976 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:26.979 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:26.982 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:26.985 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:26.988 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:26.991 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03712, max. 0.03712
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21543, max. 0.19769
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.47989, max. 0.62944
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02594, max. 0.02566
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00065
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.18933, max. 0.13995
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03712, max. 0.03712
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21576, max. 0.20749
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.47979, max. 0.62945
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02560, max. 0.02531
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00065
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.17003, max. 0.12729
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:27.272 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:27.275 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:27.275 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:27.275 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:27.276 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:27.276 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:28.734 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:28.794 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:28.798 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:28.801 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:28.804 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:28.807 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:28.810 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03683, max. 0.03683
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21098, max. 0.18612
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.47133, max. 0.60414
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02467, max. 0.02622
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00049, max. 0.00062
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.10117, max. 0.07967
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03694, max. 0.03694
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21791, max. 0.20204
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.47858, max. 0.62905
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02347, max. 0.02521
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00049, max. 0.00065
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.19255, max. 0.14283
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:29.088 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:29.091 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:29.091 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:29.091 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:29.092 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:29.092 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:30.529 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:30.589 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:30.592 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:30.595 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:30.598 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:30.601 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:30.604 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03701, max. 0.03701
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21246, max. 0.20435
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.47588, max. 0.62424
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02508, max. 0.02495
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00049, max. 0.00064
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.16486, max. 0.12343
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03699, max. 0.03698
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21196, max. 0.18530
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.47377, max. 0.61850
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02584, max. 0.02506
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00049, max. 0.00064
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13496, max. 0.09862
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:30.885 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:30.888 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:30.888 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:30.888 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:30.889 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:30.889 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:32.433 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:32.494 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:32.497 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:32.500 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:32.503 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:32.507 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:32.510 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03723, max. 0.03723
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.19761, max. 0.18658
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.45491, max. 0.55747
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02559, max. 0.02626
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00047, max. 0.00057
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06619, max. 0.07109
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03720, max. 0.03720
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21035, max. 0.18552
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.47267, max. 0.62043
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02397, max. 0.02468
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00049, max. 0.00064
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.16299, max. 0.11991
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:32.788 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:32.791 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:32.791 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:32.791 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:32.792 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:32.792 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:34.245 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:34.305 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:34.308 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:34.311 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:34.314 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:34.317 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:34.320 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03726, max. 0.03726
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21642, max. 0.19808
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.47039, max. 0.61095
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02266, max. 0.02393
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00049, max. 0.00063
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.09227, max. 0.09227
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03726, max. 0.03726
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20812, max. 0.19025
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.47039, max. 0.61095
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01996, max. 0.02384
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00049, max. 0.00063
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.12938, max. 0.09766
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:34.601 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:34.604 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:34.605 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:34.605 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:34.605 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:34.605 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:36.054 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:36.114 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:36.117 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:36.121 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:36.124 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:36.127 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:36.130 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03773, max. 0.03773
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21538, max. 0.20326
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.47436, max. 0.62250
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02316, max. 0.02412
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00049, max. 0.00064
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.17713, max. 0.13186
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03773, max. 0.03773
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20981, max. 0.18983
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.47447, max. 0.62249
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02489, max. 0.02416
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00049, max. 0.00064
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.17813, max. 0.13171
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:36.427 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:36.431 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:36.431 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:36.431 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:36.431 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:36.432 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:37.888 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:37.949 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:37.952 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:37.955 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:37.958 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:37.962 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:37.965 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03817, max. 0.03817
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21108, max. 0.18331
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.47637, max. 0.63009
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02506, max. 0.02400
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00049, max. 0.00065
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.12891, max. 0.09838
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03818, max. 0.03818
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21102, max. 0.18399
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.47911, max. 0.63436
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02371, max. 0.02402
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00049, max. 0.00065
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.15763, max. 0.11575
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:38.244 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:38.248 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:38.248 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:38.249 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:38.249 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:38.249 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:39.688 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:39.856 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:39.859 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:39.863 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:39.866 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:39.869 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:39.872 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03854, max. 0.03854
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21274, max. 0.19558
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48581, max. 0.64351
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02277, max. 0.02371
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00066
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.17936, max. 0.13266
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03854, max. 0.03854
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21261, max. 0.18741
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48595, max. 0.64365
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02447, max. 0.02370
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00066
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.17418, max. 0.12834
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:40.155 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:40.158 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:40.159 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:40.159 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:40.159 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:40.159 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:41.609 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:41.669 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:41.673 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:41.676 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:41.679 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:41.682 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:41.685 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03886, max. 0.03885
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21338, max. 0.19963
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49006, max. 0.65007
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02269, max. 0.02352
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00067
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.17100, max. 0.12670
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03874, max. 0.03874
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20821, max. 0.18200
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.47096, max. 0.61899
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02336, max. 0.02415
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00049, max. 0.00064
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.09402, max. 0.09768
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:41.965 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:41.968 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:41.968 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:41.968 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:41.968 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:41.969 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:43.419 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:43.479 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:43.483 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:43.486 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:43.489 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:43.492 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:43.495 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03907, max. 0.03907
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21258, max. 0.18883
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48942, max. 0.64920
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01926, max. 0.02280
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00067
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.12044, max. 0.10877
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03907, max. 0.03907
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21258, max. 0.19738
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48942, max. 0.64920
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02404, max. 0.02419
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00067
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.15021, max. 0.11290
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:43.774 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:43.777 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:43.777 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:43.778 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:43.778 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:43.778 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:45.218 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:45.279 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:45.282 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:45.285 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:45.288 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:45.291 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:45.295 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03948, max. 0.03948
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.19730, max. 0.17678
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.45918, max. 0.58300
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02430, max. 0.02452
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00047, max. 0.00060
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.07530, max. 0.09054
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03945, max. 0.03945
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21395, max. 0.19262
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49599, max. 0.65948
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02171, max. 0.02271
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00068
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.08357, max. 0.11423
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:45.575 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:45.579 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:45.579 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:45.579 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:45.579 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:45.579 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:47.050 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:47.107 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:47.110 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:47.113 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:47.115 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:47.117 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:47.119 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03870, max. 0.03870
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23517, max. 0.20990
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.45070, max. 0.60175
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02647, max. 0.02448
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00047, max. 0.00062
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06110, max. 0.07127
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03870, max. 0.03870
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.23407, max. 0.20866
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.45070, max. 0.60175
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02592, max. 0.02415
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00047, max. 0.00062
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06110, max. 0.07243
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:47.483 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:47.486 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:47.486 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:47.487 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:47.487 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:47.487 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:47.878 INFO: node_feats: torch.Size([20, 2304])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:48:47.932 INFO: node_feats: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:48:47.935 INFO: node_feats_out:torch.Size([20, 256]), torch.Size([2560]), 20
2024-10-31 23:48:47.937 INFO: linear_node_feats: torch.Size([20, 256])
2024-10-31 23:48:47.939 INFO: q_node_feats: torch.Size([20, 256])
2024-10-31 23:48:47.942 INFO: v_node_feats: torch.Size([20, 256])
2024-10-31 23:48:47.944 INFO: k_node_feats: torch.Size([20, 256])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03870, max. 0.03870
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22033, max. 0.20177
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.45070, max. 0.60175
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02553, max. 0.02661
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00047, max. 0.00062
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06121, max. 0.08504
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
2024-10-31 23:48:48.080 INFO: long_range_embedding: torch.Size([20, 256])
2024-10-31 23:48:48.082 INFO: long_range_energy: torch.Size([20, 1]), torch.Size([1])
2024-10-31 23:48:48.082 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-10-31 23:48:48.083 INFO: total_energy: torch.Size([1])
2024-10-31 23:48:48.083 INFO: node_energy_contributions: torch.Size([20, 4])
2024-10-31 23:48:48.083 INFO: node_energy: torch.Size([20])
2024-10-31 23:48:48.291 INFO: Epoch 606: head: default, loss=  2.6866, MAE_E_per_atom=    38.9 meV, MAE_F=    85.9 meV / A
2024-10-31 23:48:48.844 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:48.904 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:48.908 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:48.911 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:48.914 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:48.917 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:48.920 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03975, max. 0.03975
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21276, max. 0.18558
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49237, max. 0.65417
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02421, max. 0.02298
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00068
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.12334, max. 0.11315
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03977, max. 0.03977
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21393, max. 0.19552
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49820, max. 0.66244
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02375, max. 0.02395
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00068
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.14691, max. 0.11942
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:49.195 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:49.198 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:49.199 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:49.199 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:49.199 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:49.199 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:50.637 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:50.697 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:50.706 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:50.709 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:50.712 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:50.715 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:50.718 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04004, max. 0.04004
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21244, max. 0.18581
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49578, max. 0.65893
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02311, max. 0.02305
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00068
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.14992, max. 0.11897
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04005, max. 0.04005
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21344, max. 0.19668
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49822, max. 0.66201
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02214, max. 0.02302
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00068
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.16504, max. 0.12637
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:51.000 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:51.003 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:51.003 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:51.004 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:51.004 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:51.004 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:52.447 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:52.508 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:52.512 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:52.515 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:52.518 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:52.521 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:52.524 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04029, max. 0.04029
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21261, max. 0.19211
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49678, max. 0.65954
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02137, max. 0.02200
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00068
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.08023, max. 0.12015
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04029, max. 0.04029
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21261, max. 0.18808
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49678, max. 0.65954
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01862, max. 0.02181
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00068
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.11525, max. 0.11867
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:52.803 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:52.806 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:52.806 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:52.806 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:52.806 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:52.807 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:54.242 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:54.303 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:54.306 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:54.309 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:54.312 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:54.315 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:54.318 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04078, max. 0.04078
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.19636, max. 0.17359
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.46595, max. 0.58818
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02360, max. 0.02362
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00048, max. 0.00061
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.08361, max. 0.09982
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04064, max. 0.04064
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20810, max. 0.18193
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48087, max. 0.63468
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02230, max. 0.02294
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00065
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.09156, max. 0.11291
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:54.599 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:54.602 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:54.602 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:54.602 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:54.602 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:54.603 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:56.083 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:56.144 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:56.147 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:56.150 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:56.153 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:56.156 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:56.159 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04096, max. 0.04096
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21245, max. 0.18907
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49819, max. 0.66002
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02206, max. 0.02275
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00068
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.17024, max. 0.13287
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04096, max. 0.04096
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21232, max. 0.18489
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49834, max. 0.66017
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02329, max. 0.02251
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00068
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.16563, max. 0.12986
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:56.545 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:56.548 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:56.548 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:56.549 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:56.549 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:56.549 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:58.013 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:58.074 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:58.077 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:58.080 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:58.083 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:58.087 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:58.090 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04111, max. 0.04111
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20853, max. 0.18075
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48636, max. 0.63976
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02356, max. 0.02372
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00066
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.12237, max. 0.12004
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04114, max. 0.04114
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21004, max. 0.19278
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49156, max. 0.64938
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02296, max. 0.02402
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00067
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.14215, max. 0.12590
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:48:58.367 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:48:58.370 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:48:58.370 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:48:58.371 INFO: total_energy: torch.Size([2])
2024-10-31 23:48:58.371 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:48:58.371 INFO: node_energy: torch.Size([40])
2024-10-31 23:48:59.805 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:59.865 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:48:59.869 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:48:59.872 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:48:59.875 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:48:59.878 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:48:59.881 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04133, max. 0.04133
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20788, max. 0.19182
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48829, max. 0.63943
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02107, max. 0.02222
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00066
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.08061, max. 0.12372
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04133, max. 0.04133
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20776, max. 0.18482
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48839, max. 0.63940
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02309, max. 0.02278
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00066
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.16351, max. 0.12927
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:00.160 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:00.164 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:00.164 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:00.164 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:00.164 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:00.164 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:01.615 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:01.677 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:01.680 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:01.683 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:01.686 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:01.689 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:01.692 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04168, max. 0.04168
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20793, max. 0.18778
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49002, max. 0.64010
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01783, max. 0.02123
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00066
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.10997, max. 0.12441
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04168, max. 0.04168
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20793, max. 0.18662
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49000, max. 0.63992
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02174, max. 0.02350
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00066
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.16673, max. 0.13433
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:01.969 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:01.973 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:01.973 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:01.973 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:01.973 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:01.973 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:03.412 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:03.472 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:03.476 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:03.479 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:03.482 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:03.485 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:03.488 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04202, max. 0.04202
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20183, max. 0.17669
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48005, max. 0.60924
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02159, max. 0.02324
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00049, max. 0.00063
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.10003, max. 0.11807
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04216, max. 0.04216
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.18918, max. 0.17268
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.46489, max. 0.55924
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02278, max. 0.02412
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00048, max. 0.00058
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.09407, max. 0.10422
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:03.766 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:03.769 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:03.769 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:03.769 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:03.770 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:03.770 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:05.324 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:05.386 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:05.389 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:05.393 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:05.396 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:05.399 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:05.402 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04244, max. 0.04244
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20797, max. 0.19424
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49317, max. 0.64203
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02115, max. 0.02418
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00066
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.15624, max. 0.13724
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04243, max. 0.04243
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20679, max. 0.17946
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49138, max. 0.63823
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02195, max. 0.02395
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00066
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.14450, max. 0.13112
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:05.682 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:05.685 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:05.685 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:05.686 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:05.686 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:05.686 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:07.147 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:07.207 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:07.211 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:07.214 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:07.217 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:07.220 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:07.223 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04271, max. 0.04270
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20476, max. 0.17723
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48856, max. 0.62536
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02281, max. 0.02431
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00064
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.11876, max. 0.12772
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04262, max. 0.04262
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.19848, max. 0.17726
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.47848, max. 0.59411
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02146, max. 0.02353
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00049, max. 0.00061
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.10369, max. 0.11903
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:07.502 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:07.505 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:07.505 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:07.506 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:07.506 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:07.506 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:08.945 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:09.005 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:09.009 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:09.012 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:09.015 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:09.018 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:09.021 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04297, max. 0.04297
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20402, max. 0.18569
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49081, max. 0.62286
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02256, max. 0.02337
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00064
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.15716, max. 0.13589
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04296, max. 0.04296
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20292, max. 0.17987
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48885, max. 0.61893
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02177, max. 0.02464
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00064
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.14260, max. 0.13079
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:09.303 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:09.306 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:09.306 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:09.307 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:09.307 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:09.307 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:10.751 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:10.812 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:10.816 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:10.819 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:10.822 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:10.825 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:10.828 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04318, max. 0.04318
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20190, max. 0.18988
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48872, max. 0.61144
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02207, max. 0.02448
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00063
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13188, max. 0.13075
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04318, max. 0.04318
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20190, max. 0.19369
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48872, max. 0.61144
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02105, max. 0.02546
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00063
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.15194, max. 0.13557
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:11.108 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:11.112 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:11.112 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:11.112 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:11.112 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:11.112 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:12.549 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:12.610 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:12.613 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:12.616 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:12.619 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:12.622 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:12.626 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04350, max. 0.04350
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.17871, max. 0.17438
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.45718, max. 0.51263
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02243, max. 0.02527
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00047, max. 0.00053
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.10129, max. 0.10382
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04348, max. 0.04347
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20164, max. 0.18732
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48875, max. 0.61292
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02134, max. 0.02483
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00063
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.15692, max. 0.13942
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:12.905 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:12.908 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:12.909 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:12.909 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:12.909 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:12.909 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:14.459 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:14.521 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:14.525 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:14.528 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:14.531 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:14.534 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:14.537 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04374, max. 0.04374
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20141, max. 0.19345
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48854, max. 0.61240
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02061, max. 0.02355
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00063
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.08979, max. 0.13329
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04374, max. 0.04374
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20084, max. 0.18940
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48854, max. 0.61240
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01708, max. 0.02210
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00063
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.09838, max. 0.13164
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:14.816 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:14.819 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:14.819 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:14.820 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:14.820 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:14.820 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:16.273 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:16.330 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:16.333 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:16.335 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:16.338 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:16.340 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:16.342 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03767, max. 0.03767
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22465, max. 0.20304
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.46698, max. 0.61915
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02558, max. 0.02335
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00048, max. 0.00064
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06340, max. 0.09472
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03767, max. 0.03767
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22345, max. 0.20180
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.46698, max. 0.61915
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02497, max. 0.02296
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00048, max. 0.00064
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06338, max. 0.09566
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:16.612 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:16.615 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:16.615 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:16.615 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:16.615 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:16.615 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:17.006 INFO: node_feats: torch.Size([20, 2304])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:49:17.059 INFO: node_feats: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:49:17.061 INFO: node_feats_out:torch.Size([20, 256]), torch.Size([2560]), 20
2024-10-31 23:49:17.064 INFO: linear_node_feats: torch.Size([20, 256])
2024-10-31 23:49:17.066 INFO: q_node_feats: torch.Size([20, 256])
2024-10-31 23:49:17.068 INFO: v_node_feats: torch.Size([20, 256])
2024-10-31 23:49:17.071 INFO: k_node_feats: torch.Size([20, 256])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.03767, max. 0.03767
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21035, max. 0.19201
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.46698, max. 0.61915
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02474, max. 0.02506
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00048, max. 0.00064
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.06293, max. 0.09462
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
2024-10-31 23:49:17.206 INFO: long_range_embedding: torch.Size([20, 256])
2024-10-31 23:49:17.208 INFO: long_range_energy: torch.Size([20, 1]), torch.Size([1])
2024-10-31 23:49:17.208 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-10-31 23:49:17.209 INFO: total_energy: torch.Size([1])
2024-10-31 23:49:17.209 INFO: node_energy_contributions: torch.Size([20, 4])
2024-10-31 23:49:17.209 INFO: node_energy: torch.Size([20])
2024-10-31 23:49:17.416 INFO: Epoch 609: head: default, loss=  2.5454, MAE_E_per_atom=    37.6 meV, MAE_F=    84.7 meV / A
2024-10-31 23:49:17.954 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:18.014 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:18.018 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:18.021 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:18.024 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:18.027 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:18.030 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04433, max. 0.04433
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20328, max. 0.18060
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49383, max. 0.62858
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02152, max. 0.02454
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00065
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13571, max. 0.14006
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04434, max. 0.04434
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20449, max. 0.18648
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49593, max. 0.63293
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02208, max. 0.02347
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00065
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.14943, max. 0.14495
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:18.307 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:18.310 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:18.310 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:18.311 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:18.311 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:18.311 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:19.743 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:19.804 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:19.808 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:19.811 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:19.814 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:19.817 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:19.820 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04485, max. 0.04485
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20562, max. 0.17839
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49777, max. 0.63854
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02221, max. 0.02435
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00066
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.11492, max. 0.14151
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04488, max. 0.04488
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20822, max. 0.18948
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.50265, max. 0.65231
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01714, max. 0.02163
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00052, max. 0.00067
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.09548, max. 0.14389
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:20.100 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:20.104 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:20.104 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:20.104 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:20.104 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:20.104 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:21.550 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:21.612 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:21.615 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:21.618 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:21.622 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:21.625 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:21.628 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04548, max. 0.04548
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21249, max. 0.19375
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.51452, max. 0.67357
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02015, max. 0.02275
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00053, max. 0.00069
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.09797, max. 0.15185
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04548, max. 0.04548
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21249, max. 0.18777
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.51452, max. 0.67357
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02161, max. 0.02384
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00053, max. 0.00069
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.12315, max. 0.15259
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:21.907 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:21.910 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:21.910 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:21.910 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:21.911 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:21.911 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:23.350 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:23.509 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:23.513 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:23.516 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:23.519 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:23.522 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:23.525 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04626, max. 0.04626
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.19258, max. 0.17499
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48603, max. 0.59258
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02156, max. 0.02380
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00061
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.10994, max. 0.13029
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04622, max. 0.04622
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21878, max. 0.19418
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.53590, max. 0.70460
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02045, max. 0.02350
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00055, max. 0.00073
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.14398, max. 0.16561
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:23.808 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:23.812 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:23.812 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:23.812 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:23.812 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:23.812 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:25.273 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:25.333 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:25.337 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:25.340 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:25.343 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:25.346 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:25.349 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04666, max. 0.04666
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21057, max. 0.18281
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.51115, max. 0.66705
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02038, max. 0.02332
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00053, max. 0.00069
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.11950, max. 0.15146
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04678, max. 0.04678
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22176, max. 0.19098
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.54687, max. 0.72012
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02121, max. 0.02266
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00056, max. 0.00074
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.15199, max. 0.17302
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:25.632 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:25.635 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:25.635 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:25.635 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:25.636 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:25.636 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:27.102 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:27.164 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:27.167 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:27.170 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:27.173 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:27.176 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:27.179 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04716, max. 0.04716
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22159, max. 0.19042
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.54784, max. 0.72083
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02174, max. 0.02328
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00057, max. 0.00074
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.14808, max. 0.17332
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04716, max. 0.04716
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22174, max. 0.19053
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.54783, max. 0.72089
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02138, max. 0.02357
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00057, max. 0.00074
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.12158, max. 0.16844
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:27.458 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:27.461 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:27.461 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:27.461 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:27.461 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:27.462 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:28.890 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:28.951 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:28.954 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:28.957 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:28.960 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:28.963 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:28.966 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04749, max. 0.04749
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22134, max. 0.19430
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.54713, max. 0.71959
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02025, max. 0.02288
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00056, max. 0.00074
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.11102, max. 0.16968
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04737, max. 0.04737
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20959, max. 0.18125
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.51381, max. 0.66422
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02023, max. 0.02339
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00053, max. 0.00069
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.12411, max. 0.15490
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:29.248 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:29.251 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:29.252 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:29.252 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:29.252 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:29.252 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:30.699 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:30.760 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:30.764 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:30.767 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:30.770 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:30.773 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:30.776 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04777, max. 0.04777
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21957, max. 0.18995
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.54161, max. 0.71096
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02100, max. 0.02317
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00056, max. 0.00073
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.15108, max. 0.17696
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04776, max. 0.04776
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21798, max. 0.18660
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.53778, max. 0.70585
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02125, max. 0.02387
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00055, max. 0.00073
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13965, max. 0.17041
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:31.152 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:31.156 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:31.156 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:31.156 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:31.156 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:31.157 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:32.602 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:32.663 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:32.666 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:32.669 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:32.672 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:32.675 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:32.679 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04799, max. 0.04799
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21692, max. 0.19341
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.53315, max. 0.69812
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01999, max. 0.02403
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00055, max. 0.00072
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.14180, max. 0.17372
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04803, max. 0.04803
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.18827, max. 0.17696
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49034, max. 0.57692
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02092, max. 0.02411
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00059
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.12145, max. 0.13714
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:32.957 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:32.960 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:32.961 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:32.961 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:32.961 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:32.961 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:34.411 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:34.472 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:34.475 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:34.478 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:34.481 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:34.484 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:34.487 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04810, max. 0.04810
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20915, max. 0.18129
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.51911, max. 0.66058
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02126, max. 0.02446
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00054, max. 0.00068
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13489, max. 0.16216
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04813, max. 0.04813
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21253, max. 0.19046
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.52511, max. 0.67734
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01726, max. 0.02187
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00054, max. 0.00070
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.10832, max. 0.16469
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:34.769 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:34.773 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:34.773 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:34.773 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:34.773 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:34.773 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:36.233 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:36.294 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:36.297 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:36.300 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:36.303 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:36.306 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:36.309 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04836, max. 0.04836
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20923, max. 0.19279
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.52181, max. 0.66119
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01982, max. 0.02506
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00054, max. 0.00068
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13917, max. 0.16951
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04833, max. 0.04833
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20575, max. 0.18228
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.51570, max. 0.64403
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02119, max. 0.02492
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00053, max. 0.00066
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13694, max. 0.16070
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:36.604 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:36.607 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:36.607 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:36.607 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:36.608 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:36.608 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:38.042 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:38.103 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:38.106 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:38.109 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:38.112 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:38.115 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:38.118 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04860, max. 0.04860
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.17583, max. 0.18053
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.47681, max. 0.51931
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02062, max. 0.02511
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00049, max. 0.00053
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.12633, max. 0.13120
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04857, max. 0.04857
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20607, max. 0.19530
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.51817, max. 0.64665
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02057, max. 0.02403
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00053, max. 0.00067
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.10871, max. 0.16405
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:38.401 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:38.406 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:38.406 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:38.406 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:38.406 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:38.406 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:39.951 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:40.012 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:40.016 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:40.019 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:40.022 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:40.025 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:40.028 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04897, max. 0.04897
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20313, max. 0.18417
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.51626, max. 0.63403
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02039, max. 0.02555
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00053, max. 0.00065
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.14887, max. 0.16529
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04898, max. 0.04898
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20487, max. 0.18984
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.51875, max. 0.63973
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02064, max. 0.02549
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00053, max. 0.00066
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.14482, max. 0.17133
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:40.306 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:40.310 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:40.310 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:40.310 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:40.310 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:40.310 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:41.764 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:41.825 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:41.828 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:41.832 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:41.835 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:41.838 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:41.841 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04920, max. 0.04920
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.19013, max. 0.18259
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49694, max. 0.57335
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02005, max. 0.02453
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00051, max. 0.00059
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13792, max. 0.15029
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04931, max. 0.04931
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20393, max. 0.19163
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.51870, max. 0.63676
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01758, max. 0.02279
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00053, max. 0.00066
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.10941, max. 0.16425
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:42.119 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:42.122 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:42.122 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:42.123 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:42.123 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:42.123 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:43.573 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:43.634 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:43.637 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:43.640 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:43.643 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:43.646 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:43.649 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04979, max. 0.04979
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20468, max. 0.18952
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.52171, max. 0.64121
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02024, max. 0.02464
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00054, max. 0.00066
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.11290, max. 0.16879
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04979, max. 0.04979
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20453, max. 0.18891
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.52182, max. 0.64110
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02081, max. 0.02464
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00054, max. 0.00066
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.15326, max. 0.17458
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:43.931 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:43.934 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:43.934 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:43.934 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:43.935 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:43.935 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:45.380 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:45.438 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:45.440 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:45.443 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:45.445 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:45.447 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:45.450 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04118, max. 0.04118
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21463, max. 0.19862
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48798, max. 0.64196
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02452, max. 0.02398
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00066
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.07838, max. 0.12083
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04118, max. 0.04118
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21332, max. 0.19693
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48798, max. 0.64196
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02388, max. 0.02350
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00066
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.07865, max. 0.12153
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:45.720 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:45.722 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:45.723 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:45.723 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:45.723 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:45.723 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:46.109 INFO: node_feats: torch.Size([20, 2304])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:49:46.161 INFO: node_feats: torch.Size([20, 256])
energy graph: torch.Size([1]) torch.Size([20])
2024-10-31 23:49:46.164 INFO: node_feats_out:torch.Size([20, 256]), torch.Size([2560]), 20
2024-10-31 23:49:46.166 INFO: linear_node_feats: torch.Size([20, 256])
2024-10-31 23:49:46.169 INFO: q_node_feats: torch.Size([20, 256])
2024-10-31 23:49:46.171 INFO: v_node_feats: torch.Size([20, 256])
2024-10-31 23:49:46.173 INFO: k_node_feats: torch.Size([20, 256])
q_vector from forward: torch.Size([20, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.04118, max. 0.04118
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21166, max. 0.18730
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48798, max. 0.64196
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02373, max. 0.02360
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00066
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.07776, max. 0.12043
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([20, 256])
2024-10-31 23:49:46.310 INFO: long_range_embedding: torch.Size([20, 256])
2024-10-31 23:49:46.312 INFO: long_range_energy: torch.Size([20, 1]), torch.Size([1])
2024-10-31 23:49:46.312 INFO: contributions: torch.Size([1, 4]), 4, torch.Size([1])
2024-10-31 23:49:46.313 INFO: total_energy: torch.Size([1])
2024-10-31 23:49:46.313 INFO: node_energy_contributions: torch.Size([20, 4])
2024-10-31 23:49:46.313 INFO: node_energy: torch.Size([20])
2024-10-31 23:49:46.520 INFO: Epoch 612: head: default, loss=  2.3417, MAE_E_per_atom=    34.9 meV, MAE_F=    83.8 meV / A
2024-10-31 23:49:47.003 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:47.065 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:47.068 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:47.071 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:47.074 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:47.077 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:47.081 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.05027, max. 0.05027
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20673, max. 0.19028
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.52577, max. 0.65408
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02029, max. 0.02504
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00054, max. 0.00067
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.14324, max. 0.18073
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.05027, max. 0.05027
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20675, max. 0.19355
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.52582, max. 0.65432
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01897, max. 0.02533
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00054, max. 0.00067
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.12835, max. 0.17804
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:47.357 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:47.361 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:47.361 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:47.361 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:47.361 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:47.362 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:48.882 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:48.942 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:48.946 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:48.949 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:48.952 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:48.955 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:48.958 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.05069, max. 0.05069
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20953, max. 0.19594
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.53007, max. 0.67334
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.02002, max. 0.02325
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00055, max. 0.00069
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.12200, max. 0.18128
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.05069, max. 0.05069
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20953, max. 0.19159
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.53007, max. 0.67334
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01740, max. 0.02189
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00055, max. 0.00069
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.12050, max. 0.17938
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:49.240 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:49.243 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:49.243 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:49.243 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:49.244 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:49.244 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:50.679 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:50.740 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:50.744 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:50.747 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:50.750 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:50.753 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:50.756 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.05135, max. 0.05135
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21371, max. 0.18300
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.53821, max. 0.70070
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01933, max. 0.02341
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00056, max. 0.00072
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.15439, max. 0.19208
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.05132, max. 0.05132
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21107, max. 0.18372
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.53254, max. 0.68609
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01919, max. 0.02299
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00055, max. 0.00071
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.14780, max. 0.18692
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:51.034 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:51.038 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:51.038 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:51.038 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:51.038 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:51.038 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:52.484 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:52.545 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:52.548 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:52.551 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:52.554 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:52.557 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:52.560 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.05193, max. 0.05193
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21944, max. 0.18811
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.55714, max. 0.72762
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01965, max. 0.02301
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00057, max. 0.00075
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.15781, max. 0.20327
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.05181, max. 0.05181
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20348, max. 0.18287
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.52190, max. 0.65630
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01809, max. 0.02269
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00054, max. 0.00068
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.14543, max. 0.18138
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:52.838 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:52.842 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:52.842 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:52.842 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:52.842 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:52.842 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:54.298 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:54.359 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:54.363 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:54.366 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:54.369 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:54.372 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:54.375 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.05235, max. 0.05235
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.22031, max. 0.18775
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.55993, max. 0.73134
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01843, max. 0.02274
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00058, max. 0.00075
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13530, max. 0.20004
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.05239, max. 0.05239
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.18412, max. 0.18223
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.49989, max. 0.58278
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01801, max. 0.02228
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00052, max. 0.00060
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13945, max. 0.16501
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:54.653 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:54.656 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:54.656 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:54.656 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:54.657 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:54.657 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:56.123 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:56.183 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:56.187 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:56.190 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:56.193 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:56.196 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:56.199 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.05265, max. 0.05265
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21831, max. 0.18825
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.55449, max. 0.72270
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01821, max. 0.02285
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00057, max. 0.00075
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13571, max. 0.20021
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.05264, max. 0.05264
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21620, max. 0.18334
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.54920, max. 0.71551
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01875, max. 0.02298
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00057, max. 0.00074
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.16149, max. 0.20212
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:56.573 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:56.577 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:56.577 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:56.577 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:56.577 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:56.577 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:58.009 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:58.070 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:58.074 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:58.077 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:58.080 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:58.083 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:58.086 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.05292, max. 0.05292
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21619, max. 0.19520
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.54801, max. 0.71256
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01741, max. 0.02306
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00057, max. 0.00074
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13851, max. 0.20355
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.05280, max. 0.05280
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.19909, max. 0.18409
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.52097, max. 0.63753
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01800, max. 0.02280
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00054, max. 0.00066
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.15261, max. 0.18375
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:49:58.366 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:49:58.369 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:49:58.370 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:49:58.370 INFO: total_energy: torch.Size([2])
2024-10-31 23:49:58.370 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:49:58.370 INFO: node_energy: torch.Size([40])
2024-10-31 23:49:59.815 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:59.876 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:49:59.879 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:49:59.883 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:49:59.886 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:49:59.889 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:49:59.892 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.05312, max. 0.05312
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21257, max. 0.19242
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.54426, max. 0.69515
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01916, max. 0.02310
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00056, max. 0.00072
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.15383, max. 0.20449
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.05312, max. 0.05312
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21259, max. 0.19061
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.54432, max. 0.69540
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01745, max. 0.02166
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00056, max. 0.00072
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13426, max. 0.19720
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:50:00.171 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:50:00.174 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:50:00.175 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:50:00.175 INFO: total_energy: torch.Size([2])
2024-10-31 23:50:00.175 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:50:00.175 INFO: node_energy: torch.Size([40])
2024-10-31 23:50:01.625 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:50:01.686 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:50:01.690 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:50:01.693 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:50:01.696 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:50:01.699 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:50:01.702 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.05333, max. 0.05332
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20531, max. 0.18612
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.53469, max. 0.66287
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01845, max. 0.02339
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00055, max. 0.00068
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.16196, max. 0.19435
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.05336, max. 0.05336
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.21052, max. 0.18774
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.54344, max. 0.68694
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01912, max. 0.02359
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00056, max. 0.00071
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.16867, max. 0.20417
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:50:01.979 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:50:01.983 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:50:01.983 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:50:01.983 INFO: total_energy: torch.Size([2])
2024-10-31 23:50:01.983 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:50:01.983 INFO: node_energy: torch.Size([40])
2024-10-31 23:50:03.427 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:50:03.487 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:50:03.491 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:50:03.494 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:50:03.497 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:50:03.500 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:50:03.503 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.05357, max. 0.05357
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20824, max. 0.19514
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.54156, max. 0.67522
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01999, max. 0.02323
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00056, max. 0.00070
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13557, max. 0.19807
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.05361, max. 0.05361
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.17141, max. 0.18572
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.48880, max. 0.51658
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01805, max. 0.02348
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00050, max. 0.00053
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.14968, max. 0.16027
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:50:03.783 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:50:03.787 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:50:03.787 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:50:03.787 INFO: total_energy: torch.Size([2])
2024-10-31 23:50:03.787 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:50:03.787 INFO: node_energy: torch.Size([40])
2024-10-31 23:50:05.351 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:50:05.412 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:50:05.415 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:50:05.418 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:50:05.421 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:50:05.425 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:50:05.428 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.05385, max. 0.05385
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.18874, max. 0.18711
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.51490, max. 0.58861
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01846, max. 0.02369
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00053, max. 0.00061
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.16168, max. 0.17995
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.05396, max. 0.05396
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20703, max. 0.19270
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.54291, max. 0.66825
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01793, max. 0.02239
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00056, max. 0.00069
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13432, max. 0.19604
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:50:05.705 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:50:05.708 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:50:05.709 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:50:05.709 INFO: total_energy: torch.Size([2])
2024-10-31 23:50:05.709 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:50:05.709 INFO: node_energy: torch.Size([40])
2024-10-31 23:50:07.146 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:50:07.207 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:50:07.210 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:50:07.213 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:50:07.216 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:50:07.219 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:50:07.222 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.05447, max. 0.05447
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20466, max. 0.19008
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.54318, max. 0.65849
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01917, max. 0.02502
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00056, max. 0.00068
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.17606, max. 0.20012
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.05449, max. 0.05449
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20704, max. 0.19669
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.54651, max. 0.66664
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01791, max. 0.02509
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00056, max. 0.00069
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13849, max. 0.20125
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:50:07.503 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:50:07.507 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:50:07.507 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:50:07.507 INFO: total_energy: torch.Size([2])
2024-10-31 23:50:07.507 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:50:07.507 INFO: node_energy: torch.Size([40])
2024-10-31 23:50:08.949 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:50:09.010 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:50:09.013 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:50:09.017 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:50:09.020 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:50:09.023 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:50:09.026 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.05496, max. 0.05495
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20754, max. 0.19602
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.54974, max. 0.66921
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01953, max. 0.02497
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00057, max. 0.00069
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.16563, max. 0.20695
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.05495, max. 0.05495
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20756, max. 0.19581
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.54979, max. 0.66947
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01864, max. 0.02466
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00057, max. 0.00069
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.13761, max. 0.19990
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:50:09.305 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:50:09.309 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:50:09.309 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:50:09.309 INFO: total_energy: torch.Size([2])
2024-10-31 23:50:09.309 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:50:09.309 INFO: node_energy: torch.Size([40])
2024-10-31 23:50:10.746 INFO: node_feats: torch.Size([40, 2304])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:50:10.808 INFO: node_feats: torch.Size([40, 256])
energy graph: torch.Size([2]) torch.Size([40])
2024-10-31 23:50:10.811 INFO: node_feats_out:torch.Size([40, 256]), torch.Size([2560]), 40
2024-10-31 23:50:10.815 INFO: linear_node_feats: torch.Size([40, 256])
2024-10-31 23:50:10.818 INFO: q_node_feats: torch.Size([40, 256])
2024-10-31 23:50:10.821 INFO: v_node_feats: torch.Size([40, 256])
2024-10-31 23:50:10.824 INFO: k_node_feats: torch.Size([40, 256])
q_vector from forward: torch.Size([40, 256])
torch.isfinite(q_vector).all(): tensor(True, device='cuda:0') torch.isnan(q_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(k_vector).all(): tensor(True, device='cuda:0') torch.isnan(k_vector).any(): tensor(False, device='cuda:0')
torch.isfinite(v_vector).all(): tensor(True, device='cuda:0') torch.isnan(v_vector).any(): tensor(False, device='cuda:0')
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.05537, max. 0.05537
i: tensor(0, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20331, max. 0.19110
i: tensor(0, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.54426, max. 0.65454
i: tensor(0, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01876, max. 0.02435
weighted_values: torch.Size([20, 983, 256])
i: tensor(0, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00056, max. 0.00067
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(0, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.17477, max. 0.20171
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from value or key: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
k_vectors shape: torch.Size([983, 3]) torch.return_types.max(
values=tensor([ 7, 14, 14], device='cuda:0'),
indices=tensor([962, 171,  65], device='cuda:0')) torch.return_types.min(
values=tensor([0, 0, 0], device='cuda:0'),
indices=tensor([ 0,  0, 51], device='cuda:0'))
k_vectors dtype: torch.int64
value from optimized ewald sum: torch.Size([20, 256, 8, 15, 15])
term from query: torch.Size([20, 256, 983]) torch.Size([20, 256, 8, 15, 15]) torch.Size([983, 256])
q_pot: torch.Size([20, 256, 983]) torch.Size([983, 256]) torch.Size([983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(q_pot).all(): tensor(True, device='cuda:0') torch.isnan(q_pot).any(): tensor(False, device='cuda:0') min. -0.05541, max. 0.05541
i: tensor(1, device='cuda:0') torch.isfinite(k_pot).all(): tensor(True, device='cuda:0') torch.isnan(k_pot).any(): tensor(False, device='cuda:0') min. -0.20927, max. 0.19023
i: tensor(1, device='cuda:0') torch.isfinite(v_pot).all(): tensor(True, device='cuda:0') torch.isnan(v_pot).any(): tensor(False, device='cuda:0') min. -0.55399, max. 0.68138
i: tensor(1, device='cuda:0') torch.isfinite(attention_weights).all(): tensor(True, device='cuda:0') torch.isnan(attention_weights).any(): tensor(False, device='cuda:0') min. -0.01929, max. 0.02442
weighted_values: torch.Size([20, 983, 256])
i: tensor(1, device='cuda:0') torch.isfinite(weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(weighted_values).any(): tensor(False, device='cuda:0') min. -0.00057, max. 0.00070
optimized_inverse: torch.Size([20, 3]) torch.Size([20, 3])
mesh grid shape: torch.Size([8, 15, 15]) torch.Size([8, 15, 15]) torch.Size([8, 15, 15])
valid_k: torch.Size([8, 15, 15])
shapes of q: torch.Size([20, 983, 256])
new_sum_term: torch.Size([20, 256]) 20
i: tensor(1, device='cuda:0') torch.isfinite(real_space_weighted_values).all(): tensor(True, device='cuda:0') torch.isnan(real_space_weighted_values).any(): tensor(False, device='cuda:0') min. -0.18086, max. 0.21191
node_feats_list: torch.Size([20, 983, 256]) torch.Size([20, 256])
results: torch.Size([40, 256])
2024-10-31 23:50:11.101 INFO: long_range_embedding: torch.Size([40, 256])
2024-10-31 23:50:11.104 INFO: long_range_energy: torch.Size([40, 1]), torch.Size([2])
2024-10-31 23:50:11.104 INFO: contributions: torch.Size([2, 4]), 4, torch.Size([2])
2024-10-31 23:50:11.104 INFO: total_energy: torch.Size([2])
2024-10-31 23:50:11.104 INFO: node_energy_contributions: torch.Size([40, 4])
2024-10-31 23:50:11.105 INFO: node_energy: torch.Size([40])
slurmstepd: error: *** JOB 116398 ON trace04 CANCELLED AT 2024-10-31T23:50:11 ***
